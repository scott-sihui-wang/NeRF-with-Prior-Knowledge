{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuClass": "premium"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "af43ce25edbe4c2a9caab2c96e0935fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e37aa05ca1784241ae929bec4f793bb4",
              "IPY_MODEL_27d967098b9e4817bf02eabf4f2c6845",
              "IPY_MODEL_f8065984b011411ea9591d6e64b550b3"
            ],
            "layout": "IPY_MODEL_1cf17b3601594ec9acb7cca0f12309ac"
          }
        },
        "e37aa05ca1784241ae929bec4f793bb4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0bb91f0b8c8f44439c84cfa4e00c7b23",
            "placeholder": "​",
            "style": "IPY_MODEL_8ccad8d0e3f84e15b485cd8f270451a4",
            "value": "100%"
          }
        },
        "27d967098b9e4817bf02eabf4f2c6845": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_714a4adca8b243aba7479f67aac9465d",
            "max": 553433881,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_62a1da966caa464dbf5a5fdd019e30b4",
            "value": 553433881
          }
        },
        "f8065984b011411ea9591d6e64b550b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f424c0b335bf455196768083d5fda7a3",
            "placeholder": "​",
            "style": "IPY_MODEL_ac37ed5d009d4fb09d41716d9c9ed4d9",
            "value": " 528M/528M [00:04&lt;00:00, 103MB/s]"
          }
        },
        "1cf17b3601594ec9acb7cca0f12309ac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0bb91f0b8c8f44439c84cfa4e00c7b23": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8ccad8d0e3f84e15b485cd8f270451a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "714a4adca8b243aba7479f67aac9465d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "62a1da966caa464dbf5a5fdd019e30b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f424c0b335bf455196768083d5fda7a3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ac37ed5d009d4fb09d41716d9c9ed4d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pyhocon"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        },
        "id": "ZMsjPDEU85FB",
        "outputId": "83472016-5977-4c47-9177-308ed7ec2246"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyhocon\n",
            "  Downloading pyhocon-0.3.59.tar.gz (116 kB)\n",
            "\u001b[K     |████████████████████████████████| 116 kB 28.5 MB/s \n",
            "\u001b[?25hCollecting pyparsing~=2.0\n",
            "  Downloading pyparsing-2.4.7-py2.py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 2.8 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyhocon\n",
            "  Building wheel for pyhocon (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyhocon: filename=pyhocon-0.3.59-py3-none-any.whl size=19967 sha256=5ee3af932d15ff7087d76d57b5bc0488096d4eef392a303edc9d26f8183fffe8\n",
            "  Stored in directory: /root/.cache/pip/wheels/ea/60/70/84539a4fcc8be998fcadd3291883b58a7f3ef003239dd7081c\n",
            "Successfully built pyhocon\n",
            "Installing collected packages: pyparsing, pyhocon\n",
            "  Attempting uninstall: pyparsing\n",
            "    Found existing installation: pyparsing 3.0.9\n",
            "    Uninstalling pyparsing-3.0.9:\n",
            "      Successfully uninstalled pyparsing-3.0.9\n",
            "Successfully installed pyhocon-0.3.59 pyparsing-2.4.7\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pyparsing"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyMCubes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ixWH2ytY9hHG",
        "outputId": "d829e6d3-8c9d-48d4-d6a7-ebe0e090287c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting PyMCubes\n",
            "  Downloading PyMCubes-0.1.2-cp38-cp38-manylinux2010_x86_64.whl (282 kB)\n",
            "\u001b[K     |████████████████████████████████| 282 kB 20.9 MB/s \n",
            "\u001b[?25hInstalling collected packages: PyMCubes\n",
            "Successfully installed PyMCubes-0.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dotmap"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TKqbN9tYlq_p",
        "outputId": "0289ce83-01d1-400d-f067-5bd5d866919d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting dotmap\n",
            "  Downloading dotmap-1.3.30-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: dotmap\n",
            "Successfully installed dotmap-1.3.30\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lpips"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fkp2TkmgoQCM",
        "outputId": "8f2d824c-55b4-423c-b925-dd5dd810b080"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting lpips\n",
            "  Downloading lpips-0.1.4-py3-none-any.whl (53 kB)\n",
            "\u001b[K     |████████████████████████████████| 53 kB 2.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torchvision>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from lpips) (0.14.0+cu116)\n",
            "Requirement already satisfied: torch>=0.4.0 in /usr/local/lib/python3.8/dist-packages (from lpips) (1.13.0+cu116)\n",
            "Requirement already satisfied: tqdm>=4.28.1 in /usr/local/lib/python3.8/dist-packages (from lpips) (4.64.1)\n",
            "Requirement already satisfied: numpy>=1.14.3 in /usr/local/lib/python3.8/dist-packages (from lpips) (1.21.6)\n",
            "Requirement already satisfied: scipy>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from lpips) (1.7.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch>=0.4.0->lpips) (4.4.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision>=0.2.1->lpips) (7.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchvision>=0.2.1->lpips) (2.23.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision>=0.2.1->lpips) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision>=0.2.1->lpips) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision>=0.2.1->lpips) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision>=0.2.1->lpips) (2022.9.24)\n",
            "Installing collected packages: lpips\n",
            "Successfully installed lpips-0.1.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJwYnMJWe2tP",
        "outputId": "1a11a887-1076-4bf8-c537-d0056bd15e61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir(\"/content/gdrive/MyDrive/Colab Notebooks/NeRF/\")"
      ],
      "metadata": {
        "id": "fibG2aPoe5J1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "model/code.py"
      ],
      "metadata": {
        "id": "v6CE_V9m4l2P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torch.autograd.profiler as profiler\n",
        "\n",
        "\n",
        "class PositionalEncoding(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Implement NeRF's positional encoding\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_freqs=6, d_in=3, freq_factor=np.pi, include_input=True):\n",
        "        super().__init__()\n",
        "        self.num_freqs = num_freqs\n",
        "        self.d_in = d_in\n",
        "        self.freqs = freq_factor * 2.0 ** torch.arange(0, num_freqs)\n",
        "        self.d_out = self.num_freqs * 2 * d_in\n",
        "        self.include_input = include_input\n",
        "        if include_input:\n",
        "            self.d_out += d_in\n",
        "        # f1 f1 f2 f2 ... to multiply x by\n",
        "        self.register_buffer(\n",
        "            \"_freqs\", torch.repeat_interleave(self.freqs, 2).view(1, -1, 1)\n",
        "        )\n",
        "        # 0 pi/2 0 pi/2 ... so that\n",
        "        # (sin(x + _phases[0]), sin(x + _phases[1]) ...) = (sin(x), cos(x)...)\n",
        "        _phases = torch.zeros(2 * self.num_freqs)\n",
        "        _phases[1::2] = np.pi * 0.5\n",
        "        self.register_buffer(\"_phases\", _phases.view(1, -1, 1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Apply positional encoding (new implementation)\n",
        "        :param x (batch, self.d_in)\n",
        "        :return (batch, self.d_out)\n",
        "        \"\"\"\n",
        "        with profiler.record_function(\"positional_enc\"):\n",
        "            embed = x.unsqueeze(1).repeat(1, self.num_freqs * 2, 1)\n",
        "            embed = torch.sin(torch.addcmul(self._phases, embed, self._freqs))\n",
        "            embed = embed.view(x.shape[0], -1)\n",
        "            if self.include_input:\n",
        "                embed = torch.cat((x, embed), dim=-1)\n",
        "            return embed\n",
        "\n",
        "    @classmethod\n",
        "    def from_conf(cls, conf, d_in=3):\n",
        "        # PyHocon construction\n",
        "        return cls(\n",
        "            conf.get_int(\"num_freqs\", 6),\n",
        "            d_in,\n",
        "            conf.get_float(\"freq_factor\", np.pi),\n",
        "            conf.get_bool(\"include_input\", True),\n",
        "        )"
      ],
      "metadata": {
        "id": "__zgOsIr4le6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "util/util.py"
      ],
      "metadata": {
        "id": "dp77Ehmh9tOA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from torch import nn\n",
        "from torch.nn import init\n",
        "import torch.nn.functional as F\n",
        "import functools\n",
        "import math\n",
        "import warnings\n",
        "\n",
        "\n",
        "def image_float_to_uint8(img):\n",
        "    \"\"\"\n",
        "    Convert a float image (0.0-1.0) to uint8 (0-255)\n",
        "    \"\"\"\n",
        "    vmin = np.min(img)\n",
        "    vmax = np.max(img)\n",
        "    if vmax - vmin < 1e-10:\n",
        "        vmax += 1e-10\n",
        "    img = (img - vmin) / (vmax - vmin)\n",
        "    img *= 255.0\n",
        "    return img.astype(np.uint8)\n",
        "\n",
        "\n",
        "def cmap(img, color_map=cv2.COLORMAP_HOT):\n",
        "    \"\"\"\n",
        "    Apply 'HOT' color to a float image\n",
        "    \"\"\"\n",
        "    return cv2.applyColorMap(image_float_to_uint8(img), color_map)\n",
        "\n",
        "\n",
        "def batched_index_select_nd(t, inds):\n",
        "    \"\"\"\n",
        "    Index select on dim 1 of a n-dimensional batched tensor.\n",
        "    :param t (batch, n, ...)\n",
        "    :param inds (batch, k)\n",
        "    :return (batch, k, ...)\n",
        "    \"\"\"\n",
        "    return t.gather(\n",
        "        1, inds[(...,) + (None,) * (len(t.shape) - 2)].expand(-1, -1, *t.shape[2:])\n",
        "    )\n",
        "\n",
        "\n",
        "def batched_index_select_nd_last(t, inds):\n",
        "    \"\"\"\n",
        "    Index select on dim -1 of a >=2D multi-batched tensor. inds assumed\n",
        "    to have all batch dimensions except one data dimension 'n'\n",
        "    :param t (batch..., n, m)\n",
        "    :param inds (batch..., k)\n",
        "    :return (batch..., n, k)\n",
        "    \"\"\"\n",
        "    dummy = inds.unsqueeze(-2).expand(*inds.shape[:-1], t.size(-2), inds.size(-1))\n",
        "    out = t.gather(-1, dummy)\n",
        "    return out\n",
        "\n",
        "\n",
        "def repeat_interleave(input, repeats, dim=0):\n",
        "    \"\"\"\n",
        "    Repeat interleave along axis 0\n",
        "    torch.repeat_interleave is currently very slow\n",
        "    https://github.com/pytorch/pytorch/issues/31980\n",
        "    \"\"\"\n",
        "    output = input.unsqueeze(1).expand(-1, repeats, *input.shape[1:])\n",
        "    return output.reshape(-1, *input.shape[1:])\n",
        "\n",
        "\n",
        "def get_image_to_tensor_balanced(image_size=0):\n",
        "    ops = []\n",
        "    if image_size > 0:\n",
        "        ops.append(transforms.Resize(image_size))\n",
        "    ops.extend(\n",
        "        [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),]\n",
        "    )\n",
        "    return transforms.Compose(ops)\n",
        "\n",
        "\n",
        "def get_mask_to_tensor():\n",
        "    return transforms.Compose(\n",
        "        [transforms.ToTensor(), transforms.Normalize((0.0,), (1.0,))]\n",
        "    )\n",
        "\n",
        "\n",
        "def homogeneous(points):\n",
        "    \"\"\"\n",
        "    Concat 1 to each point\n",
        "    :param points (..., 3)\n",
        "    :return (..., 4)\n",
        "    \"\"\"\n",
        "    return F.pad(points, (0, 1), \"constant\", 1.0)\n",
        "\n",
        "\n",
        "def gen_grid(*args, ij_indexing=False):\n",
        "    \"\"\"\n",
        "    Generete len(args)-dimensional grid.\n",
        "    Each arg should be (lo, hi, sz) so that in that dimension points\n",
        "    are taken at linspace(lo, hi, sz).\n",
        "    Example: gen_grid((0,1,10), (-1,1,20))\n",
        "    :return (prod_i args_i[2], len(args)), len(args)-dimensional grid points\n",
        "    \"\"\"\n",
        "    return torch.from_numpy(\n",
        "        np.vstack(\n",
        "            np.meshgrid(\n",
        "                *(np.linspace(lo, hi, sz, dtype=np.float32) for lo, hi, sz in args),\n",
        "                indexing=\"ij\" if ij_indexing else \"xy\"\n",
        "            )\n",
        "        )\n",
        "        .reshape(len(args), -1)\n",
        "        .T\n",
        "    )\n",
        "\n",
        "\n",
        "def unproj_map(width, height, f, c=None, device=\"cpu\"):\n",
        "    \"\"\"\n",
        "    Get camera unprojection map for given image size.\n",
        "    [y,x] of output tensor will contain unit vector of camera ray of that pixel.\n",
        "    :param width image width\n",
        "    :param height image height\n",
        "    :param f focal length, either a number or tensor [fx, fy]\n",
        "    :param c principal point, optional, either None or tensor [fx, fy]\n",
        "    if not specified uses center of image\n",
        "    :return unproj map (height, width, 3)\n",
        "    \"\"\"\n",
        "    if c is None:\n",
        "        c = [width * 0.5, height * 0.5]\n",
        "    else:\n",
        "        c = c.squeeze()\n",
        "    if isinstance(f, float):\n",
        "        f = [f, f]\n",
        "    elif len(f.shape) == 0:\n",
        "        f = f[None].expand(2)\n",
        "    elif len(f.shape) == 1:\n",
        "        f = f.expand(2)\n",
        "    Y, X = torch.meshgrid(\n",
        "        torch.arange(height, dtype=torch.float32) - float(c[1]),\n",
        "        torch.arange(width, dtype=torch.float32) - float(c[0]),\n",
        "    )\n",
        "    X = X.to(device=device) / float(f[0])\n",
        "    Y = Y.to(device=device) / float(f[1])\n",
        "    Z = torch.ones_like(X)\n",
        "    unproj = torch.stack((X, -Y, -Z), dim=-1)\n",
        "    unproj /= torch.norm(unproj, dim=-1).unsqueeze(-1)\n",
        "    return unproj\n",
        "\n",
        "\n",
        "def coord_from_blender(dtype=torch.float32, device=\"cpu\"):\n",
        "    \"\"\"\n",
        "    Blender to standard coordinate system transform.\n",
        "    Standard coordinate system is: x right y up z out (out=screen to face)\n",
        "    Blender coordinate system is: x right y in z up\n",
        "    :return (4, 4)\n",
        "    \"\"\"\n",
        "    return torch.tensor(\n",
        "        [[1, 0, 0, 0], [0, 0, 1, 0], [0, -1, 0, 0], [0, 0, 0, 1]],\n",
        "        dtype=dtype,\n",
        "        device=device,\n",
        "    )\n",
        "\n",
        "\n",
        "def coord_to_blender(dtype=torch.float32, device=\"cpu\"):\n",
        "    \"\"\"\n",
        "    Standard to Blender coordinate system transform.\n",
        "    Standard coordinate system is: x right y up z out (out=screen to face)\n",
        "    Blender coordinate system is: x right y in z up\n",
        "    :return (4, 4)\n",
        "    \"\"\"\n",
        "    return torch.tensor(\n",
        "        [[1, 0, 0, 0], [0, 0, -1, 0], [0, 1, 0, 0], [0, 0, 0, 1]],\n",
        "        dtype=dtype,\n",
        "        device=device,\n",
        "    )\n",
        "\n",
        "\n",
        "def look_at(origin, target, world_up=np.array([0, 1, 0], dtype=np.float32)):\n",
        "    \"\"\"\n",
        "    Get 4x4 camera to world space matrix, for camera looking at target\n",
        "    \"\"\"\n",
        "    back = origin - target\n",
        "    back /= np.linalg.norm(back)\n",
        "    right = np.cross(world_up, back)\n",
        "    right /= np.linalg.norm(right)\n",
        "    up = np.cross(back, right)\n",
        "\n",
        "    cam_to_world = np.empty((4, 4), dtype=np.float32)\n",
        "    cam_to_world[:3, 0] = right\n",
        "    cam_to_world[:3, 1] = up\n",
        "    cam_to_world[:3, 2] = back\n",
        "    cam_to_world[:3, 3] = origin\n",
        "    cam_to_world[3, :] = [0, 0, 0, 1]\n",
        "    return cam_to_world\n",
        "\n",
        "\n",
        "def get_cuda(gpu_id):\n",
        "    \"\"\"\n",
        "    Get a torch.device for GPU gpu_id. If GPU not available,\n",
        "    returns CPU device.\n",
        "    \"\"\"\n",
        "    return (\n",
        "        torch.device(\"cuda:%d\" % gpu_id)\n",
        "        if torch.cuda.is_available()\n",
        "        else torch.device(\"cpu\")\n",
        "    )\n",
        "\n",
        "\n",
        "def masked_sample(masks, num_pix, prop_inside, thresh=0.5):\n",
        "    \"\"\"\n",
        "    :return (num_pix, 3)\n",
        "    \"\"\"\n",
        "    num_inside = int(num_pix * prop_inside + 0.5)\n",
        "    num_outside = num_pix - num_inside\n",
        "    inside = (masks >= thresh).nonzero(as_tuple=False)\n",
        "    outside = (masks < thresh).nonzero(as_tuple=False)\n",
        "\n",
        "    pix_inside = inside[torch.randint(0, inside.shape[0], (num_inside,))]\n",
        "    pix_outside = outside[torch.randint(0, outside.shape[0], (num_outside,))]\n",
        "    pix = torch.cat((pix_inside, pix_outside))\n",
        "    return pix\n",
        "\n",
        "\n",
        "def bbox_sample(bboxes, num_pix):\n",
        "    \"\"\"\n",
        "    :return (num_pix, 3)\n",
        "    \"\"\"\n",
        "    image_ids = torch.randint(0, bboxes.shape[0], (num_pix,))\n",
        "    pix_bboxes = bboxes[image_ids]\n",
        "    x = (\n",
        "        torch.rand(num_pix) * (pix_bboxes[:, 2] + 1 - pix_bboxes[:, 0])\n",
        "        + pix_bboxes[:, 0]\n",
        "    ).long()\n",
        "    y = (\n",
        "        torch.rand(num_pix) * (pix_bboxes[:, 3] + 1 - pix_bboxes[:, 1])\n",
        "        + pix_bboxes[:, 1]\n",
        "    ).long()\n",
        "    pix = torch.stack((image_ids, y, x), dim=-1)\n",
        "    return pix\n",
        "\n",
        "\n",
        "def gen_rays(poses, width, height, focal, z_near, z_far, c=None, ndc=False):\n",
        "    \"\"\"\n",
        "    Generate camera rays\n",
        "    :return (B, H, W, 8)\n",
        "    \"\"\"\n",
        "    num_images = poses.shape[0]\n",
        "    device = poses.device\n",
        "    cam_unproj_map = (\n",
        "        unproj_map(width, height, focal.squeeze(), c=c, device=device)\n",
        "        .unsqueeze(0)\n",
        "        .repeat(num_images, 1, 1, 1)\n",
        "    )\n",
        "    cam_centers = poses[:, None, None, :3, 3].expand(-1, height, width, -1)\n",
        "    cam_raydir = torch.matmul(\n",
        "        poses[:, None, None, :3, :3], cam_unproj_map.unsqueeze(-1)\n",
        "    )[:, :, :, :, 0]\n",
        "    if ndc:\n",
        "        if not (z_near == 0 and z_far == 1):\n",
        "            warnings.warn(\n",
        "                \"dataset z near and z_far not compatible with NDC, setting them to 0, 1 NOW\"\n",
        "            )\n",
        "        z_near, z_far = 0.0, 1.0\n",
        "        cam_centers, cam_raydir = ndc_rays(\n",
        "            width, height, focal, 1.0, cam_centers, cam_raydir\n",
        "        )\n",
        "\n",
        "    cam_nears = (\n",
        "        torch.tensor(z_near, device=device)\n",
        "        .view(1, 1, 1, 1)\n",
        "        .expand(num_images, height, width, -1)\n",
        "    )\n",
        "    cam_fars = (\n",
        "        torch.tensor(z_far, device=device)\n",
        "        .view(1, 1, 1, 1)\n",
        "        .expand(num_images, height, width, -1)\n",
        "    )\n",
        "    return torch.cat(\n",
        "        (cam_centers, cam_raydir, cam_nears, cam_fars), dim=-1\n",
        "    )  # (B, H, W, 8)\n",
        "\n",
        "\n",
        "def trans_t(t):\n",
        "    return torch.tensor(\n",
        "        [[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, t], [0, 0, 0, 1],], dtype=torch.float32,\n",
        "    )\n",
        "\n",
        "\n",
        "def rot_phi(phi):\n",
        "    return torch.tensor(\n",
        "        [\n",
        "            [1, 0, 0, 0],\n",
        "            [0, np.cos(phi), -np.sin(phi), 0],\n",
        "            [0, np.sin(phi), np.cos(phi), 0],\n",
        "            [0, 0, 0, 1],\n",
        "        ],\n",
        "        dtype=torch.float32,\n",
        "    )\n",
        "\n",
        "\n",
        "def rot_theta(th):\n",
        "    return torch.tensor(\n",
        "        [\n",
        "            [np.cos(th), 0, -np.sin(th), 0],\n",
        "            [0, 1, 0, 0],\n",
        "            [np.sin(th), 0, np.cos(th), 0],\n",
        "            [0, 0, 0, 1],\n",
        "        ],\n",
        "        dtype=torch.float32,\n",
        "    )\n",
        "\n",
        "\n",
        "def pose_spherical(theta, phi, radius):\n",
        "    \"\"\"\n",
        "    Spherical rendering poses, from NeRF\n",
        "    \"\"\"\n",
        "    c2w = trans_t(radius)\n",
        "    c2w = rot_phi(phi / 180.0 * np.pi) @ c2w\n",
        "    c2w = rot_theta(theta / 180.0 * np.pi) @ c2w\n",
        "    c2w = (\n",
        "        torch.tensor(\n",
        "            [[-1, 0, 0, 0], [0, 0, 1, 0], [0, 1, 0, 0], [0, 0, 0, 1]],\n",
        "            dtype=torch.float32,\n",
        "        )\n",
        "        @ c2w\n",
        "    )\n",
        "    return c2w\n",
        "\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "def get_norm_layer(norm_type=\"instance\", group_norm_groups=32):\n",
        "    \"\"\"Return a normalization layer\n",
        "    Parameters:\n",
        "        norm_type (str) -- the name of the normalization layer: batch | instance | none\n",
        "    For BatchNorm, we use learnable affine parameters and track running statistics (mean/stddev).\n",
        "    For InstanceNorm, we do not use learnable affine parameters. We do not track running statistics.\n",
        "    \"\"\"\n",
        "    if norm_type == \"batch\":\n",
        "        norm_layer = functools.partial(\n",
        "            nn.BatchNorm2d, affine=True, track_running_stats=True\n",
        "        )\n",
        "    elif norm_type == \"instance\":\n",
        "        norm_layer = functools.partial(\n",
        "            nn.InstanceNorm2d, affine=False, track_running_stats=False\n",
        "        )\n",
        "    elif norm_type == \"group\":\n",
        "        norm_layer = functools.partial(nn.GroupNorm, group_norm_groups)\n",
        "    elif norm_type == \"none\":\n",
        "        norm_layer = None\n",
        "    else:\n",
        "        raise NotImplementedError(\"normalization layer [%s] is not found\" % norm_type)\n",
        "    return norm_layer\n",
        "\n",
        "\n",
        "def make_conv_2d(\n",
        "    dim_in,\n",
        "    dim_out,\n",
        "    padding_type=\"reflect\",\n",
        "    norm_layer=None,\n",
        "    activation=None,\n",
        "    kernel_size=3,\n",
        "    use_bias=False,\n",
        "    stride=1,\n",
        "    no_pad=False,\n",
        "    zero_init=False,\n",
        "):\n",
        "    conv_block = []\n",
        "    amt = kernel_size // 2\n",
        "    if stride > 1 and not no_pad:\n",
        "        raise NotImplementedError(\n",
        "            \"Padding with stride > 1 not supported, use same_pad_conv2d\"\n",
        "        )\n",
        "\n",
        "    if amt > 0 and not no_pad:\n",
        "        if padding_type == \"reflect\":\n",
        "            conv_block += [nn.ReflectionPad2d(amt)]\n",
        "        elif padding_type == \"replicate\":\n",
        "            conv_block += [nn.ReplicationPad2d(amt)]\n",
        "        elif padding_type == \"zero\":\n",
        "            conv_block += [nn.ZeroPad2d(amt)]\n",
        "        else:\n",
        "            raise NotImplementedError(\"padding [%s] is not implemented\" % padding_type)\n",
        "\n",
        "    conv_block.append(\n",
        "        nn.Conv2d(\n",
        "            dim_in, dim_out, kernel_size=kernel_size, bias=use_bias, stride=stride\n",
        "        )\n",
        "    )\n",
        "    if zero_init:\n",
        "        nn.init.zeros_(conv_block[-1].weight)\n",
        "    #  else:\n",
        "    #  nn.init.kaiming_normal_(conv_block[-1].weight)\n",
        "    if norm_layer is not None:\n",
        "        conv_block.append(norm_layer(dim_out))\n",
        "\n",
        "    if activation is not None:\n",
        "        conv_block.append(activation)\n",
        "    return nn.Sequential(*conv_block)\n",
        "\n",
        "\n",
        "def calc_same_pad_conv2d(t_shape, kernel_size=3, stride=1):\n",
        "    in_height, in_width = t_shape[-2:]\n",
        "    out_height = math.ceil(in_height / stride)\n",
        "    out_width = math.ceil(in_width / stride)\n",
        "\n",
        "    pad_along_height = max((out_height - 1) * stride + kernel_size - in_height, 0)\n",
        "    pad_along_width = max((out_width - 1) * stride + kernel_size - in_width, 0)\n",
        "    pad_top = pad_along_height // 2\n",
        "    pad_bottom = pad_along_height - pad_top\n",
        "    pad_left = pad_along_width // 2\n",
        "    pad_right = pad_along_width - pad_left\n",
        "    return pad_left, pad_right, pad_top, pad_bottom\n",
        "\n",
        "\n",
        "def same_pad_conv2d(t, padding_type=\"reflect\", kernel_size=3, stride=1, layer=None):\n",
        "    \"\"\"\n",
        "    Perform SAME padding on tensor, given kernel size/stride of conv operator\n",
        "    assumes kernel/stride are equal in all dimensions.\n",
        "    Use before conv called.\n",
        "    Dilation not supported.\n",
        "    :param t image tensor input (B, C, H, W)\n",
        "    :param padding_type padding type constant | reflect | replicate | circular\n",
        "    constant is 0-pad.\n",
        "    :param kernel_size kernel size of conv\n",
        "    :param stride stride of conv\n",
        "    :param layer optionally, pass conv layer to automatically get kernel_size and stride\n",
        "    (overrides these)\n",
        "    \"\"\"\n",
        "    if layer is not None:\n",
        "        if isinstance(layer, nn.Sequential):\n",
        "            layer = next(layer.children())\n",
        "        kernel_size = layer.kernel_size[0]\n",
        "        stride = layer.stride[0]\n",
        "    return F.pad(\n",
        "        t, calc_same_pad_conv2d(t.shape, kernel_size, stride), mode=padding_type\n",
        "    )\n",
        "\n",
        "\n",
        "def same_unpad_deconv2d(t, kernel_size=3, stride=1, layer=None):\n",
        "    \"\"\"\n",
        "    Perform SAME unpad on tensor, given kernel/stride of deconv operator.\n",
        "    Use after deconv called.\n",
        "    Dilation not supported.\n",
        "    \"\"\"\n",
        "    if layer is not None:\n",
        "        if isinstance(layer, nn.Sequential):\n",
        "            layer = next(layer.children())\n",
        "        kernel_size = layer.kernel_size[0]\n",
        "        stride = layer.stride[0]\n",
        "    h_scaled = (t.shape[-2] - 1) * stride\n",
        "    w_scaled = (t.shape[-1] - 1) * stride\n",
        "    pad_left, pad_right, pad_top, pad_bottom = calc_same_pad_conv2d(\n",
        "        (h_scaled, w_scaled), kernel_size, stride\n",
        "    )\n",
        "    if pad_right == 0:\n",
        "        pad_right = -10000\n",
        "    if pad_bottom == 0:\n",
        "        pad_bottom = -10000\n",
        "    return t[..., pad_top:-pad_bottom, pad_left:-pad_right]\n",
        "\n",
        "\n",
        "def combine_interleaved(t, inner_dims=(1,), agg_type=\"average\"):\n",
        "    if len(inner_dims) == 1 and inner_dims[0] == 1:\n",
        "        return t\n",
        "    t = t.reshape(-1, *inner_dims, *t.shape[1:])\n",
        "    if agg_type == \"average\":\n",
        "        t = torch.mean(t, dim=1)\n",
        "    elif agg_type == \"max\":\n",
        "        t = torch.max(t, dim=1)[0]\n",
        "    else:\n",
        "        raise NotImplementedError(\"Unsupported combine type \" + agg_type)\n",
        "    return t\n",
        "\n",
        "\n",
        "def psnr(pred, target):\n",
        "    \"\"\"\n",
        "    Compute PSNR of two tensors in decibels.\n",
        "    pred/target should be of same size or broadcastable\n",
        "    \"\"\"\n",
        "    mse = ((pred - target) ** 2).mean()\n",
        "    psnr = -10 * math.log10(mse)\n",
        "    return psnr\n",
        "\n",
        "\n",
        "def quat_to_rot(q):\n",
        "    \"\"\"\n",
        "    Quaternion to rotation matrix\n",
        "    \"\"\"\n",
        "    batch_size, _ = q.shape\n",
        "    q = F.normalize(q, dim=1)\n",
        "    R = torch.ones((batch_size, 3, 3), device=q.device)\n",
        "    qr = q[:, 0]\n",
        "    qi = q[:, 1]\n",
        "    qj = q[:, 2]\n",
        "    qk = q[:, 3]\n",
        "    R[:, 0, 0] = 1 - 2 * (qj ** 2 + qk ** 2)\n",
        "    R[:, 0, 1] = 2 * (qj * qi - qk * qr)\n",
        "    R[:, 0, 2] = 2 * (qi * qk + qr * qj)\n",
        "    R[:, 1, 0] = 2 * (qj * qi + qk * qr)\n",
        "    R[:, 1, 1] = 1 - 2 * (qi ** 2 + qk ** 2)\n",
        "    R[:, 1, 2] = 2 * (qj * qk - qi * qr)\n",
        "    R[:, 2, 0] = 2 * (qk * qi - qj * qr)\n",
        "    R[:, 2, 1] = 2 * (qj * qk + qi * qr)\n",
        "    R[:, 2, 2] = 1 - 2 * (qi ** 2 + qj ** 2)\n",
        "    return R\n",
        "\n",
        "\n",
        "def rot_to_quat(R):\n",
        "    \"\"\"\n",
        "    Rotation matrix to quaternion\n",
        "    \"\"\"\n",
        "    batch_size, _, _ = R.shape\n",
        "    q = torch.ones((batch_size, 4), device=R.device)\n",
        "\n",
        "    R00 = R[:, 0, 0]\n",
        "    R01 = R[:, 0, 1]\n",
        "    R02 = R[:, 0, 2]\n",
        "    R10 = R[:, 1, 0]\n",
        "    R11 = R[:, 1, 1]\n",
        "    R12 = R[:, 1, 2]\n",
        "    R20 = R[:, 2, 0]\n",
        "    R21 = R[:, 2, 1]\n",
        "    R22 = R[:, 2, 2]\n",
        "\n",
        "    q[:, 0] = torch.sqrt(1.0 + R00 + R11 + R22) / 2\n",
        "    q[:, 1] = (R21 - R12) / (4 * q[:, 0])\n",
        "    q[:, 2] = (R02 - R20) / (4 * q[:, 0])\n",
        "    q[:, 3] = (R10 - R01) / (4 * q[:, 0])\n",
        "    return q\n",
        "\n",
        "\n",
        "def get_module(net):\n",
        "    \"\"\"\n",
        "    Shorthand for either net.module (if net is instance of DataParallel) or net\n",
        "    \"\"\"\n",
        "    if isinstance(net, torch.nn.DataParallel):\n",
        "        return net.module\n",
        "    else:\n",
        "        return net"
      ],
      "metadata": {
        "id": "n21sVVop9s_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "util/args.py"
      ],
      "metadata": {
        "id": "xrhe5qb075im"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "\n",
        "import argparse\n",
        "from pyhocon import ConfigFactory\n",
        "\n",
        "\n",
        "def parse_args(\n",
        "    callback=None,\n",
        "    training=False,\n",
        "    default_conf=\"conf/default_mv.conf\",\n",
        "    default_expname=\"example\",\n",
        "    default_data_format=\"dvr\",\n",
        "    default_num_epochs=10000000,\n",
        "    default_lr=1e-4,\n",
        "    default_gamma=1.00,\n",
        "    default_datadir=\"data\",\n",
        "    default_ray_batch_size=50000,\n",
        "):\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--conf\", \"-c\", type=str, default=None)\n",
        "    parser.add_argument(\"--resume\", \"-r\", action=\"store_true\", help=\"continue training\")\n",
        "    parser.add_argument(\n",
        "        \"--gpu_id\", type=str, default=\"0\", help=\"GPU(s) to use, space delimited\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--name\", \"-n\", type=str, default=default_expname, help=\"experiment name\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--dataset_format\",\n",
        "        \"-F\",\n",
        "        type=str,\n",
        "        default=None,\n",
        "        help=\"Dataset format, multi_obj | dvr | dvr_gen | dvr_dtu | srn\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--exp_group_name\",\n",
        "        \"-G\",\n",
        "        type=str,\n",
        "        default=None,\n",
        "        help=\"if we want to group some experiments together\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--logs_path\", type=str, default=\"logs\", help=\"logs output directory\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--checkpoints_path\",\n",
        "        type=str,\n",
        "        default=\"checkpoints\",\n",
        "        help=\"checkpoints output directory\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--visual_path\",\n",
        "        type=str,\n",
        "        default=\"visuals\",\n",
        "        help=\"visualization output directory\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--epochs\",\n",
        "        type=int,\n",
        "        default=default_num_epochs,\n",
        "        help=\"number of epochs to train for\",\n",
        "    )\n",
        "    parser.add_argument(\"--lr\", type=float, default=default_lr, help=\"learning rate\")\n",
        "    parser.add_argument(\n",
        "        \"--gamma\", type=float, default=default_gamma, help=\"learning rate decay factor\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--datadir\", \"-D\", type=str, default=None, help=\"Dataset directory\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--ray_batch_size\", \"-R\", type=int, default=default_ray_batch_size, help=\"Ray batch size\"\n",
        "    )\n",
        "    if callback is not None:\n",
        "        parser = callback(parser)\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    if args.exp_group_name is not None:\n",
        "        args.logs_path = os.path.join(args.logs_path, args.exp_group_name)\n",
        "        args.checkpoints_path = os.path.join(args.checkpoints_path, args.exp_group_name)\n",
        "        args.visual_path = os.path.join(args.visual_path, args.exp_group_name)\n",
        "\n",
        "    os.makedirs(os.path.join(args.checkpoints_path, args.name), exist_ok=True)\n",
        "    os.makedirs(os.path.join(args.visual_path, args.name), exist_ok=True)\n",
        "\n",
        "    PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), \"..\", \"..\"))\n",
        "    EXPCONF_PATH = os.path.join(PROJECT_ROOT, \"expconf.conf\")\n",
        "    expconf = ConfigFactory.parse_file(EXPCONF_PATH)\n",
        "\n",
        "    if args.conf is None:\n",
        "        args.conf = expconf.get_string(\"config.\" + args.name, default_conf)\n",
        "\n",
        "    if args.conf is None:\n",
        "        args.conf = expconf.get_string(\"config.\" + args.name, default_conf)\n",
        "    if args.datadir is None:\n",
        "        args.datadir = expconf.get_string(\"datadir.\" + args.name, default_datadir)\n",
        "\n",
        "    conf = ConfigFactory.parse_file(args.conf)\n",
        "\n",
        "    if args.dataset_format is None:\n",
        "        args.dataset_format = conf.get_string(\"data.format\", default_data_format)\n",
        "\n",
        "    args.gpu_id = list(map(int, args.gpu_id.split()))\n",
        "\n",
        "    print(\"EXPERIMENT NAME:\", args.name)\n",
        "    if training:\n",
        "        print(\"CONTINUE?\", \"yes\" if args.resume else \"no\")\n",
        "    print(\"* Config file:\", args.conf)\n",
        "    print(\"* Dataset format:\", args.dataset_format)\n",
        "    print(\"* Dataset location:\", args.datadir)\n",
        "    return args, conf"
      ],
      "metadata": {
        "id": "pSaZi5Cy7567"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "util/recon.py"
      ],
      "metadata": {
        "id": "5SmZRSex9TUR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import mcubes\n",
        "import torch\n",
        "import numpy as np\n",
        "import tqdm\n",
        "import warnings\n",
        "\n",
        "\n",
        "def marching_cubes(\n",
        "    occu_net,\n",
        "    c1=[-1, -1, -1],\n",
        "    c2=[1, 1, 1],\n",
        "    reso=[128, 128, 128],\n",
        "    isosurface=50.0,\n",
        "    sigma_idx=3,\n",
        "    eval_batch_size=100000,\n",
        "    coarse=True,\n",
        "    device=None,\n",
        "):\n",
        "    \"\"\"\n",
        "    Run marching cubes on network. Uses PyMCubes.\n",
        "    WARNING: does not make much sense with viewdirs in current form, since\n",
        "    sigma depends on viewdirs.\n",
        "    :param occu_net main NeRF type network\n",
        "    :param c1 corner 1 of marching cube bounds x,y,z\n",
        "    :param c2 corner 2 of marching cube bounds x,y,z (all > c1)\n",
        "    :param reso resolutions of marching cubes x,y,z\n",
        "    :param isosurface sigma-isosurface of marching cubes\n",
        "    :param sigma_idx index of 'sigma' value in last dimension of occu_net's output\n",
        "    :param eval_batch_size batch size for evaluation\n",
        "    :param coarse whether to use coarse NeRF for evaluation\n",
        "    :param device optionally, device to put points for evaluation.\n",
        "    By default uses device of occu_net's first parameter.\n",
        "    \"\"\"\n",
        "    if occu_net.use_viewdirs:\n",
        "        warnings.warn(\n",
        "            \"Running marching cubes with fake view dirs (pointing to origin), output may be invalid\"\n",
        "        )\n",
        "    with torch.no_grad():\n",
        "        grid = gen_grid(*zip(c1, c2, reso), ij_indexing=True)\n",
        "        is_train = occu_net.training\n",
        "\n",
        "        print(\"Evaluating sigma @\", grid.size(0), \"points\")\n",
        "        occu_net.eval()\n",
        "\n",
        "        all_sigmas = []\n",
        "        if device is None:\n",
        "            device = next(occu_net.parameters()).device\n",
        "        grid_spl = torch.split(grid, eval_batch_size, dim=0)\n",
        "        if occu_net.use_viewdirs:\n",
        "            fake_viewdirs = -grid / torch.norm(grid, dim=-1).unsqueeze(-1)\n",
        "            vd_spl = torch.split(fake_viewdirs, eval_batch_size, dim=0)\n",
        "            for pnts, vd in tqdm.tqdm(zip(grid_spl, vd_spl), total=len(grid_spl)):\n",
        "                outputs = occu_net(\n",
        "                    pnts.to(device=device), coarse=coarse, viewdirs=vd.to(device=device)\n",
        "                )\n",
        "                sigmas = outputs[..., sigma_idx]\n",
        "                all_sigmas.append(sigmas.cpu())\n",
        "        else:\n",
        "            for pnts in tqdm.tqdm(grid_spl):\n",
        "                outputs = occu_net(pnts.to(device=device), coarse=coarse)\n",
        "                sigmas = outputs[..., sigma_idx]\n",
        "                all_sigmas.append(sigmas.cpu())\n",
        "        sigmas = torch.cat(all_sigmas, dim=0)\n",
        "        sigmas = sigmas.view(*reso).cpu().numpy()\n",
        "\n",
        "        print(\"Running marching cubes\")\n",
        "        vertices, triangles = mcubes.marching_cubes(sigmas, isosurface)\n",
        "        # Scale\n",
        "        c1, c2 = np.array(c1), np.array(c2)\n",
        "        vertices *= (c2 - c1) / np.array(reso)\n",
        "\n",
        "    if is_train:\n",
        "        occu_net.train()\n",
        "    return vertices + c1, triangles\n",
        "\n",
        "\n",
        "def save_obj(vertices, triangles, path, vert_rgb=None):\n",
        "    \"\"\"\n",
        "    Save OBJ file, optionally with vertex colors.\n",
        "    This version is faster than PyMCubes and supports color.\n",
        "    Taken from PIFu.\n",
        "    :param vertices (N, 3)\n",
        "    :param triangles (N, 3)\n",
        "    :param vert_rgb (N, 3) rgb\n",
        "    \"\"\"\n",
        "    file = open(path, \"w\")\n",
        "    if vert_rgb is None:\n",
        "        # No color\n",
        "        for v in vertices:\n",
        "            file.write(\"v %.4f %.4f %.4f\\n\" % (v[0], v[1], v[2]))\n",
        "    else:\n",
        "        # Color\n",
        "        for idx, v in enumerate(vertices):\n",
        "            c = vert_rgb[idx]\n",
        "            file.write(\n",
        "                \"v %.4f %.4f %.4f %.4f %.4f %.4f\\n\"\n",
        "                % (v[0], v[1], v[2], c[0], c[1], c[2])\n",
        "            )\n",
        "    for f in triangles:\n",
        "        f_plus = f + 1\n",
        "        file.write(\"f %d %d %d\\n\" % (f_plus[0], f_plus[1], f_plus[2]))\n",
        "    file.close()"
      ],
      "metadata": {
        "id": "Mt0md0gY9Tuy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "custom_encoder.py"
      ],
      "metadata": {
        "id": "zCjcjP-H7zo5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class ConvEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Basic, extremely simple convolutional encoder\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim_in=3,\n",
        "        norm_layer=get_norm_layer(\"group\"),\n",
        "        padding_type=\"reflect\",\n",
        "        use_leaky_relu=True,\n",
        "        use_skip_conn=True,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.dim_in = dim_in\n",
        "        self.norm_layer = norm_layer\n",
        "        self.activation = nn.LeakyReLU() if use_leaky_relu else nn.ReLU()\n",
        "        self.padding_type = padding_type\n",
        "        self.use_skip_conn = use_skip_conn\n",
        "\n",
        "        # TODO: make these configurable\n",
        "        first_layer_chnls = 64\n",
        "        mid_layer_chnls = 128\n",
        "        last_layer_chnls = 128\n",
        "        n_down_layers = 3\n",
        "        self.n_down_layers = n_down_layers\n",
        "\n",
        "        self.conv_in = nn.Sequential(\n",
        "            nn.Conv2d(dim_in, first_layer_chnls, kernel_size=7, stride=2, bias=False),\n",
        "            norm_layer(first_layer_chnls),\n",
        "            self.activation,\n",
        "        )\n",
        "\n",
        "        chnls = first_layer_chnls\n",
        "        for i in range(0, n_down_layers):\n",
        "            conv = nn.Sequential(\n",
        "                nn.Conv2d(chnls, 2 * chnls, kernel_size=3, stride=2, bias=False),\n",
        "                norm_layer(2 * chnls),\n",
        "                self.activation,\n",
        "            )\n",
        "            setattr(self, \"conv\" + str(i), conv)\n",
        "\n",
        "            deconv = nn.Sequential(\n",
        "                nn.ConvTranspose2d(\n",
        "                    4 * chnls, chnls, kernel_size=3, stride=2, bias=False\n",
        "                ),\n",
        "                norm_layer(chnls),\n",
        "                self.activation,\n",
        "            )\n",
        "            setattr(self, \"deconv\" + str(i), deconv)\n",
        "            chnls *= 2\n",
        "\n",
        "        self.conv_mid = nn.Sequential(\n",
        "            nn.Conv2d(chnls, mid_layer_chnls, kernel_size=4, stride=4, bias=False),\n",
        "            norm_layer(mid_layer_chnls),\n",
        "            self.activation,\n",
        "        )\n",
        "\n",
        "        self.deconv_last = nn.ConvTranspose2d(\n",
        "            first_layer_chnls, last_layer_chnls, kernel_size=3, stride=2, bias=True\n",
        "        )\n",
        "\n",
        "        self.dims = [last_layer_chnls]\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = same_pad_conv2d(x, padding_type=self.padding_type, layer=self.conv_in)\n",
        "        x = self.conv_in(x)\n",
        "\n",
        "        inters = []\n",
        "        for i in range(0, self.n_down_layers):\n",
        "            conv_i = getattr(self, \"conv\" + str(i))\n",
        "            x = same_pad_conv2d(x, padding_type=self.padding_type, layer=conv_i)\n",
        "            x = conv_i(x)\n",
        "            inters.append(x)\n",
        "\n",
        "        x = same_pad_conv2d(x, padding_type=self.padding_type, layer=self.conv_mid)\n",
        "        x = self.conv_mid(x)\n",
        "        x = x.reshape(x.shape[0], -1, 1, 1).expand(-1, -1, *inters[-1].shape[-2:])\n",
        "\n",
        "        for i in reversed(range(0, self.n_down_layers)):\n",
        "            if self.use_skip_conn:\n",
        "                x = torch.cat((x, inters[i]), dim=1)\n",
        "            deconv_i = getattr(self, \"deconv\" + str(i))\n",
        "            x = deconv_i(x)\n",
        "            x = same_unpad_deconv2d(x, layer=deconv_i)\n",
        "        x = self.deconv_last(x)\n",
        "        x = same_unpad_deconv2d(x, layer=self.deconv_last)\n",
        "        return x"
      ],
      "metadata": {
        "id": "i61zPpAM7z72"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "model/encoder.py"
      ],
      "metadata": {
        "id": "0b8GpX9Zifnv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torch.autograd.profiler as profiler\n",
        "\n",
        "\n",
        "class SpatialEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    2D (Spatial/Pixel-aligned/local) image encoder\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        backbone=\"resnet34\",\n",
        "        pretrained=True,\n",
        "        num_layers=4,\n",
        "        index_interp=\"bilinear\",\n",
        "        index_padding=\"border\",\n",
        "        upsample_interp=\"bilinear\",\n",
        "        feature_scale=1.0,\n",
        "        use_first_pool=True,\n",
        "        norm_type=\"batch\",\n",
        "    ):\n",
        "        \"\"\"\n",
        "        :param backbone Backbone network. Either custom, in which case\n",
        "        model.custom_encoder.ConvEncoder is used OR resnet18/resnet34, in which case the relevant\n",
        "        model from torchvision is used\n",
        "        :param num_layers number of resnet layers to use, 1-5\n",
        "        :param pretrained Whether to use model weights pretrained on ImageNet\n",
        "        :param index_interp Interpolation to use for indexing\n",
        "        :param index_padding Padding mode to use for indexing, border | zeros | reflection\n",
        "        :param upsample_interp Interpolation to use for upscaling latent code\n",
        "        :param feature_scale factor to scale all latent by. Useful (<1) if image\n",
        "        is extremely large, to fit in memory.\n",
        "        :param use_first_pool if false, skips first maxpool layer to avoid downscaling image\n",
        "        features too much (ResNet only)\n",
        "        :param norm_type norm type to applied; pretrained model must use batch\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        if norm_type != \"batch\":\n",
        "            assert not pretrained\n",
        "\n",
        "        self.use_custom_resnet = backbone == \"custom\"\n",
        "        self.feature_scale = feature_scale\n",
        "        self.use_first_pool = use_first_pool\n",
        "        norm_layer = get_norm_layer(norm_type)\n",
        "\n",
        "        if self.use_custom_resnet:\n",
        "            print(\"WARNING: Custom encoder is experimental only\")\n",
        "            print(\"Using simple convolutional encoder\")\n",
        "            self.model = ConvEncoder(3, norm_layer=norm_layer)\n",
        "            self.latent_size = self.model.dims[-1]\n",
        "        else:\n",
        "            print(\"Using torchvision\", backbone, \"encoder\")\n",
        "            self.model = getattr(torchvision.models, backbone)(\n",
        "                pretrained=pretrained, norm_layer=norm_layer\n",
        "            )\n",
        "            # Following 2 lines need to be uncommented for older configs\n",
        "            self.model.fc = nn.Sequential()\n",
        "            self.model.avgpool = nn.Sequential()\n",
        "            self.latent_size = [0, 64, 128, 256, 512, 1024][num_layers]\n",
        "\n",
        "        self.num_layers = num_layers\n",
        "        self.index_interp = index_interp\n",
        "        self.index_padding = index_padding\n",
        "        self.upsample_interp = upsample_interp\n",
        "        self.register_buffer(\"latent\", torch.empty(1, 1, 1, 1), persistent=False)\n",
        "        self.register_buffer(\n",
        "            \"latent_scaling\", torch.empty(2, dtype=torch.float32), persistent=False\n",
        "        )\n",
        "        # self.latent (B, L, H, W)\n",
        "\n",
        "    def index(self, uv, cam_z=None, image_size=(), z_bounds=None):\n",
        "        \"\"\"\n",
        "        Get pixel-aligned image features at 2D image coordinates\n",
        "        :param uv (B, N, 2) image points (x,y)\n",
        "        :param cam_z ignored (for compatibility)\n",
        "        :param image_size image size, either (width, height) or single int.\n",
        "        if not specified, assumes coords are in [-1, 1]\n",
        "        :param z_bounds ignored (for compatibility)\n",
        "        :return (B, L, N) L is latent size\n",
        "        \"\"\"\n",
        "        with profiler.record_function(\"encoder_index\"):\n",
        "            if uv.shape[0] == 1 and self.latent.shape[0] > 1:\n",
        "                uv = uv.expand(self.latent.shape[0], -1, -1)\n",
        "\n",
        "            with profiler.record_function(\"encoder_index_pre\"):\n",
        "                if len(image_size) > 0:\n",
        "                    if len(image_size) == 1:\n",
        "                        image_size = (image_size, image_size)\n",
        "                    scale = self.latent_scaling / image_size\n",
        "                    uv = uv * scale - 1.0\n",
        "\n",
        "            uv = uv.unsqueeze(2)  # (B, N, 1, 2)\n",
        "            samples = F.grid_sample(\n",
        "                self.latent,\n",
        "                uv,\n",
        "                align_corners=True,\n",
        "                mode=self.index_interp,\n",
        "                padding_mode=self.index_padding,\n",
        "            )\n",
        "            return samples[:, :, :, 0]  # (B, C, N)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        For extracting ResNet's features.\n",
        "        :param x image (B, C, H, W)\n",
        "        :return latent (B, latent_size, H, W)\n",
        "        \"\"\"\n",
        "        if self.feature_scale != 1.0:\n",
        "            x = F.interpolate(\n",
        "                x,\n",
        "                scale_factor=self.feature_scale,\n",
        "                mode=\"bilinear\" if self.feature_scale > 1.0 else \"area\",\n",
        "                align_corners=True if self.feature_scale > 1.0 else None,\n",
        "                recompute_scale_factor=True,\n",
        "            )\n",
        "        x = x.to(device=self.latent.device)\n",
        "\n",
        "        if self.use_custom_resnet:\n",
        "            self.latent = self.model(x)\n",
        "        else:\n",
        "            x = self.model.conv1(x)\n",
        "            x = self.model.bn1(x)\n",
        "            x = self.model.relu(x)\n",
        "\n",
        "            latents = [x]\n",
        "            if self.num_layers > 1:\n",
        "                if self.use_first_pool:\n",
        "                    x = self.model.maxpool(x)\n",
        "                x = self.model.layer1(x)\n",
        "                latents.append(x)\n",
        "            if self.num_layers > 2:\n",
        "                x = self.model.layer2(x)\n",
        "                latents.append(x)\n",
        "            if self.num_layers > 3:\n",
        "                x = self.model.layer3(x)\n",
        "                latents.append(x)\n",
        "            if self.num_layers > 4:\n",
        "                x = self.model.layer4(x)\n",
        "                latents.append(x)\n",
        "\n",
        "            self.latents = latents\n",
        "            align_corners = None if self.index_interp == \"nearest \" else True\n",
        "            latent_sz = latents[0].shape[-2:]\n",
        "            for i in range(len(latents)):\n",
        "                latents[i] = F.interpolate(\n",
        "                    latents[i],\n",
        "                    latent_sz,\n",
        "                    mode=self.upsample_interp,\n",
        "                    align_corners=align_corners,\n",
        "                )\n",
        "            self.latent = torch.cat(latents, dim=1)\n",
        "        self.latent_scaling[0] = self.latent.shape[-1]\n",
        "        self.latent_scaling[1] = self.latent.shape[-2]\n",
        "        self.latent_scaling = self.latent_scaling / (self.latent_scaling - 1) * 2.0\n",
        "        return self.latent\n",
        "\n",
        "    @classmethod\n",
        "    def from_conf(cls, conf):\n",
        "        return cls(\n",
        "            conf.get_string(\"backbone\"),\n",
        "            pretrained=conf.get_bool(\"pretrained\", True),\n",
        "            num_layers=conf.get_int(\"num_layers\", 4),\n",
        "            index_interp=conf.get_string(\"index_interp\", \"bilinear\"),\n",
        "            index_padding=conf.get_string(\"index_padding\", \"border\"),\n",
        "            upsample_interp=conf.get_string(\"upsample_interp\", \"bilinear\"),\n",
        "            feature_scale=conf.get_float(\"feature_scale\", 1.0),\n",
        "            use_first_pool=conf.get_bool(\"use_first_pool\", True),\n",
        "        )\n",
        "\n",
        "\n",
        "class ImageEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Global image encoder\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, backbone=\"resnet34\", pretrained=True, latent_size=128):\n",
        "        \"\"\"\n",
        "        :param backbone Backbone network. Assumes it is resnet*\n",
        "        e.g. resnet34 | resnet50\n",
        "        :param num_layers number of resnet layers to use, 1-5\n",
        "        :param pretrained Whether to use model pretrained on ImageNet\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.model = getattr(torchvision.models, backbone)(pretrained=pretrained)\n",
        "        self.model.fc = nn.Sequential()\n",
        "        self.register_buffer(\"latent\", torch.empty(1, 1), persistent=False)\n",
        "        # self.latent (B, L)\n",
        "        self.latent_size = latent_size\n",
        "        if latent_size != 512:\n",
        "            self.fc = nn.Linear(512, latent_size)\n",
        "\n",
        "    def index(self, uv, cam_z=None, image_size=(), z_bounds=()):\n",
        "        \"\"\"\n",
        "        Params ignored (compatibility)\n",
        "        :param uv (B, N, 2) only used for shape\n",
        "        :return latent vector (B, L, N)\n",
        "        \"\"\"\n",
        "        return self.latent.unsqueeze(-1).expand(-1, -1, uv.shape[1])\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        For extracting ResNet's features.\n",
        "        :param x image (B, C, H, W)\n",
        "        :return latent (B, latent_size)\n",
        "        \"\"\"\n",
        "        x = x.to(device=self.latent.device)\n",
        "        x = self.model.conv1(x)\n",
        "        x = self.model.bn1(x)\n",
        "        x = self.model.relu(x)\n",
        "\n",
        "        x = self.model.maxpool(x)\n",
        "        x = self.model.layer1(x)\n",
        "        x = self.model.layer2(x)\n",
        "        x = self.model.layer3(x)\n",
        "        x = self.model.layer4(x)\n",
        "\n",
        "        x = self.model.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "\n",
        "        if self.latent_size != 512:\n",
        "            x = self.fc(x)\n",
        "\n",
        "        self.latent = x  # (B, latent_size)\n",
        "        return self.latent\n",
        "\n",
        "    @classmethod\n",
        "    def from_conf(cls, conf):\n",
        "        return cls(\n",
        "            conf.get_string(\"backbone\"),\n",
        "            pretrained=conf.get_bool(\"pretrained\", True),\n",
        "            latent_size=conf.get_int(\"latent_size\", 128),\n",
        "        )"
      ],
      "metadata": {
        "id": "sXgsZLcqiins"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "resnetfc.py"
      ],
      "metadata": {
        "id": "fZVGojz3jP6h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "import torch\n",
        "\n",
        "#  import torch_scatter\n",
        "import torch.autograd.profiler as profiler\n",
        "\n",
        "\n",
        "# Resnet Blocks\n",
        "class ResnetBlockFC(nn.Module):\n",
        "    \"\"\"\n",
        "    Fully connected ResNet Block class.\n",
        "    Taken from DVR code.\n",
        "    :param size_in (int): input dimension\n",
        "    :param size_out (int): output dimension\n",
        "    :param size_h (int): hidden dimension\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, size_in, size_out=None, size_h=None, beta=0.0):\n",
        "        super().__init__()\n",
        "        # Attributes\n",
        "        if size_out is None:\n",
        "            size_out = size_in\n",
        "\n",
        "        if size_h is None:\n",
        "            size_h = min(size_in, size_out)\n",
        "\n",
        "        self.size_in = size_in\n",
        "        self.size_h = size_h\n",
        "        self.size_out = size_out\n",
        "        # Submodules\n",
        "        self.fc_0 = nn.Linear(size_in, size_h)\n",
        "        self.fc_1 = nn.Linear(size_h, size_out)\n",
        "\n",
        "        # Init\n",
        "        nn.init.constant_(self.fc_0.bias, 0.0)\n",
        "        nn.init.kaiming_normal_(self.fc_0.weight, a=0, mode=\"fan_in\")\n",
        "        nn.init.constant_(self.fc_1.bias, 0.0)\n",
        "        nn.init.zeros_(self.fc_1.weight)\n",
        "\n",
        "        if beta > 0:\n",
        "            self.activation = nn.Softplus(beta=beta)\n",
        "        else:\n",
        "            self.activation = nn.ReLU()\n",
        "\n",
        "        if size_in == size_out:\n",
        "            self.shortcut = None\n",
        "        else:\n",
        "            self.shortcut = nn.Linear(size_in, size_out, bias=False)\n",
        "            nn.init.constant_(self.shortcut.bias, 0.0)\n",
        "            nn.init.kaiming_normal_(self.shortcut.weight, a=0, mode=\"fan_in\")\n",
        "\n",
        "    def forward(self, x):\n",
        "        with profiler.record_function(\"resblock\"):\n",
        "            net = self.fc_0(self.activation(x))\n",
        "            dx = self.fc_1(self.activation(net))\n",
        "\n",
        "            if self.shortcut is not None:\n",
        "                x_s = self.shortcut(x)\n",
        "            else:\n",
        "                x_s = x\n",
        "            return x_s + dx\n",
        "\n",
        "\n",
        "class ResnetFC(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        d_in,\n",
        "        d_out=4,\n",
        "        n_blocks=5,\n",
        "        d_latent=0,\n",
        "        d_hidden=128,\n",
        "        beta=0.0,\n",
        "        combine_layer=1000,\n",
        "        combine_type=\"average\",\n",
        "        use_spade=False,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        :param d_in input size\n",
        "        :param d_out output size\n",
        "        :param n_blocks number of Resnet blocks\n",
        "        :param d_latent latent size, added in each resnet block (0 = disable)\n",
        "        :param d_hidden hiddent dimension throughout network\n",
        "        :param beta softplus beta, 100 is reasonable; if <=0 uses ReLU activations instead\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        if d_in > 0:\n",
        "            self.lin_in = nn.Linear(d_in, d_hidden)\n",
        "            nn.init.constant_(self.lin_in.bias, 0.0)\n",
        "            nn.init.kaiming_normal_(self.lin_in.weight, a=0, mode=\"fan_in\")\n",
        "\n",
        "        self.lin_out = nn.Linear(d_hidden, d_out)\n",
        "        nn.init.constant_(self.lin_out.bias, 0.0)\n",
        "        nn.init.kaiming_normal_(self.lin_out.weight, a=0, mode=\"fan_in\")\n",
        "\n",
        "        self.n_blocks = n_blocks\n",
        "        self.d_latent = d_latent\n",
        "        self.d_in = d_in\n",
        "        self.d_out = d_out\n",
        "        self.d_hidden = d_hidden\n",
        "\n",
        "        self.combine_layer = combine_layer\n",
        "        self.combine_type = combine_type\n",
        "        self.use_spade = use_spade\n",
        "\n",
        "        self.blocks = nn.ModuleList(\n",
        "            [ResnetBlockFC(d_hidden, beta=beta) for i in range(n_blocks)]\n",
        "        )\n",
        "\n",
        "        if d_latent != 0:\n",
        "            n_lin_z = min(combine_layer, n_blocks)\n",
        "            self.lin_z = nn.ModuleList(\n",
        "                [nn.Linear(d_latent, d_hidden) for i in range(n_lin_z)]\n",
        "            )\n",
        "            for i in range(n_lin_z):\n",
        "                nn.init.constant_(self.lin_z[i].bias, 0.0)\n",
        "                nn.init.kaiming_normal_(self.lin_z[i].weight, a=0, mode=\"fan_in\")\n",
        "\n",
        "            if self.use_spade:\n",
        "                self.scale_z = nn.ModuleList(\n",
        "                    [nn.Linear(d_latent, d_hidden) for _ in range(n_lin_z)]\n",
        "                )\n",
        "                for i in range(n_lin_z):\n",
        "                    nn.init.constant_(self.scale_z[i].bias, 0.0)\n",
        "                    nn.init.kaiming_normal_(self.scale_z[i].weight, a=0, mode=\"fan_in\")\n",
        "\n",
        "        if beta > 0:\n",
        "            self.activation = nn.Softplus(beta=beta)\n",
        "        else:\n",
        "            self.activation = nn.ReLU()\n",
        "\n",
        "    def forward(self, zx, combine_inner_dims=(1,), combine_index=None, dim_size=None):\n",
        "        \"\"\"\n",
        "        :param zx (..., d_latent + d_in)\n",
        "        :param combine_inner_dims Combining dimensions for use with multiview inputs.\n",
        "        Tensor will be reshaped to (-1, combine_inner_dims, ...) and reduced using combine_type\n",
        "        on dim 1, at combine_layer\n",
        "        \"\"\"\n",
        "        with profiler.record_function(\"resnetfc_infer\"):\n",
        "            assert zx.size(-1) == self.d_latent + self.d_in\n",
        "            if self.d_latent > 0:\n",
        "                z = zx[..., : self.d_latent]\n",
        "                x = zx[..., self.d_latent :]\n",
        "            else:\n",
        "                x = zx\n",
        "            if self.d_in > 0:\n",
        "                x = self.lin_in(x)\n",
        "            else:\n",
        "                x = torch.zeros(self.d_hidden, device=zx.device)\n",
        "\n",
        "            for blkid in range(self.n_blocks):\n",
        "                if blkid == self.combine_layer:\n",
        "                    # The following implements camera frustum culling, requires torch_scatter\n",
        "                    #  if combine_index is not None:\n",
        "                    #      combine_type = (\n",
        "                    #          \"mean\"\n",
        "                    #          if self.combine_type == \"average\"\n",
        "                    #          else self.combine_type\n",
        "                    #      )\n",
        "                    #      if dim_size is not None:\n",
        "                    #          assert isinstance(dim_size, int)\n",
        "                    #      x = torch_scatter.scatter(\n",
        "                    #          x,\n",
        "                    #          combine_index,\n",
        "                    #          dim=0,\n",
        "                    #          dim_size=dim_size,\n",
        "                    #          reduce=combine_type,\n",
        "                    #      )\n",
        "                    #  else:\n",
        "                    x = combine_interleaved(\n",
        "                        x, combine_inner_dims, self.combine_type\n",
        "                    )\n",
        "\n",
        "                if self.d_latent > 0 and blkid < self.combine_layer:\n",
        "                    tz = self.lin_z[blkid](z)\n",
        "                    if self.use_spade:\n",
        "                        sz = self.scale_z[blkid](z)\n",
        "                        x = sz * x + tz\n",
        "                    else:\n",
        "                        x = x + tz\n",
        "\n",
        "                x = self.blocks[blkid](x)\n",
        "            out = self.lin_out(self.activation(x))\n",
        "            return out\n",
        "\n",
        "    @classmethod\n",
        "    def from_conf(cls, conf, d_in, **kwargs):\n",
        "        # PyHocon construction\n",
        "        return cls(\n",
        "            d_in,\n",
        "            n_blocks=conf.get_int(\"n_blocks\", 5),\n",
        "            d_hidden=conf.get_int(\"d_hidden\", 128),\n",
        "            beta=conf.get_float(\"beta\", 0.0),\n",
        "            combine_layer=conf.get_int(\"combine_layer\", 1000),\n",
        "            combine_type=conf.get_string(\"combine_type\", \"average\"),  # average | max\n",
        "            use_spade=conf.get_bool(\"use_spade\", False),\n",
        "            **kwargs\n",
        "        )"
      ],
      "metadata": {
        "id": "0MPk5BiSjSQh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "model/models.py"
      ],
      "metadata": {
        "id": "-e0SRpJEiQqT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.autograd.profiler as profiler\n",
        "import os\n",
        "import os.path as osp\n",
        "import warnings\n",
        "\n",
        "\n",
        "class PixelNeRFNet(torch.nn.Module):\n",
        "    def __init__(self, conf, stop_encoder_grad=False):\n",
        "        \"\"\"\n",
        "        :param conf PyHocon config subtree 'model'\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.encoder = make_encoder(conf[\"encoder\"])\n",
        "        self.use_encoder = conf.get_bool(\"use_encoder\", True)  # Image features?\n",
        "\n",
        "        self.use_xyz = conf.get_bool(\"use_xyz\", False)\n",
        "\n",
        "        assert self.use_encoder or self.use_xyz  # Must use some feature..\n",
        "\n",
        "        # Whether to shift z to align in canonical frame.\n",
        "        # So that all objects, regardless of camera distance to center, will\n",
        "        # be centered at z=0.\n",
        "        # Only makes sense in ShapeNet-type setting.\n",
        "        self.normalize_z = conf.get_bool(\"normalize_z\", True)\n",
        "\n",
        "        self.stop_encoder_grad = (\n",
        "            stop_encoder_grad  # Stop ConvNet gradient (freeze weights)\n",
        "        )\n",
        "        self.use_code = conf.get_bool(\"use_code\", False)  # Positional encoding\n",
        "        self.use_code_viewdirs = conf.get_bool(\n",
        "            \"use_code_viewdirs\", True\n",
        "        )  # Positional encoding applies to viewdirs\n",
        "\n",
        "        # Enable view directions\n",
        "        self.use_viewdirs = conf.get_bool(\"use_viewdirs\", False)\n",
        "\n",
        "        # Global image features?\n",
        "        self.use_global_encoder = conf.get_bool(\"use_global_encoder\", False)\n",
        "\n",
        "        d_latent = self.encoder.latent_size if self.use_encoder else 0\n",
        "        d_in = 3 if self.use_xyz else 1\n",
        "\n",
        "        if self.use_viewdirs and self.use_code_viewdirs:\n",
        "            # Apply positional encoding to viewdirs\n",
        "            d_in += 3\n",
        "        if self.use_code and d_in > 0:\n",
        "            # Positional encoding for x,y,z OR view z\n",
        "            self.code = PositionalEncoding.from_conf(conf[\"code\"], d_in=d_in)\n",
        "            d_in = self.code.d_out\n",
        "        if self.use_viewdirs and not self.use_code_viewdirs:\n",
        "            # Don't apply positional encoding to viewdirs (concat after encoded)\n",
        "            d_in += 3\n",
        "\n",
        "        if self.use_global_encoder:\n",
        "            # Global image feature\n",
        "            self.global_encoder = ImageEncoder.from_conf(conf[\"global_encoder\"])\n",
        "            self.global_latent_size = self.global_encoder.latent_size\n",
        "            d_latent += self.global_latent_size\n",
        "\n",
        "        d_out = 4\n",
        "\n",
        "        self.latent_size = self.encoder.latent_size\n",
        "        self.mlp_coarse = make_mlp(conf[\"mlp_coarse\"], d_in, d_latent, d_out=d_out)\n",
        "        self.mlp_fine = make_mlp(\n",
        "            conf[\"mlp_fine\"], d_in, d_latent, d_out=d_out, allow_empty=True\n",
        "        )\n",
        "        # Note: this is world -> camera, and bottom row is omitted\n",
        "        self.register_buffer(\"poses\", torch.empty(1, 3, 4), persistent=False)\n",
        "        self.register_buffer(\"image_shape\", torch.empty(2), persistent=False)\n",
        "\n",
        "        self.d_in = d_in\n",
        "        self.d_out = d_out\n",
        "        self.d_latent = d_latent\n",
        "        self.register_buffer(\"focal\", torch.empty(1, 2), persistent=False)\n",
        "        # Principal point\n",
        "        self.register_buffer(\"c\", torch.empty(1, 2), persistent=False)\n",
        "\n",
        "        self.num_objs = 0\n",
        "        self.num_views_per_obj = 1\n",
        "\n",
        "    def encode(self, images, poses, focal, z_bounds=None, c=None):\n",
        "        \"\"\"\n",
        "        :param images (NS, 3, H, W)\n",
        "        NS is number of input (aka source or reference) views\n",
        "        :param poses (NS, 4, 4)\n",
        "        :param focal focal length () or (2) or (NS) or (NS, 2) [fx, fy]\n",
        "        :param z_bounds ignored argument (used in the past)\n",
        "        :param c principal point None or () or (2) or (NS) or (NS, 2) [cx, cy],\n",
        "        default is center of image\n",
        "        \"\"\"\n",
        "        self.num_objs = images.size(0)\n",
        "        if len(images.shape) == 5:\n",
        "            assert len(poses.shape) == 4\n",
        "            assert poses.size(1) == images.size(\n",
        "                1\n",
        "            )  # Be consistent with NS = num input views\n",
        "            self.num_views_per_obj = images.size(1)\n",
        "            images = images.reshape(-1, *images.shape[2:])\n",
        "            poses = poses.reshape(-1, 4, 4)\n",
        "        else:\n",
        "            self.num_views_per_obj = 1\n",
        "\n",
        "        self.encoder(images)\n",
        "        rot = poses[:, :3, :3].transpose(1, 2)  # (B, 3, 3)\n",
        "        trans = -torch.bmm(rot, poses[:, :3, 3:])  # (B, 3, 1)\n",
        "        self.poses = torch.cat((rot, trans), dim=-1)  # (B, 3, 4)\n",
        "\n",
        "        self.image_shape[0] = images.shape[-1]\n",
        "        self.image_shape[1] = images.shape[-2]\n",
        "\n",
        "        # Handle various focal length/principal point formats\n",
        "        if len(focal.shape) == 0:\n",
        "            # Scalar: fx = fy = value for all views\n",
        "            focal = focal[None, None].repeat((1, 2))\n",
        "        elif len(focal.shape) == 1:\n",
        "            # Vector f: fx = fy = f_i *for view i*\n",
        "            # Length should match NS (or 1 for broadcast)\n",
        "            focal = focal.unsqueeze(-1).repeat((1, 2))\n",
        "        else:\n",
        "            focal = focal.clone()\n",
        "        self.focal = focal.float()\n",
        "        self.focal[..., 1] *= -1.0\n",
        "\n",
        "        if c is None:\n",
        "            # Default principal point is center of image\n",
        "            c = (self.image_shape * 0.5).unsqueeze(0)\n",
        "        elif len(c.shape) == 0:\n",
        "            # Scalar: cx = cy = value for all views\n",
        "            c = c[None, None].repeat((1, 2))\n",
        "        elif len(c.shape) == 1:\n",
        "            # Vector c: cx = cy = c_i *for view i*\n",
        "            c = c.unsqueeze(-1).repeat((1, 2))\n",
        "        self.c = c\n",
        "\n",
        "        if self.use_global_encoder:\n",
        "            self.global_encoder(images)\n",
        "\n",
        "    def forward(self, xyz, coarse=True, viewdirs=None, far=False):\n",
        "        \"\"\"\n",
        "        Predict (r, g, b, sigma) at world space points xyz.\n",
        "        Please call encode first!\n",
        "        :param xyz (SB, B, 3)\n",
        "        SB is batch of objects\n",
        "        B is batch of points (in rays)\n",
        "        NS is number of input views\n",
        "        :return (SB, B, 4) r g b sigma\n",
        "        \"\"\"\n",
        "        with profiler.record_function(\"model_inference\"):\n",
        "            SB, B, _ = xyz.shape\n",
        "            NS = self.num_views_per_obj\n",
        "\n",
        "            # Transform query points into the camera spaces of the input views\n",
        "            xyz = repeat_interleave(xyz, NS)  # (SB*NS, B, 3)\n",
        "            xyz_rot = torch.matmul(self.poses[:, None, :3, :3], xyz.unsqueeze(-1))[\n",
        "                ..., 0\n",
        "            ]\n",
        "            xyz = xyz_rot + self.poses[:, None, :3, 3]\n",
        "\n",
        "            if self.d_in > 0:\n",
        "                # * Encode the xyz coordinates\n",
        "                if self.use_xyz:\n",
        "                    if self.normalize_z:\n",
        "                        z_feature = xyz_rot.reshape(-1, 3)  # (SB*B, 3)\n",
        "                    else:\n",
        "                        z_feature = xyz.reshape(-1, 3)  # (SB*B, 3)\n",
        "                else:\n",
        "                    if self.normalize_z:\n",
        "                        z_feature = -xyz_rot[..., 2].reshape(-1, 1)  # (SB*B, 1)\n",
        "                    else:\n",
        "                        z_feature = -xyz[..., 2].reshape(-1, 1)  # (SB*B, 1)\n",
        "\n",
        "                if self.use_code and not self.use_code_viewdirs:\n",
        "                    # Positional encoding (no viewdirs)\n",
        "                    z_feature = self.code(z_feature)\n",
        "\n",
        "                if self.use_viewdirs:\n",
        "                    # * Encode the view directions\n",
        "                    assert viewdirs is not None\n",
        "                    # Viewdirs to input view space\n",
        "                    viewdirs = viewdirs.reshape(SB, B, 3, 1)\n",
        "                    viewdirs = repeat_interleave(viewdirs, NS)  # (SB*NS, B, 3, 1)\n",
        "                    viewdirs = torch.matmul(\n",
        "                        self.poses[:, None, :3, :3], viewdirs\n",
        "                    )  # (SB*NS, B, 3, 1)\n",
        "                    viewdirs = viewdirs.reshape(-1, 3)  # (SB*B, 3)\n",
        "                    z_feature = torch.cat(\n",
        "                        (z_feature, viewdirs), dim=1\n",
        "                    )  # (SB*B, 4 or 6)\n",
        "\n",
        "                if self.use_code and self.use_code_viewdirs:\n",
        "                    # Positional encoding (with viewdirs)\n",
        "                    z_feature = self.code(z_feature)\n",
        "\n",
        "                mlp_input = z_feature\n",
        "\n",
        "            if self.use_encoder:\n",
        "                # Grab encoder's latent code.\n",
        "                uv = -xyz[:, :, :2] / xyz[:, :, 2:]  # (SB, B, 2)\n",
        "                uv *= repeat_interleave(\n",
        "                    self.focal.unsqueeze(1), NS if self.focal.shape[0] > 1 else 1\n",
        "                )\n",
        "                uv += repeat_interleave(\n",
        "                    self.c.unsqueeze(1), NS if self.c.shape[0] > 1 else 1\n",
        "                )  # (SB*NS, B, 2)\n",
        "                latent = self.encoder.index(\n",
        "                    uv, None, self.image_shape\n",
        "                )  # (SB * NS, latent, B)\n",
        "\n",
        "                if self.stop_encoder_grad:\n",
        "                    latent = latent.detach()\n",
        "                latent = latent.transpose(1, 2).reshape(\n",
        "                    -1, self.latent_size\n",
        "                )  # (SB * NS * B, latent)\n",
        "\n",
        "                if self.d_in == 0:\n",
        "                    # z_feature not needed\n",
        "                    mlp_input = latent\n",
        "                else:\n",
        "                    mlp_input = torch.cat((latent, z_feature), dim=-1)\n",
        "\n",
        "            if self.use_global_encoder:\n",
        "                # Concat global latent code if enabled\n",
        "                global_latent = self.global_encoder.latent\n",
        "                assert mlp_input.shape[0] % global_latent.shape[0] == 0\n",
        "                num_repeats = mlp_input.shape[0] // global_latent.shape[0]\n",
        "                global_latent = repeat_interleave(global_latent, num_repeats)\n",
        "                mlp_input = torch.cat((global_latent, mlp_input), dim=-1)\n",
        "\n",
        "            # Camera frustum culling stuff, currently disabled\n",
        "            combine_index = None\n",
        "            dim_size = None\n",
        "\n",
        "            # Run main NeRF network\n",
        "            if coarse or self.mlp_fine is None:\n",
        "                mlp_output = self.mlp_coarse(\n",
        "                    mlp_input,\n",
        "                    combine_inner_dims=(self.num_views_per_obj, B),\n",
        "                    combine_index=combine_index,\n",
        "                    dim_size=dim_size,\n",
        "                )\n",
        "            else:\n",
        "                mlp_output = self.mlp_fine(\n",
        "                    mlp_input,\n",
        "                    combine_inner_dims=(self.num_views_per_obj, B),\n",
        "                    combine_index=combine_index,\n",
        "                    dim_size=dim_size,\n",
        "                )\n",
        "\n",
        "            # Interpret the output\n",
        "            mlp_output = mlp_output.reshape(-1, B, self.d_out)\n",
        "\n",
        "            rgb = mlp_output[..., :3]\n",
        "            sigma = mlp_output[..., 3:4]\n",
        "\n",
        "            output_list = [torch.sigmoid(rgb), torch.relu(sigma)]\n",
        "            output = torch.cat(output_list, dim=-1)\n",
        "            output = output.reshape(SB, B, -1)\n",
        "        return output\n",
        "\n",
        "    def load_weights(self, args, opt_init=False, strict=True, device=None):\n",
        "        \"\"\"\n",
        "        Helper for loading weights according to argparse arguments.\n",
        "        Your can put a checkpoint at checkpoints/<exp>/pixel_nerf_init to use as initialization.\n",
        "        :param opt_init if true, loads from init checkpoint instead of usual even when resuming\n",
        "        \"\"\"\n",
        "        # TODO: make backups\n",
        "        if opt_init and not args.resume:\n",
        "            return\n",
        "        ckpt_name = (\n",
        "            \"pixel_nerf_init\" if opt_init or not args.resume else \"pixel_nerf_latest\"\n",
        "        )\n",
        "        model_path = \"%s/%s/%s\" % (args.checkpoints_path, args.name, ckpt_name)\n",
        "\n",
        "        if device is None:\n",
        "            device = self.poses.device\n",
        "\n",
        "        if os.path.exists(model_path):\n",
        "            print(\"Load\", model_path)\n",
        "            self.load_state_dict(\n",
        "                torch.load(model_path, map_location=device), strict=strict\n",
        "            )\n",
        "        elif not opt_init:\n",
        "            warnings.warn(\n",
        "                (\n",
        "                    \"WARNING: {} does not exist, not loaded!! Model will be re-initialized.\\n\"\n",
        "                    + \"If you are trying to load a pretrained model, STOP since it's not in the right place. \"\n",
        "                    + \"If training, unless you are startin a new experiment, please remember to pass --resume.\"\n",
        "                ).format(model_path)\n",
        "            )\n",
        "        return self\n",
        "\n",
        "    def save_weights(self, args, opt_init=False):\n",
        "        \"\"\"\n",
        "        Helper for saving weights according to argparse arguments\n",
        "        :param opt_init if true, saves from init checkpoint instead of usual\n",
        "        \"\"\"\n",
        "        from shutil import copyfile\n",
        "\n",
        "        ckpt_name = \"pixel_nerf_init\" if opt_init else \"pixel_nerf_latest\"\n",
        "        backup_name = \"pixel_nerf_init_backup\" if opt_init else \"pixel_nerf_backup\"\n",
        "\n",
        "        ckpt_path = osp.join(args.checkpoints_path, args.name, ckpt_name)\n",
        "        ckpt_backup_path = osp.join(args.checkpoints_path, args.name, backup_name)\n",
        "\n",
        "        if osp.exists(ckpt_path):\n",
        "            copyfile(ckpt_path, ckpt_backup_path)\n",
        "        torch.save(self.state_dict(), ckpt_path)\n",
        "        return self"
      ],
      "metadata": {
        "id": "LNpyBXDSiQBm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "model/__init__.py"
      ],
      "metadata": {
        "id": "e_R8kZ314ZUm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wMlFFczI4HXA"
      },
      "outputs": [],
      "source": [
        "def make_model(conf, *args, **kwargs):\n",
        "    \"\"\" Placeholder to allow more model types \"\"\"\n",
        "    model_type = conf.get_string(\"type\", \"pixelnerf\")  # single\n",
        "    if model_type == \"pixelnerf\":\n",
        "        net = PixelNeRFNet(conf, *args, **kwargs)\n",
        "    else:\n",
        "        raise NotImplementedError(\"Unsupported model type\", model_type)\n",
        "    return net"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "model/loss.py"
      ],
      "metadata": {
        "id": "7HtgL8A_kFxx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "\n",
        "class AlphaLossNV2(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Implement Neural Volumes alpha loss 2\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, lambda_alpha, clamp_alpha, init_epoch, force_opaque=False):\n",
        "        super().__init__()\n",
        "        self.lambda_alpha = lambda_alpha\n",
        "        self.clamp_alpha = clamp_alpha\n",
        "        self.init_epoch = init_epoch\n",
        "        self.force_opaque = force_opaque\n",
        "        if force_opaque:\n",
        "            self.bceloss = torch.nn.BCELoss()\n",
        "        self.register_buffer(\n",
        "            \"epoch\", torch.tensor(0, dtype=torch.long), persistent=True\n",
        "        )\n",
        "\n",
        "    def sched_step(self, num=1):\n",
        "        self.epoch += num\n",
        "\n",
        "    def forward(self, alpha_fine):\n",
        "        if self.lambda_alpha > 0.0 and self.epoch.item() >= self.init_epoch:\n",
        "            alpha_fine = torch.clamp(alpha_fine, 0.01, 0.99)\n",
        "            if self.force_opaque:\n",
        "                alpha_loss = self.lambda_alpha * self.bceloss(\n",
        "                    alpha_fine, torch.ones_like(alpha_fine)\n",
        "                )\n",
        "            else:\n",
        "                alpha_loss = torch.log(alpha_fine) + torch.log(1.0 - alpha_fine)\n",
        "                alpha_loss = torch.clamp_min(alpha_loss, -self.clamp_alpha)\n",
        "                alpha_loss = self.lambda_alpha * alpha_loss.mean()\n",
        "        else:\n",
        "            alpha_loss = torch.zeros(1, device=alpha_fine.device)\n",
        "        return alpha_loss\n",
        "\n",
        "\n",
        "def get_alpha_loss(conf):\n",
        "    lambda_alpha = conf.get_float(\"lambda_alpha\")\n",
        "    clamp_alpha = conf.get_float(\"clamp_alpha\")\n",
        "    init_epoch = conf.get_int(\"init_epoch\")\n",
        "    force_opaque = conf.get_bool(\"force_opaque\", False)\n",
        "\n",
        "    return AlphaLossNV2(\n",
        "        lambda_alpha, clamp_alpha, init_epoch, force_opaque=force_opaque\n",
        "    )\n",
        "\n",
        "\n",
        "class RGBWithUncertainty(torch.nn.Module):\n",
        "    \"\"\"Implement the uncertainty loss from Kendall '17\"\"\"\n",
        "\n",
        "    def __init__(self, conf):\n",
        "        super().__init__()\n",
        "        self.element_loss = (\n",
        "            torch.nn.L1Loss(reduction=\"none\")\n",
        "            if conf.get_bool(\"use_l1\")\n",
        "            else torch.nn.MSELoss(reduction=\"none\")\n",
        "        )\n",
        "\n",
        "    def forward(self, outputs, targets, betas):\n",
        "        \"\"\"computes the error per output, weights each element by the log variance\n",
        "        outputs is B x 3, targets is B x 3, betas is B\"\"\"\n",
        "        weighted_element_err = (\n",
        "            torch.mean(self.element_loss(outputs, targets), -1) / betas\n",
        "        )\n",
        "        return torch.mean(weighted_element_err) + torch.mean(torch.log(betas))\n",
        "\n",
        "\n",
        "class RGBWithBackground(torch.nn.Module):\n",
        "    \"\"\"Implement the uncertainty loss from Kendall '17\"\"\"\n",
        "\n",
        "    def __init__(self, conf):\n",
        "        super().__init__()\n",
        "        self.element_loss = (\n",
        "            torch.nn.L1Loss(reduction=\"none\")\n",
        "            if conf.get_bool(\"use_l1\")\n",
        "            else torch.nn.MSELoss(reduction=\"none\")\n",
        "        )\n",
        "\n",
        "    def forward(self, outputs, targets, lambda_bg):\n",
        "        \"\"\"If we're using background, then the color is color_fg + lambda_bg * color_bg.\n",
        "        We want to weight the background rays less, while not putting all alpha on bg\"\"\"\n",
        "        weighted_element_err = torch.mean(self.element_loss(outputs, targets), -1) / (\n",
        "            1 + lambda_bg\n",
        "        )\n",
        "        return torch.mean(weighted_element_err) + torch.mean(torch.log(lambda_bg))\n",
        "\n",
        "\n",
        "def get_rgb_loss(conf, coarse=True, using_bg=False, reduction=\"mean\"):\n",
        "    if conf.get_bool(\"use_uncertainty\", False) and not coarse:\n",
        "        print(\"using loss with uncertainty\")\n",
        "        return RGBWithUncertainty(conf)\n",
        "    #     if using_bg:\n",
        "    #         print(\"using loss with background\")\n",
        "    #         return RGBWithBackground(conf)\n",
        "    print(\"using vanilla rgb loss\")\n",
        "    return (\n",
        "        torch.nn.L1Loss(reduction=reduction)\n",
        "        if conf.get_bool(\"use_l1\")\n",
        "        else torch.nn.MSELoss(reduction=reduction)\n",
        "    )"
      ],
      "metadata": {
        "id": "_jpwM_aH4cSK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "model/mlp.py"
      ],
      "metadata": {
        "id": "PhCgFfkzkLdQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class ImplicitNet(nn.Module):\n",
        "    \"\"\"\n",
        "    Represents a MLP;\n",
        "    Original code from IGR\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        d_in,\n",
        "        dims,\n",
        "        skip_in=(),\n",
        "        d_out=4,\n",
        "        geometric_init=True,\n",
        "        radius_init=0.3,\n",
        "        beta=0.0,\n",
        "        output_init_gain=2.0,\n",
        "        num_position_inputs=3,\n",
        "        sdf_scale=1.0,\n",
        "        dim_excludes_skip=False,\n",
        "        combine_layer=1000,\n",
        "        combine_type=\"average\",\n",
        "    ):\n",
        "        \"\"\"\n",
        "        :param d_in input size\n",
        "        :param dims dimensions of hidden layers. Num hidden layers == len(dims)\n",
        "        :param skip_in layers with skip connections from input (residual)\n",
        "        :param d_out output size\n",
        "        :param geometric_init if true, uses geometric initialization\n",
        "               (to SDF of sphere)\n",
        "        :param radius_init if geometric_init, then SDF sphere will have\n",
        "               this radius\n",
        "        :param beta softplus beta, 100 is reasonable; if <=0 uses ReLU activations instead\n",
        "        :param output_init_gain output layer normal std, only used for\n",
        "                                output dimension >= 1, when d_out >= 1\n",
        "        :param dim_excludes_skip if true, dimension sizes do not include skip\n",
        "        connections\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        dims = [d_in] + dims + [d_out]\n",
        "        if dim_excludes_skip:\n",
        "            for i in range(1, len(dims) - 1):\n",
        "                if i in skip_in:\n",
        "                    dims[i] += d_in\n",
        "\n",
        "        self.num_layers = len(dims)\n",
        "        self.skip_in = skip_in\n",
        "        self.dims = dims\n",
        "        self.combine_layer = combine_layer\n",
        "        self.combine_type = combine_type\n",
        "\n",
        "        for layer in range(0, self.num_layers - 1):\n",
        "            if layer + 1 in skip_in:\n",
        "                out_dim = dims[layer + 1] - d_in\n",
        "            else:\n",
        "                out_dim = dims[layer + 1]\n",
        "            lin = nn.Linear(dims[layer], out_dim)\n",
        "\n",
        "            # if true preform geometric initialization\n",
        "            if geometric_init:\n",
        "                if layer == self.num_layers - 2:\n",
        "                    # Note our geometric init is negated (compared to IDR)\n",
        "                    # since we are using the opposite SDF convention:\n",
        "                    # inside is +\n",
        "                    nn.init.normal_(\n",
        "                        lin.weight[0],\n",
        "                        mean=-np.sqrt(np.pi) / np.sqrt(dims[layer]) * sdf_scale,\n",
        "                        std=0.00001,\n",
        "                    )\n",
        "                    nn.init.constant_(lin.bias[0], radius_init)\n",
        "                    if d_out > 1:\n",
        "                        # More than SDF output\n",
        "                        nn.init.normal_(lin.weight[1:], mean=0.0, std=output_init_gain)\n",
        "                        nn.init.constant_(lin.bias[1:], 0.0)\n",
        "                else:\n",
        "                    nn.init.constant_(lin.bias, 0.0)\n",
        "                    nn.init.normal_(lin.weight, 0.0, np.sqrt(2) / np.sqrt(out_dim))\n",
        "                if d_in > num_position_inputs and (layer == 0 or layer in skip_in):\n",
        "                    # Special handling for input to allow positional encoding\n",
        "                    nn.init.constant_(lin.weight[:, -d_in + num_position_inputs :], 0.0)\n",
        "            else:\n",
        "                nn.init.constant_(lin.bias, 0.0)\n",
        "                nn.init.kaiming_normal_(lin.weight, a=0, mode=\"fan_in\")\n",
        "\n",
        "            setattr(self, \"lin\" + str(layer), lin)\n",
        "\n",
        "        if beta > 0:\n",
        "            self.activation = nn.Softplus(beta=beta)\n",
        "        else:\n",
        "            # Vanilla ReLU\n",
        "            self.activation = nn.ReLU()\n",
        "\n",
        "    def forward(self, x, combine_inner_dims=(1,)):\n",
        "        \"\"\"\n",
        "        :param x (..., d_in)\n",
        "        :param combine_inner_dims Combining dimensions for use with multiview inputs.\n",
        "        Tensor will be reshaped to (-1, combine_inner_dims, ...) and reduced using combine_type\n",
        "        on dim 1, at combine_layer\n",
        "        \"\"\"\n",
        "        x_init = x\n",
        "        for layer in range(0, self.num_layers - 1):\n",
        "            lin = getattr(self, \"lin\" + str(layer))\n",
        "\n",
        "            if layer == self.combine_layer:\n",
        "                x = combine_interleaved(x, combine_inner_dims, self.combine_type)\n",
        "                x_init = combine_interleaved(\n",
        "                    x_init, combine_inner_dims, self.combine_type\n",
        "                )\n",
        "\n",
        "            if layer < self.combine_layer and layer in self.skip_in:\n",
        "                x = torch.cat([x, x_init], -1) / np.sqrt(2)\n",
        "\n",
        "            x = lin(x)\n",
        "            if layer < self.num_layers - 2:\n",
        "                x = self.activation(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    @classmethod\n",
        "    def from_conf(cls, conf, d_in, **kwargs):\n",
        "        # PyHocon construction\n",
        "        return cls(\n",
        "            d_in,\n",
        "            conf.get_list(\"dims\"),\n",
        "            skip_in=conf.get_list(\"skip_in\"),\n",
        "            beta=conf.get_float(\"beta\", 0.0),\n",
        "            dim_excludes_skip=conf.get_bool(\"dim_excludes_skip\", False),\n",
        "            combine_layer=conf.get_int(\"combine_layer\", 1000),\n",
        "            combine_type=conf.get_string(\"combine_type\", \"average\"),  # average | max\n",
        "            **kwargs\n",
        "        )"
      ],
      "metadata": {
        "id": "HB6cWpEikJDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "model/model_util.py"
      ],
      "metadata": {
        "id": "ItbkxOuokh-_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_mlp(conf, d_in, d_latent=0, allow_empty=False, **kwargs):\n",
        "    mlp_type = conf.get_string(\"type\", \"mlp\")  # mlp | resnet\n",
        "    if mlp_type == \"mlp\":\n",
        "        net = ImplicitNet.from_conf(conf, d_in + d_latent, **kwargs)\n",
        "    elif mlp_type == \"resnet\":\n",
        "        net = ResnetFC.from_conf(conf, d_in, d_latent=d_latent, **kwargs)\n",
        "    elif mlp_type == \"empty\" and allow_empty:\n",
        "        net = None\n",
        "    else:\n",
        "        raise NotImplementedError(\"Unsupported MLP type\")\n",
        "    return net\n",
        "\n",
        "\n",
        "def make_encoder(conf, **kwargs):\n",
        "    enc_type = conf.get_string(\"type\", \"spatial\")  # spatial | global\n",
        "    if enc_type == \"spatial\":\n",
        "        net = SpatialEncoder.from_conf(conf, **kwargs)\n",
        "    elif enc_type == \"global\":\n",
        "        net = ImageEncoder.from_conf(conf, **kwargs)\n",
        "    else:\n",
        "        raise NotImplementedError(\"Unsupported encoder type\")\n",
        "    return net"
      ],
      "metadata": {
        "id": "NzGbysxHkPwQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "src/render/nerf.py"
      ],
      "metadata": {
        "id": "olO6A6jvlUom"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.autograd.profiler as profiler\n",
        "from torch.nn import DataParallel\n",
        "from dotmap import DotMap\n",
        "\n",
        "\n",
        "class _RenderWrapper(torch.nn.Module):\n",
        "    def __init__(self, net, renderer, simple_output):\n",
        "        super().__init__()\n",
        "        self.net = net\n",
        "        self.renderer = renderer\n",
        "        self.simple_output = simple_output\n",
        "\n",
        "    def forward(self, rays, want_weights=False):\n",
        "        if rays.shape[0] == 0:\n",
        "            return (\n",
        "                torch.zeros(0, 3, device=rays.device),\n",
        "                torch.zeros(0, device=rays.device),\n",
        "            )\n",
        "\n",
        "        outputs = self.renderer(\n",
        "            self.net, rays, want_weights=want_weights and not self.simple_output\n",
        "        )\n",
        "        if self.simple_output:\n",
        "            if self.renderer.using_fine:\n",
        "                rgb = outputs.fine.rgb\n",
        "                depth = outputs.fine.depth\n",
        "            else:\n",
        "                rgb = outputs.coarse.rgb\n",
        "                depth = outputs.coarse.depth\n",
        "            return rgb, depth\n",
        "        else:\n",
        "            # Make DotMap to dict to support DataParallel\n",
        "            return outputs.toDict()\n",
        "\n",
        "\n",
        "class NeRFRenderer(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    NeRF differentiable renderer\n",
        "    :param n_coarse number of coarse (binned uniform) samples\n",
        "    :param n_fine number of fine (importance) samples\n",
        "    :param n_fine_depth number of expected depth samples\n",
        "    :param noise_std noise to add to sigma. We do not use it\n",
        "    :param depth_std noise for depth samples\n",
        "    :param eval_batch_size ray batch size for evaluation\n",
        "    :param white_bkgd if true, background color is white; else black\n",
        "    :param lindisp if to use samples linear in disparity instead of distance\n",
        "    :param sched ray sampling schedule. list containing 3 lists of equal length.\n",
        "    sched[0] is list of iteration numbers,\n",
        "    sched[1] is list of coarse sample numbers,\n",
        "    sched[2] is list of fine sample numbers\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_coarse=128,\n",
        "        n_fine=0,\n",
        "        n_fine_depth=0,\n",
        "        noise_std=0.0,\n",
        "        depth_std=0.01,\n",
        "        eval_batch_size=100000,\n",
        "        white_bkgd=False,\n",
        "        lindisp=False,\n",
        "        sched=None,  # ray sampling schedule for coarse and fine rays\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.n_coarse = n_coarse\n",
        "        self.n_fine = n_fine\n",
        "        self.n_fine_depth = n_fine_depth\n",
        "\n",
        "        self.noise_std = noise_std\n",
        "        self.depth_std = depth_std\n",
        "\n",
        "        self.eval_batch_size = eval_batch_size\n",
        "        self.white_bkgd = white_bkgd\n",
        "        self.lindisp = lindisp\n",
        "        if lindisp:\n",
        "            print(\"Using linear displacement rays\")\n",
        "        self.using_fine = n_fine > 0\n",
        "        self.sched = sched\n",
        "        if sched is not None and len(sched) == 0:\n",
        "            self.sched = None\n",
        "        self.register_buffer(\n",
        "            \"iter_idx\", torch.tensor(0, dtype=torch.long), persistent=True\n",
        "        )\n",
        "        self.register_buffer(\n",
        "            \"last_sched\", torch.tensor(0, dtype=torch.long), persistent=True\n",
        "        )\n",
        "\n",
        "    def sample_coarse(self, rays):\n",
        "        \"\"\"\n",
        "        Stratified sampling. Note this is different from original NeRF slightly.\n",
        "        :param rays ray [origins (3), directions (3), near (1), far (1)] (B, 8)\n",
        "        :return (B, Kc)\n",
        "        \"\"\"\n",
        "        device = rays.device\n",
        "        near, far = rays[:, -2:-1], rays[:, -1:]  # (B, 1)\n",
        "\n",
        "        step = 1.0 / self.n_coarse\n",
        "        B = rays.shape[0]\n",
        "        z_steps = torch.linspace(0, 1 - step, self.n_coarse, device=device)  # (Kc)\n",
        "        z_steps = z_steps.unsqueeze(0).repeat(B, 1)  # (B, Kc)\n",
        "        z_steps += torch.rand_like(z_steps) * step\n",
        "        if not self.lindisp:  # Use linear sampling in depth space\n",
        "            return near * (1 - z_steps) + far * z_steps  # (B, Kf)\n",
        "        else:  # Use linear sampling in disparity space\n",
        "            return 1 / (1 / near * (1 - z_steps) + 1 / far * z_steps)  # (B, Kf)\n",
        "\n",
        "        # Use linear sampling in depth space\n",
        "        return near * (1 - z_steps) + far * z_steps  # (B, Kc)\n",
        "\n",
        "    def sample_fine(self, rays, weights):\n",
        "        \"\"\"\n",
        "        Weighted stratified (importance) sample\n",
        "        :param rays ray [origins (3), directions (3), near (1), far (1)] (B, 8)\n",
        "        :param weights (B, Kc)\n",
        "        :return (B, Kf-Kfd)\n",
        "        \"\"\"\n",
        "        device = rays.device\n",
        "        B = rays.shape[0]\n",
        "\n",
        "        weights = weights.detach() + 1e-5  # Prevent division by zero\n",
        "        pdf = weights / torch.sum(weights, -1, keepdim=True)  # (B, Kc)\n",
        "        cdf = torch.cumsum(pdf, -1)  # (B, Kc)\n",
        "        cdf = torch.cat([torch.zeros_like(cdf[:, :1]), cdf], -1)  # (B, Kc+1)\n",
        "\n",
        "        u = torch.rand(\n",
        "            B, self.n_fine - self.n_fine_depth, dtype=torch.float32, device=device\n",
        "        )  # (B, Kf)\n",
        "        inds = torch.searchsorted(cdf, u, right=True).float() - 1.0  # (B, Kf)\n",
        "        inds = torch.clamp_min(inds, 0.0)\n",
        "\n",
        "        z_steps = (inds + torch.rand_like(inds)) / self.n_coarse  # (B, Kf)\n",
        "\n",
        "        near, far = rays[:, -2:-1], rays[:, -1:]  # (B, 1)\n",
        "        if not self.lindisp:  # Use linear sampling in depth space\n",
        "            z_samp = near * (1 - z_steps) + far * z_steps  # (B, Kf)\n",
        "        else:  # Use linear sampling in disparity space\n",
        "            z_samp = 1 / (1 / near * (1 - z_steps) + 1 / far * z_steps)  # (B, Kf)\n",
        "        return z_samp\n",
        "\n",
        "    def sample_fine_depth(self, rays, depth):\n",
        "        \"\"\"\n",
        "        Sample around specified depth\n",
        "        :param rays ray [origins (3), directions (3), near (1), far (1)] (B, 8)\n",
        "        :param depth (B)\n",
        "        :return (B, Kfd)\n",
        "        \"\"\"\n",
        "        z_samp = depth.unsqueeze(1).repeat((1, self.n_fine_depth))\n",
        "        z_samp += torch.randn_like(z_samp) * self.depth_std\n",
        "        # Clamp does not support tensor bounds\n",
        "        z_samp = torch.max(torch.min(z_samp, rays[:, -1:]), rays[:, -2:-1])\n",
        "        return z_samp\n",
        "\n",
        "    def composite(self, model, rays, z_samp, coarse=True, sb=0):\n",
        "        \"\"\"\n",
        "        Render RGB and depth for each ray using NeRF alpha-compositing formula,\n",
        "        given sampled positions along each ray (see sample_*)\n",
        "        :param model should return (B, (r, g, b, sigma)) when called with (B, (x, y, z))\n",
        "        should also support 'coarse' boolean argument\n",
        "        :param rays ray [origins (3), directions (3), near (1), far (1)] (B, 8)\n",
        "        :param z_samp z positions sampled for each ray (B, K)\n",
        "        :param coarse whether to evaluate using coarse NeRF\n",
        "        :param sb super-batch dimension; 0 = disable\n",
        "        :return weights (B, K), rgb (B, 3), depth (B)\n",
        "        \"\"\"\n",
        "        with profiler.record_function(\"renderer_composite\"):\n",
        "            B, K = z_samp.shape\n",
        "\n",
        "            deltas = z_samp[:, 1:] - z_samp[:, :-1]  # (B, K-1)\n",
        "            #  if far:\n",
        "            #      delta_inf = 1e10 * torch.ones_like(deltas[:, :1])  # infty (B, 1)\n",
        "            delta_inf = rays[:, -1:] - z_samp[:, -1:]\n",
        "            deltas = torch.cat([deltas, delta_inf], -1)  # (B, K)\n",
        "\n",
        "            # (B, K, 3)\n",
        "            points = rays[:, None, :3] + z_samp.unsqueeze(2) * rays[:, None, 3:6]\n",
        "            points = points.reshape(-1, 3)  # (B*K, 3)\n",
        "\n",
        "            use_viewdirs = hasattr(model, \"use_viewdirs\") and model.use_viewdirs\n",
        "\n",
        "            val_all = []\n",
        "            if sb > 0:\n",
        "                points = points.reshape(\n",
        "                    sb, -1, 3\n",
        "                )  # (SB, B'*K, 3) B' is real ray batch size\n",
        "                eval_batch_size = (self.eval_batch_size - 1) // sb + 1\n",
        "                eval_batch_dim = 1\n",
        "            else:\n",
        "                eval_batch_size = self.eval_batch_size\n",
        "                eval_batch_dim = 0\n",
        "\n",
        "            split_points = torch.split(points, eval_batch_size, dim=eval_batch_dim)\n",
        "            if use_viewdirs:\n",
        "                dim1 = K\n",
        "                viewdirs = rays[:, None, 3:6].expand(-1, dim1, -1)  # (B, K, 3)\n",
        "                if sb > 0:\n",
        "                    viewdirs = viewdirs.reshape(sb, -1, 3)  # (SB, B'*K, 3)\n",
        "                else:\n",
        "                    viewdirs = viewdirs.reshape(-1, 3)  # (B*K, 3)\n",
        "                split_viewdirs = torch.split(\n",
        "                    viewdirs, eval_batch_size, dim=eval_batch_dim\n",
        "                )\n",
        "                for pnts, dirs in zip(split_points, split_viewdirs):\n",
        "                    val_all.append(model(pnts, coarse=coarse, viewdirs=dirs))\n",
        "            else:\n",
        "                for pnts in split_points:\n",
        "                    val_all.append(model(pnts, coarse=coarse))\n",
        "            points = None\n",
        "            viewdirs = None\n",
        "            # (B*K, 4) OR (SB, B'*K, 4)\n",
        "            out = torch.cat(val_all, dim=eval_batch_dim)\n",
        "            out = out.reshape(B, K, -1)  # (B, K, 4 or 5)\n",
        "\n",
        "            rgbs = out[..., :3]  # (B, K, 3)\n",
        "            sigmas = out[..., 3]  # (B, K)\n",
        "            if self.training and self.noise_std > 0.0:\n",
        "                sigmas = sigmas + torch.randn_like(sigmas) * self.noise_std\n",
        "\n",
        "            alphas = 1 - torch.exp(-deltas * torch.relu(sigmas))  # (B, K)\n",
        "            deltas = None\n",
        "            sigmas = None\n",
        "            alphas_shifted = torch.cat(\n",
        "                [torch.ones_like(alphas[:, :1]), 1 - alphas + 1e-10], -1\n",
        "            )  # (B, K+1) = [1, a1, a2, ...]\n",
        "            T = torch.cumprod(alphas_shifted, -1)  # (B)\n",
        "            weights = alphas * T[:, :-1]  # (B, K)\n",
        "            alphas = None\n",
        "            alphas_shifted = None\n",
        "\n",
        "            rgb_final = torch.sum(weights.unsqueeze(-1) * rgbs, -2)  # (B, 3)\n",
        "            depth_final = torch.sum(weights * z_samp, -1)  # (B)\n",
        "            if self.white_bkgd:\n",
        "                # White background\n",
        "                pix_alpha = weights.sum(dim=1)  # (B), pixel alpha\n",
        "                rgb_final = rgb_final + 1 - pix_alpha.unsqueeze(-1)  # (B, 3)\n",
        "            return (\n",
        "                weights,\n",
        "                rgb_final,\n",
        "                depth_final,\n",
        "            )\n",
        "\n",
        "    def forward(\n",
        "        self, model, rays, want_weights=False,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        :model nerf model, should return (SB, B, (r, g, b, sigma))\n",
        "        when called with (SB, B, (x, y, z)), for multi-object:\n",
        "        SB = 'super-batch' = size of object batch,\n",
        "        B  = size of per-object ray batch.\n",
        "        Should also support 'coarse' boolean argument for coarse NeRF.\n",
        "        :param rays ray spec [origins (3), directions (3), near (1), far (1)] (SB, B, 8)\n",
        "        :param want_weights if true, returns compositing weights (SB, B, K)\n",
        "        :return render dict\n",
        "        \"\"\"\n",
        "        with profiler.record_function(\"renderer_forward\"):\n",
        "            if self.sched is not None and self.last_sched.item() > 0:\n",
        "                self.n_coarse = self.sched[1][self.last_sched.item() - 1]\n",
        "                self.n_fine = self.sched[2][self.last_sched.item() - 1]\n",
        "\n",
        "            assert len(rays.shape) == 3\n",
        "            superbatch_size = rays.shape[0]\n",
        "            rays = rays.reshape(-1, 8)  # (SB * B, 8)\n",
        "\n",
        "            z_coarse = self.sample_coarse(rays)  # (B, Kc)\n",
        "            coarse_composite = self.composite(\n",
        "                model, rays, z_coarse, coarse=True, sb=superbatch_size,\n",
        "            )\n",
        "\n",
        "            outputs = DotMap(\n",
        "                coarse=self._format_outputs(\n",
        "                    coarse_composite, superbatch_size, want_weights=want_weights,\n",
        "                ),\n",
        "            )\n",
        "\n",
        "            if self.using_fine:\n",
        "                all_samps = [z_coarse]\n",
        "                if self.n_fine - self.n_fine_depth > 0:\n",
        "                    all_samps.append(\n",
        "                        self.sample_fine(rays, coarse_composite[0].detach())\n",
        "                    )  # (B, Kf - Kfd)\n",
        "                if self.n_fine_depth > 0:\n",
        "                    all_samps.append(\n",
        "                        self.sample_fine_depth(rays, coarse_composite[2])\n",
        "                    )  # (B, Kfd)\n",
        "                z_combine = torch.cat(all_samps, dim=-1)  # (B, Kc + Kf)\n",
        "                z_combine_sorted, argsort = torch.sort(z_combine, dim=-1)\n",
        "                fine_composite = self.composite(\n",
        "                    model, rays, z_combine_sorted, coarse=False, sb=superbatch_size,\n",
        "                )\n",
        "                outputs.fine = self._format_outputs(\n",
        "                    fine_composite, superbatch_size, want_weights=want_weights,\n",
        "                )\n",
        "\n",
        "            return outputs\n",
        "\n",
        "    def _format_outputs(\n",
        "        self, rendered_outputs, superbatch_size, want_weights=False,\n",
        "    ):\n",
        "        weights, rgb, depth = rendered_outputs\n",
        "        if superbatch_size > 0:\n",
        "            rgb = rgb.reshape(superbatch_size, -1, 3)\n",
        "            depth = depth.reshape(superbatch_size, -1)\n",
        "            weights = weights.reshape(superbatch_size, -1, weights.shape[-1])\n",
        "        ret_dict = DotMap(rgb=rgb, depth=depth)\n",
        "        if want_weights:\n",
        "            ret_dict.weights = weights\n",
        "        return ret_dict\n",
        "\n",
        "    def sched_step(self, steps=1):\n",
        "        \"\"\"\n",
        "        Called each training iteration to update sample numbers\n",
        "        according to schedule\n",
        "        \"\"\"\n",
        "        if self.sched is None:\n",
        "            return\n",
        "        self.iter_idx += steps\n",
        "        while (\n",
        "            self.last_sched.item() < len(self.sched[0])\n",
        "            and self.iter_idx.item() >= self.sched[0][self.last_sched.item()]\n",
        "        ):\n",
        "            self.n_coarse = self.sched[1][self.last_sched.item()]\n",
        "            self.n_fine = self.sched[2][self.last_sched.item()]\n",
        "            print(\n",
        "                \"INFO: NeRF sampling resolution changed on schedule ==> c\",\n",
        "                self.n_coarse,\n",
        "                \"f\",\n",
        "                self.n_fine,\n",
        "            )\n",
        "            self.last_sched += 1\n",
        "\n",
        "    @classmethod\n",
        "    def from_conf(cls, conf, white_bkgd=False, lindisp=False, eval_batch_size=100000):\n",
        "        return cls(\n",
        "            conf.get_int(\"n_coarse\", 128),\n",
        "            conf.get_int(\"n_fine\", 0),\n",
        "            n_fine_depth=conf.get_int(\"n_fine_depth\", 0),\n",
        "            noise_std=conf.get_float(\"noise_std\", 0.0),\n",
        "            depth_std=conf.get_float(\"depth_std\", 0.01),\n",
        "            white_bkgd=conf.get_float(\"white_bkgd\", white_bkgd),\n",
        "            lindisp=lindisp,\n",
        "            eval_batch_size=conf.get_int(\"eval_batch_size\", eval_batch_size),\n",
        "            sched=conf.get_list(\"sched\", None),\n",
        "        )\n",
        "\n",
        "    def bind_parallel(self, net, gpus=None, simple_output=False):\n",
        "        \"\"\"\n",
        "        Returns a wrapper module compatible with DataParallel.\n",
        "        Specifically, it renders rays with this renderer\n",
        "        but always using the given network instance.\n",
        "        Specify a list of GPU ids in 'gpus' to apply DataParallel automatically.\n",
        "        :param net A PixelNeRF network\n",
        "        :param gpus list of GPU ids to parallize to. If length is 1,\n",
        "        does not parallelize\n",
        "        :param simple_output only returns rendered (rgb, depth) instead of the\n",
        "        full render output map. Saves data tranfer cost.\n",
        "        :return torch module\n",
        "        \"\"\"\n",
        "        wrapped = _RenderWrapper(net, self, simple_output=simple_output)\n",
        "        if gpus is not None and len(gpus) > 1:\n",
        "            print(\"Using multi-GPU\", gpus)\n",
        "            wrapped = torch.nn.DataParallel(wrapped, gpus, dim=1)\n",
        "        return wrapped"
      ],
      "metadata": {
        "id": "a7nHDpd4kZwX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "data/DVRDataset.py"
      ],
      "metadata": {
        "id": "07_Hcr8zmDQv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import glob\n",
        "import imageio\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "class DVRDataset(torch.utils.data.Dataset):\n",
        "    \"\"\"\n",
        "    Dataset from DVR (Niemeyer et al. 2020)\n",
        "    Provides 3D-R2N2 and NMR renderings\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        path,\n",
        "        stage=\"train\",\n",
        "        list_prefix=\"softras_\",\n",
        "        image_size=None,\n",
        "        sub_format=\"shapenet\",\n",
        "        scale_focal=True,\n",
        "        max_imgs=100000,\n",
        "        z_near=1.2,\n",
        "        z_far=4.0,\n",
        "        skip_step=None,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        :param path dataset root path, contains metadata.yml\n",
        "        :param stage train | val | test\n",
        "        :param list_prefix prefix for split lists: <list_prefix>[train, val, test].lst\n",
        "        :param image_size result image size (resizes if different); None to keep original size\n",
        "        :param sub_format shapenet | dtu dataset sub-type.\n",
        "        :param scale_focal if true, assume focal length is specified for\n",
        "        image of side length 2 instead of actual image size. This is used\n",
        "        where image coordinates are placed in [-1, 1].\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.base_path = path\n",
        "        assert os.path.exists(self.base_path)\n",
        "\n",
        "        cats = [x for x in glob.glob(os.path.join(path, \"*\")) if os.path.isdir(x)]\n",
        "\n",
        "        if stage == \"train\":\n",
        "            file_lists = [os.path.join(x, list_prefix + \"train.lst\") for x in cats]\n",
        "        elif stage == \"val\":\n",
        "            file_lists = [os.path.join(x, list_prefix + \"val.lst\") for x in cats]\n",
        "        elif stage == \"test\":\n",
        "            file_lists = [os.path.join(x, list_prefix + \"test.lst\") for x in cats]\n",
        "\n",
        "        all_objs = []\n",
        "        for file_list in file_lists:\n",
        "            if not os.path.exists(file_list):\n",
        "                continue\n",
        "            base_dir = os.path.dirname(file_list)\n",
        "            cat = os.path.basename(base_dir)\n",
        "            with open(file_list, \"r\") as f:\n",
        "                objs = [(cat, os.path.join(base_dir, x.strip())) for x in f.readlines()]\n",
        "            all_objs.extend(objs)\n",
        "\n",
        "        self.all_objs = all_objs\n",
        "        self.stage = stage\n",
        "\n",
        "        self.image_to_tensor = get_image_to_tensor_balanced()\n",
        "        self.mask_to_tensor = get_mask_to_tensor()\n",
        "        print(\n",
        "            \"Loading DVR dataset\",\n",
        "            self.base_path,\n",
        "            \"stage\",\n",
        "            stage,\n",
        "            len(self.all_objs),\n",
        "            \"objs\",\n",
        "            \"type:\",\n",
        "            sub_format,\n",
        "        )\n",
        "\n",
        "        self.image_size = image_size\n",
        "        if sub_format == \"dtu\":\n",
        "            self._coord_trans_world = torch.tensor(\n",
        "                [[1, 0, 0, 0], [0, -1, 0, 0], [0, 0, -1, 0], [0, 0, 0, 1]],\n",
        "                dtype=torch.float32,\n",
        "            )\n",
        "            self._coord_trans_cam = torch.tensor(\n",
        "                [[1, 0, 0, 0], [0, -1, 0, 0], [0, 0, -1, 0], [0, 0, 0, 1]],\n",
        "                dtype=torch.float32,\n",
        "            )\n",
        "        else:\n",
        "            self._coord_trans_world = torch.tensor(\n",
        "                [[1, 0, 0, 0], [0, 0, -1, 0], [0, 1, 0, 0], [0, 0, 0, 1]],\n",
        "                dtype=torch.float32,\n",
        "            )\n",
        "            self._coord_trans_cam = torch.tensor(\n",
        "                [[1, 0, 0, 0], [0, -1, 0, 0], [0, 0, -1, 0], [0, 0, 0, 1]],\n",
        "                dtype=torch.float32,\n",
        "            )\n",
        "        self.sub_format = sub_format\n",
        "        self.scale_focal = scale_focal\n",
        "        self.max_imgs = max_imgs\n",
        "\n",
        "        self.z_near = z_near\n",
        "        self.z_far = z_far\n",
        "        self.lindisp = False\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.all_objs)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        cat, root_dir = self.all_objs[index]\n",
        "\n",
        "        rgb_paths = [\n",
        "            x\n",
        "            for x in glob.glob(os.path.join(root_dir, \"image\", \"*\"))\n",
        "            if (x.endswith(\".jpg\") or x.endswith(\".png\"))\n",
        "        ]\n",
        "        rgb_paths = sorted(rgb_paths)\n",
        "        mask_paths = sorted(glob.glob(os.path.join(root_dir, \"mask\", \"*.png\")))\n",
        "        if len(mask_paths) == 0:\n",
        "            mask_paths = [None] * len(rgb_paths)\n",
        "\n",
        "        if len(rgb_paths) <= self.max_imgs:\n",
        "            sel_indices = np.arange(len(rgb_paths))\n",
        "        else:\n",
        "            sel_indices = np.random.choice(len(rgb_paths), self.max_imgs, replace=False)\n",
        "            rgb_paths = [rgb_paths[i] for i in sel_indices]\n",
        "            mask_paths = [mask_paths[i] for i in sel_indices]\n",
        "\n",
        "        cam_path = os.path.join(root_dir, \"cameras.npz\")\n",
        "        all_cam = np.load(cam_path)\n",
        "\n",
        "        all_imgs = []\n",
        "        all_poses = []\n",
        "        all_masks = []\n",
        "        all_bboxes = []\n",
        "        focal = None\n",
        "        if self.sub_format != \"shapenet\":\n",
        "            # Prepare to average intrinsics over images\n",
        "            fx, fy, cx, cy = 0.0, 0.0, 0.0, 0.0\n",
        "\n",
        "        for idx, (rgb_path, mask_path) in enumerate(zip(rgb_paths, mask_paths)):\n",
        "            i = sel_indices[idx]\n",
        "            img = imageio.imread(rgb_path)[..., :3]\n",
        "            if self.scale_focal:\n",
        "                x_scale = img.shape[1] / 2.0\n",
        "                y_scale = img.shape[0] / 2.0\n",
        "                xy_delta = 1.0\n",
        "            else:\n",
        "                x_scale = y_scale = 1.0\n",
        "                xy_delta = 0.0\n",
        "\n",
        "            if mask_path is not None:\n",
        "                mask = imageio.imread(mask_path)\n",
        "                if len(mask.shape) == 2:\n",
        "                    mask = mask[..., None]\n",
        "                mask = mask[..., :1]\n",
        "            if self.sub_format == \"dtu\":\n",
        "                # Decompose projection matrix\n",
        "                # DVR uses slightly different format for DTU set\n",
        "                P = all_cam[\"world_mat_\" + str(i)]\n",
        "                P = P[:3]\n",
        "\n",
        "                K, R, t = cv2.decomposeProjectionMatrix(P)[:3]\n",
        "                K = K / K[2, 2]\n",
        "\n",
        "                pose = np.eye(4, dtype=np.float32)\n",
        "                pose[:3, :3] = R.transpose()\n",
        "                pose[:3, 3] = (t[:3] / t[3])[:, 0]\n",
        "\n",
        "                scale_mtx = all_cam.get(\"scale_mat_\" + str(i))\n",
        "                if scale_mtx is not None:\n",
        "                    norm_trans = scale_mtx[:3, 3:]\n",
        "                    norm_scale = np.diagonal(scale_mtx[:3, :3])[..., None]\n",
        "\n",
        "                    pose[:3, 3:] -= norm_trans\n",
        "                    pose[:3, 3:] /= norm_scale\n",
        "\n",
        "                fx += torch.tensor(K[0, 0]) * x_scale\n",
        "                fy += torch.tensor(K[1, 1]) * y_scale\n",
        "                cx += (torch.tensor(K[0, 2]) + xy_delta) * x_scale\n",
        "                cy += (torch.tensor(K[1, 2]) + xy_delta) * y_scale\n",
        "            else:\n",
        "                # ShapeNet\n",
        "                wmat_inv_key = \"world_mat_inv_\" + str(i)\n",
        "                wmat_key = \"world_mat_\" + str(i)\n",
        "                if wmat_inv_key in all_cam:\n",
        "                    extr_inv_mtx = all_cam[wmat_inv_key]\n",
        "                else:\n",
        "                    extr_inv_mtx = all_cam[wmat_key]\n",
        "                    if extr_inv_mtx.shape[0] == 3:\n",
        "                        extr_inv_mtx = np.vstack((extr_inv_mtx, np.array([0, 0, 0, 1])))\n",
        "                    extr_inv_mtx = np.linalg.inv(extr_inv_mtx)\n",
        "\n",
        "                intr_mtx = all_cam[\"camera_mat_\" + str(i)]\n",
        "                fx, fy = intr_mtx[0, 0], intr_mtx[1, 1]\n",
        "                assert abs(fx - fy) < 1e-9\n",
        "                fx = fx * x_scale\n",
        "                if focal is None:\n",
        "                    focal = fx\n",
        "                else:\n",
        "                    assert abs(fx - focal) < 1e-5\n",
        "                pose = extr_inv_mtx\n",
        "\n",
        "            pose = (\n",
        "                self._coord_trans_world\n",
        "                @ torch.tensor(pose, dtype=torch.float32)\n",
        "                @ self._coord_trans_cam\n",
        "            )\n",
        "\n",
        "            img_tensor = self.image_to_tensor(img)\n",
        "            if mask_path is not None:\n",
        "                mask_tensor = self.mask_to_tensor(mask)\n",
        "\n",
        "                rows = np.any(mask, axis=1)\n",
        "                cols = np.any(mask, axis=0)\n",
        "                rnz = np.where(rows)[0]\n",
        "                cnz = np.where(cols)[0]\n",
        "                if len(rnz) == 0:\n",
        "                    raise RuntimeError(\n",
        "                        \"ERROR: Bad image at\", rgb_path, \"please investigate!\"\n",
        "                    )\n",
        "                rmin, rmax = rnz[[0, -1]]\n",
        "                cmin, cmax = cnz[[0, -1]]\n",
        "                bbox = torch.tensor([cmin, rmin, cmax, rmax], dtype=torch.float32)\n",
        "                all_masks.append(mask_tensor)\n",
        "                all_bboxes.append(bbox)\n",
        "\n",
        "            all_imgs.append(img_tensor)\n",
        "            all_poses.append(pose)\n",
        "\n",
        "        if self.sub_format != \"shapenet\":\n",
        "            fx /= len(rgb_paths)\n",
        "            fy /= len(rgb_paths)\n",
        "            cx /= len(rgb_paths)\n",
        "            cy /= len(rgb_paths)\n",
        "            focal = torch.tensor((fx, fy), dtype=torch.float32)\n",
        "            c = torch.tensor((cx, cy), dtype=torch.float32)\n",
        "            all_bboxes = None\n",
        "        elif mask_path is not None:\n",
        "            all_bboxes = torch.stack(all_bboxes)\n",
        "\n",
        "        all_imgs = torch.stack(all_imgs)\n",
        "        all_poses = torch.stack(all_poses)\n",
        "        if len(all_masks) > 0:\n",
        "            all_masks = torch.stack(all_masks)\n",
        "        else:\n",
        "            all_masks = None\n",
        "\n",
        "        if self.image_size is not None and all_imgs.shape[-2:] != self.image_size:\n",
        "            scale = self.image_size[0] / all_imgs.shape[-2]\n",
        "            focal *= scale\n",
        "            if self.sub_format != \"shapenet\":\n",
        "                c *= scale\n",
        "            elif mask_path is not None:\n",
        "                all_bboxes *= scale\n",
        "\n",
        "            all_imgs = F.interpolate(all_imgs, size=self.image_size, mode=\"area\")\n",
        "            if all_masks is not None:\n",
        "                all_masks = F.interpolate(all_masks, size=self.image_size, mode=\"area\")\n",
        "\n",
        "        result = {\n",
        "            \"path\": root_dir,\n",
        "            \"img_id\": index,\n",
        "            \"focal\": focal,\n",
        "            \"images\": all_imgs,\n",
        "            \"poses\": all_poses,\n",
        "        }\n",
        "        if all_masks is not None:\n",
        "            result[\"masks\"] = all_masks\n",
        "        if self.sub_format != \"shapenet\":\n",
        "            result[\"c\"] = c\n",
        "        else:\n",
        "            result[\"bbox\"] = all_bboxes\n",
        "        return result"
      ],
      "metadata": {
        "id": "OozlkG6GlVPJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "data/MultiObjectDataset.py"
      ],
      "metadata": {
        "id": "hq2yA8y4mTiE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import json\n",
        "import imageio\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms\n",
        "\n",
        "class MultiObjectDataset(torch.utils.data.Dataset):\n",
        "    \"\"\"Synthetic dataset of scenes with multiple Shapenet objects\"\"\"\n",
        "\n",
        "    def __init__(self, path, stage=\"train\", z_near=4, z_far=9, n_views=None):\n",
        "        super().__init__()\n",
        "        path = os.path.join(path, stage)\n",
        "        self.base_path = path\n",
        "        print(\"Loading NeRF synthetic dataset\", self.base_path)\n",
        "        trans_files = []\n",
        "        TRANS_FILE = \"transforms.json\"\n",
        "        for root, directories, filenames in os.walk(self.base_path):\n",
        "            if TRANS_FILE in filenames:\n",
        "                trans_files.append(os.path.join(root, TRANS_FILE))\n",
        "        self.trans_files = trans_files\n",
        "        self.image_to_tensor = get_image_to_tensor_balanced()\n",
        "        self.mask_to_tensor = get_mask_to_tensor()\n",
        "\n",
        "        self.z_near = z_near\n",
        "        self.z_far = z_far\n",
        "        self.lindisp = False\n",
        "        self.n_views = n_views\n",
        "\n",
        "        print(\"{} instances in split {}\".format(len(self.trans_files), stage))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.trans_files)\n",
        "\n",
        "    def _check_valid(self, index):\n",
        "        if self.n_views is None:\n",
        "            return True\n",
        "        trans_file = self.trans_files[index]\n",
        "        dir_path = os.path.dirname(trans_file)\n",
        "        try:\n",
        "            with open(trans_file, \"r\") as f:\n",
        "                transform = json.load(f)\n",
        "        except Exception as e:\n",
        "            print(\"Problematic transforms.json file\", trans_file)\n",
        "            print(\"JSON loading exception\", e)\n",
        "            return False\n",
        "        if len(transform[\"frames\"]) != self.n_views:\n",
        "            return False\n",
        "        if len(glob.glob(os.path.join(dir_path, \"*.png\"))) != self.n_views:\n",
        "            return False\n",
        "        return True\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        if not self._check_valid(index):\n",
        "            return {}\n",
        "\n",
        "        trans_file = self.trans_files[index]\n",
        "        dir_path = os.path.dirname(trans_file)\n",
        "        with open(trans_file, \"r\") as f:\n",
        "            transform = json.load(f)\n",
        "\n",
        "        all_imgs = []\n",
        "        all_bboxes = []\n",
        "        all_masks = []\n",
        "        all_poses = []\n",
        "        for frame in transform[\"frames\"]:\n",
        "            fpath = frame[\"file_path\"]\n",
        "            basename = os.path.splitext(os.path.basename(fpath))[0]\n",
        "            obj_path = os.path.join(dir_path, \"{}_obj.png\".format(basename))\n",
        "            img = imageio.imread(obj_path)\n",
        "            mask = self.mask_to_tensor(img[..., 3])\n",
        "            rows = np.any(img, axis=1)\n",
        "            cols = np.any(img, axis=0)\n",
        "            rnz = np.where(rows)[0]\n",
        "            cnz = np.where(cols)[0]\n",
        "            if len(rnz) == 0:\n",
        "                cmin = rmin = 0\n",
        "                cmax = mask.shape[-1]\n",
        "                rmax = mask.shape[-2]\n",
        "            else:\n",
        "                rmin, rmax = rnz[[0, -1]]\n",
        "                cmin, cmax = cnz[[0, -1]]\n",
        "            bbox = torch.tensor([cmin, rmin, cmax, rmax], dtype=torch.float32)\n",
        "\n",
        "            img_tensor = self.image_to_tensor(img[..., :3])\n",
        "            img = img_tensor * mask + (\n",
        "                1.0 - mask\n",
        "            )  # solid white background where transparent\n",
        "            all_imgs.append(img)\n",
        "            all_bboxes.append(bbox)\n",
        "            all_masks.append(mask)\n",
        "            all_poses.append(torch.tensor(frame[\"transform_matrix\"]))\n",
        "        imgs = torch.stack(all_imgs)\n",
        "        masks = torch.stack(all_masks)\n",
        "        bboxes = torch.stack(all_bboxes)\n",
        "        poses = torch.stack(all_poses)\n",
        "\n",
        "        H, W = imgs.shape[-2:]\n",
        "        camera_angle_x = transform.get(\"camera_angle_x\")\n",
        "        focal = 0.5 * W / np.tan(0.5 * camera_angle_x)\n",
        "\n",
        "        result = {\n",
        "            \"path\": dir_path,\n",
        "            \"img_id\": index,\n",
        "            \"focal\": focal,\n",
        "            \"images\": imgs,\n",
        "            \"masks\": masks,\n",
        "            \"bbox\": bboxes,\n",
        "            \"poses\": poses,\n",
        "        }\n",
        "        return result"
      ],
      "metadata": {
        "id": "H0_Abn4TmKNw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "data/SRNDataset.py"
      ],
      "metadata": {
        "id": "j1LYYZOxmknt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import glob\n",
        "import imageio\n",
        "import numpy as np\n",
        "\n",
        "class SRNDataset(torch.utils.data.Dataset):\n",
        "    \"\"\"\n",
        "    Dataset from SRN (V. Sitzmann et al. 2020)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self, path, stage=\"train\", image_size=(128, 128), world_scale=1.0,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        :param stage train | val | test\n",
        "        :param image_size result image size (resizes if different)\n",
        "        :param world_scale amount to scale entire world by\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.base_path = path + \"_\" + stage\n",
        "        self.dataset_name = os.path.basename(path)\n",
        "\n",
        "        print(\"Loading SRN dataset\", self.base_path, \"name:\", self.dataset_name)\n",
        "        self.stage = stage\n",
        "        assert os.path.exists(self.base_path)\n",
        "\n",
        "        is_chair = \"chair\" in self.dataset_name\n",
        "        if is_chair and stage == \"train\":\n",
        "            # Ugly thing from SRN's public dataset\n",
        "            tmp = os.path.join(self.base_path, \"chairs_2.0_train\")\n",
        "            if os.path.exists(tmp):\n",
        "                self.base_path = tmp\n",
        "\n",
        "        self.intrins = sorted(\n",
        "            glob.glob(os.path.join(self.base_path, \"*\", \"intrinsics.txt\"))\n",
        "        )\n",
        "        self.image_to_tensor = get_image_to_tensor_balanced()\n",
        "        self.mask_to_tensor = get_mask_to_tensor()\n",
        "\n",
        "        self.image_size = image_size\n",
        "        self.world_scale = world_scale\n",
        "        self._coord_trans = torch.diag(\n",
        "            torch.tensor([1, -1, -1, 1], dtype=torch.float32)\n",
        "        )\n",
        "\n",
        "        if is_chair:\n",
        "            self.z_near = 1.25\n",
        "            self.z_far = 2.75\n",
        "        else:\n",
        "            self.z_near = 0.8\n",
        "            self.z_far = 1.8\n",
        "        self.lindisp = False\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.intrins)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        intrin_path = self.intrins[index]\n",
        "        dir_path = os.path.dirname(intrin_path)\n",
        "        rgb_paths = sorted(glob.glob(os.path.join(dir_path, \"rgb\", \"*\")))\n",
        "        pose_paths = sorted(glob.glob(os.path.join(dir_path, \"pose\", \"*\")))\n",
        "\n",
        "        assert len(rgb_paths) == len(pose_paths)\n",
        "\n",
        "        with open(intrin_path, \"r\") as intrinfile:\n",
        "            lines = intrinfile.readlines()\n",
        "            focal, cx, cy, _ = map(float, lines[0].split())\n",
        "            height, width = map(int, lines[-1].split())\n",
        "\n",
        "        all_imgs = []\n",
        "        all_poses = []\n",
        "        all_masks = []\n",
        "        all_bboxes = []\n",
        "        for rgb_path, pose_path in zip(rgb_paths, pose_paths):\n",
        "            img = imageio.imread(rgb_path)[..., :3]\n",
        "            img_tensor = self.image_to_tensor(img)\n",
        "            mask = (img != 255).all(axis=-1)[..., None].astype(np.uint8) * 255\n",
        "            mask_tensor = self.mask_to_tensor(mask)\n",
        "\n",
        "            pose = torch.from_numpy(\n",
        "                np.loadtxt(pose_path, dtype=np.float32).reshape(4, 4)\n",
        "            )\n",
        "            pose = pose @ self._coord_trans\n",
        "\n",
        "            rows = np.any(mask, axis=1)\n",
        "            cols = np.any(mask, axis=0)\n",
        "            rnz = np.where(rows)[0]\n",
        "            cnz = np.where(cols)[0]\n",
        "            if len(rnz) == 0:\n",
        "                raise RuntimeError(\n",
        "                    \"ERROR: Bad image at\", rgb_path, \"please investigate!\"\n",
        "                )\n",
        "            rmin, rmax = rnz[[0, -1]]\n",
        "            cmin, cmax = cnz[[0, -1]]\n",
        "            bbox = torch.tensor([cmin, rmin, cmax, rmax], dtype=torch.float32)\n",
        "\n",
        "            all_imgs.append(img_tensor)\n",
        "            all_masks.append(mask_tensor)\n",
        "            all_poses.append(pose)\n",
        "            all_bboxes.append(bbox)\n",
        "\n",
        "        all_imgs = torch.stack(all_imgs)\n",
        "        all_poses = torch.stack(all_poses)\n",
        "        all_masks = torch.stack(all_masks)\n",
        "        all_bboxes = torch.stack(all_bboxes)\n",
        "\n",
        "        if all_imgs.shape[-2:] != self.image_size:\n",
        "            scale = self.image_size[0] / all_imgs.shape[-2]\n",
        "            focal *= scale\n",
        "            cx *= scale\n",
        "            cy *= scale\n",
        "            all_bboxes *= scale\n",
        "\n",
        "            all_imgs = F.interpolate(all_imgs, size=self.image_size, mode=\"area\")\n",
        "            all_masks = F.interpolate(all_masks, size=self.image_size, mode=\"area\")\n",
        "\n",
        "        if self.world_scale != 1.0:\n",
        "            focal *= self.world_scale\n",
        "            all_poses[:, :3, 3] *= self.world_scale\n",
        "        focal = torch.tensor(focal, dtype=torch.float32)\n",
        "\n",
        "        result = {\n",
        "            \"path\": dir_path,\n",
        "            \"img_id\": index,\n",
        "            \"focal\": focal,\n",
        "            \"c\": torch.tensor([cx, cy], dtype=torch.float32),\n",
        "            \"images\": all_imgs,\n",
        "            \"masks\": all_masks,\n",
        "            \"bbox\": all_bboxes,\n",
        "            \"poses\": all_poses,\n",
        "        }\n",
        "        return result"
      ],
      "metadata": {
        "id": "45VTsEsqmfg_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "data/data_util.py"
      ],
      "metadata": {
        "id": "-0ia0hhpmw2R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms.functional_tensor as F_t\n",
        "import torchvision.transforms.functional as TF\n",
        "import numpy as np\n",
        "import imageio\n",
        "\n",
        "#  from util import GaussianBlur\n",
        "\n",
        "\n",
        "class ColorJitterDataset(torch.utils.data.Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        base_dset,\n",
        "        hue_range=0.1,\n",
        "        saturation_range=0.1,\n",
        "        brightness_range=0.1,\n",
        "        contrast_range=0.1,\n",
        "        extra_inherit_attrs=[],\n",
        "    ):\n",
        "        self.hue_range = [-hue_range, hue_range]\n",
        "        self.saturation_range = [1 - saturation_range, 1 + saturation_range]\n",
        "        self.brightness_range = [1 - brightness_range, 1 + brightness_range]\n",
        "        self.contrast_range = [1 - contrast_range, 1 + contrast_range]\n",
        "        inherit_attrs = [\"z_near\", \"z_far\", \"lindisp\", \"base_path\", \"image_to_tensor\"]\n",
        "        inherit_attrs.extend(extra_inherit_attrs)\n",
        "\n",
        "        self.base_dset = base_dset\n",
        "        for inherit_attr in inherit_attrs:\n",
        "            setattr(self, inherit_attr, getattr(self.base_dset, inherit_attr))\n",
        "\n",
        "    def apply_color_jitter(self, images):\n",
        "        # apply the same color jitter over batch of images\n",
        "        hue_factor = np.random.uniform(*self.hue_range)\n",
        "        saturation_factor = np.random.uniform(*self.saturation_range)\n",
        "        brightness_factor = np.random.uniform(*self.brightness_range)\n",
        "        contrast_factor = np.random.uniform(*self.contrast_range)\n",
        "        for i in range(len(images)):\n",
        "            tmp = (images[i] + 1.0) * 0.5\n",
        "            tmp = F_t.adjust_saturation(tmp, saturation_factor)\n",
        "            tmp = F_t.adjust_hue(tmp, hue_factor)\n",
        "            tmp = F_t.adjust_contrast(tmp, contrast_factor)\n",
        "            tmp = F_t.adjust_brightness(tmp, brightness_factor)\n",
        "            images[i] = tmp * 2.0 - 1.0\n",
        "        return images\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.base_dset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        data = self.base_dset[idx]\n",
        "        data[\"images\"] = self.apply_color_jitter(data[\"images\"])\n",
        "        return data"
      ],
      "metadata": {
        "id": "kwHaOxGdmoK6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "data/__init__.py"
      ],
      "metadata": {
        "id": "EQt4GSYjm2Qj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def get_split_dataset(dataset_type, datadir, want_split=\"all\", training=True, **kwargs):\n",
        "    \"\"\"\n",
        "    Retrieved desired dataset class\n",
        "    :param dataset_type dataset type name (srn|dvr|dvr_gen, etc)\n",
        "    :param datadir root directory name for the dataset. For SRN/multi_obj data:\n",
        "    if data is in dir/cars_train, dir/cars_test, ... then put dir/cars\n",
        "    :param want_split root directory name for the dataset\n",
        "    :param training set to False in eval scripts\n",
        "    \"\"\"\n",
        "    dset_class, train_aug = None, None\n",
        "    flags, train_aug_flags = {}, {}\n",
        "\n",
        "    if dataset_type == \"srn\":\n",
        "        # For ShapeNet single-category (from SRN)\n",
        "        dset_class = SRNDataset\n",
        "    elif dataset_type == \"multi_obj\":\n",
        "        # For multiple-object\n",
        "        dset_class = MultiObjectDataset\n",
        "    elif dataset_type.startswith(\"dvr\"):\n",
        "        # For ShapeNet 64x64\n",
        "        dset_class = DVRDataset\n",
        "        if dataset_type == \"dvr_gen\":\n",
        "            # For generalization training (train some categories, eval on others)\n",
        "            flags[\"list_prefix\"] = \"gen_\"\n",
        "        elif dataset_type == \"dvr_dtu\":\n",
        "            # DTU dataset\n",
        "            flags[\"list_prefix\"] = \"new_\"\n",
        "            if training:\n",
        "                flags[\"max_imgs\"] = 49\n",
        "            flags[\"sub_format\"] = \"dtu\"\n",
        "            flags[\"scale_focal\"] = False\n",
        "            flags[\"z_near\"] = 0.1\n",
        "            flags[\"z_far\"] = 5.0\n",
        "            # Apply color jitter during train\n",
        "            train_aug = ColorJitterDataset\n",
        "            train_aug_flags = {\"extra_inherit_attrs\": [\"sub_format\"]}\n",
        "    else:\n",
        "        raise NotImplementedError(\"Unsupported dataset type\", dataset_type)\n",
        "\n",
        "    want_train = want_split != \"val\" and want_split != \"test\"\n",
        "    want_val = want_split != \"train\" and want_split != \"test\"\n",
        "    want_test = want_split != \"train\" and want_split != \"val\"\n",
        "\n",
        "    if want_train:\n",
        "        train_set = dset_class(datadir, stage=\"train\", **flags, **kwargs)\n",
        "        if train_aug is not None:\n",
        "            train_set = train_aug(train_set, **train_aug_flags)\n",
        "\n",
        "    if want_val:\n",
        "        val_set = dset_class(datadir, stage=\"val\", **flags, **kwargs)\n",
        "\n",
        "    if want_test:\n",
        "        test_set = dset_class(datadir, stage=\"test\", **flags, **kwargs)\n",
        "\n",
        "    if want_split == \"train\":\n",
        "        return train_set\n",
        "    elif want_split == \"val\":\n",
        "        return val_set\n",
        "    elif want_split == \"test\":\n",
        "        return test_set\n",
        "    return train_set, val_set, test_set"
      ],
      "metadata": {
        "id": "VN-Iwaknmy-H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "calc_metric param"
      ],
      "metadata": {
        "id": "BahzoHETo0hi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Parameters_eval_metric:\n",
        "    def __init__(self):\n",
        "        self.datadir=''\n",
        "        self.output=''\n",
        "        self.dataset_format=''\n",
        "        self.list_name=''\n",
        "        self.gpu_id=0\n",
        "        self.overwrite=False\n",
        "        self.exclude_dtu_bad=False\n",
        "        self.multicat=False\n",
        "        self.viewlist=''\n",
        "        self.eval_view_list=''\n",
        "        self.primary=''\n",
        "        self.lpips_batch_size=32\n",
        "        self.reduce_only=False\n",
        "        self.metadata=''\n",
        "        self.dtu_sort=False"
      ],
      "metadata": {
        "id": "-Q6uUnfPo4LV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "eval/calc_metrics.py"
      ],
      "metadata": {
        "id": "oBQJckXLnowG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import os.path as osp\n",
        "#import argparse\n",
        "import skimage.measure\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "import lpips\n",
        "import numpy as np\n",
        "import torch\n",
        "import imageio\n",
        "import json\n",
        "\n",
        "args = Parameters_eval_metric()\n",
        "args.datadir='/content/gdrive/MyDrive/Colab Notebooks/NeRF/Data/dtu_dataset/rs_dtu_4/DTU/'\n",
        "args.output='/content/gdrive/MyDrive/Colab Notebooks/NeRF/PixelNeRF/out/'\n",
        "args.dataset_format='dvr'\n",
        "args.list_name='test'\n",
        "#args.multicat=True\n",
        "\n",
        "if args.dataset_format == \"dvr\":\n",
        "    list_name = args.list_name + \".lst\"\n",
        "    img_dir_name = \"image\"\n",
        "elif args.dataset_format == \"srn\":\n",
        "    list_name = \"\"\n",
        "    img_dir_name = \"rgb\"\n",
        "elif args.dataset_format == \"nerf\":\n",
        "    warnings.warn(\"test split not implemented for NeRF synthetic data format\")\n",
        "    list_name = \"\"\n",
        "    img_dir_name = \"\"\n",
        "else:\n",
        "    raise NotImplementedError(\"Not supported data format \" + args.dataset_format)\n",
        "\n",
        "\n",
        "data_root = args.datadir\n",
        "render_root = args.output\n",
        "\n",
        "\n",
        "def run_map():\n",
        "    print(os.listdir(data_root))\n",
        "    if args.multicat:\n",
        "        cats = os.listdir(data_root)\n",
        "\n",
        "        def fmt_obj_name(c, x):\n",
        "            return c + \"_\" + x\n",
        "\n",
        "    else:\n",
        "        cats = [\".\"]\n",
        "        cats=['']\n",
        "\n",
        "        def fmt_obj_name(c, x):\n",
        "            return x\n",
        "\n",
        "    use_exclude_lut = len(args.viewlist) > 0\n",
        "    if use_exclude_lut:\n",
        "        print(\"Excluding views from list\", args.viewlist)\n",
        "        with open(args.viewlist, \"r\") as f:\n",
        "            tmp = [x.strip().split() for x in f.readlines()]\n",
        "        exclude_lut = {\n",
        "            x[0] + \"/\" + x[1]: torch.tensor(list(map(int, x[2:])), dtype=torch.long)\n",
        "            for x in tmp\n",
        "        }\n",
        "    base_exclude_views = list(map(int, args.primary.split()))\n",
        "    if args.exclude_dtu_bad:\n",
        "        base_exclude_views.extend(\n",
        "            [3, 4, 5, 6, 7, 16, 17, 18, 19, 20, 21, 36, 37, 38, 39]\n",
        "        )\n",
        "\n",
        "    #if args.eval_view_list is not None:\n",
        "    #    with open(args.eval_view_list, \"r\") as f:\n",
        "    #        eval_views = list(map(int, f.readline().split()))\n",
        "    #        print(\"Only using views\", eval_views)\n",
        "    #else:\n",
        "    eval_views = None\n",
        "\n",
        "    all_objs = []\n",
        "\n",
        "    print(\"CATEGORICAL SUMMARY\")\n",
        "    total_objs = 0\n",
        "    print(\"cats:\",cats)\n",
        "    for cat in cats:\n",
        "        print(\"cat\",cat)\n",
        "        cat_root = osp.join(data_root, cat)\n",
        "        print(\"data_root:\",data_root)\n",
        "        print(\"cat_root:\",cat_root)\n",
        "        if not osp.isdir(cat_root):\n",
        "            continue\n",
        "\n",
        "        objs = sorted([x for x in os.listdir(cat_root)])\n",
        "\n",
        "        print(objs)\n",
        "\n",
        "        if len(list_name) > 0:\n",
        "            list_path = osp.join(cat_root, list_name)\n",
        "            with open(list_path, \"r\") as f:\n",
        "                split = set([x.strip() for x in f.readlines()])\n",
        "            objs = [x for x in objs if x in split]\n",
        "\n",
        "        print(objs)\n",
        "\n",
        "        objs_rend = [osp.join(render_root, fmt_obj_name(cat, x)) for x in objs]\n",
        "\n",
        "        print(\"objs_rend\",objs_rend)\n",
        "\n",
        "        objs = [osp.join(cat_root, x) for x in objs]\n",
        "        objs = [x for x in objs if osp.isdir(x)]\n",
        "\n",
        "        print(\"objs\",objs)\n",
        "\n",
        "        objs = list(zip(objs, objs_rend))\n",
        "        print(\"objs\",objs)\n",
        "        for i in range(10):\n",
        "            print(objs[i][1])\n",
        "            print(osp.exists(objs[i][1]))\n",
        "        objs_avail = [x for x in objs if osp.exists(x[1])]\n",
        "        print(cat, \"TOTAL\", len(objs), \"AVAILABLE\", len(objs_avail))\n",
        "        #  assert len(objs) == len(objs_avail)\n",
        "        total_objs += len(objs)\n",
        "        all_objs.extend(objs_avail)\n",
        "    print(\">>> USING\", len(all_objs), \"OF\", total_objs, \"OBJECTS\")\n",
        "\n",
        "    cuda = \"cuda:\" + str(args.gpu_id)\n",
        "    lpips_vgg = lpips.LPIPS(net=\"vgg\").to(device=cuda)\n",
        "\n",
        "    def get_metrics(rgb, gt):\n",
        "        ssim = skimage.measure.compare_ssim(rgb, gt, multichannel=True, data_range=1)\n",
        "        psnr = skimage.measure.compare_psnr(rgb, gt, data_range=1)\n",
        "        return psnr, ssim\n",
        "\n",
        "    def isimage(path):\n",
        "        ext = osp.splitext(path)[1]\n",
        "        return ext == \".jpg\" or ext == \".png\"\n",
        "\n",
        "    def process_obj(path, rend_path):\n",
        "        if len(img_dir_name) > 0:\n",
        "            im_root = osp.join(path, img_dir_name)\n",
        "        else:\n",
        "            im_root = path\n",
        "        out_path = osp.join(rend_path, \"metrics.txt\")\n",
        "        if osp.exists(out_path) and not args.overwrite:\n",
        "            return\n",
        "        ims = [x for x in sorted(os.listdir(im_root)) if isimage(x)]\n",
        "        print(\"ims:\",ims)\n",
        "        psnr_avg = 0.0\n",
        "        ssim_avg = 0.0\n",
        "        gts = []\n",
        "        preds = []\n",
        "        num_ims = 0\n",
        "        if use_exclude_lut:\n",
        "            lut_key = osp.basename(rend_path).replace(\"_\", \"/\")\n",
        "            exclude_views = exclude_lut[lut_key]\n",
        "        else:\n",
        "            exclude_views = []\n",
        "        exclude_views.extend(base_exclude_views)\n",
        "\n",
        "        for im_name in ims:\n",
        "            im_path = osp.join(im_root, im_name)\n",
        "            im_name_id = int(osp.splitext(im_name)[0])\n",
        "            im_name_out = \"{:06}.png\".format(im_name_id)\n",
        "            im_rend_path = osp.join(rend_path, im_name_out)\n",
        "            if osp.exists(im_rend_path) and im_name_id not in exclude_views:\n",
        "                if eval_views is not None and im_name_id not in eval_views:\n",
        "                    continue\n",
        "                gt = imageio.imread(im_path).astype(np.float32)[..., :3] / 255.0\n",
        "                pred = imageio.imread(im_rend_path).astype(np.float32) / 255.0\n",
        "\n",
        "                psnr, ssim = get_metrics(pred, gt)\n",
        "                psnr_avg += psnr\n",
        "                ssim_avg += ssim\n",
        "                gts.append(torch.from_numpy(gt).permute(2, 0, 1) * 2.0 - 1.0)\n",
        "                preds.append(torch.from_numpy(pred).permute(2, 0, 1) * 2.0 - 1.0)\n",
        "                num_ims += 1\n",
        "        gts = torch.stack(gts)\n",
        "        preds = torch.stack(preds)\n",
        "\n",
        "        lpips_all = []\n",
        "        preds_spl = torch.split(preds, args.lpips_batch_size, dim=0)\n",
        "        gts_spl = torch.split(gts, args.lpips_batch_size, dim=0)\n",
        "        with torch.no_grad():\n",
        "            for predi, gti in zip(preds_spl, gts_spl):\n",
        "                lpips_i = lpips_vgg(predi.to(device=cuda), gti.to(device=cuda))\n",
        "                lpips_all.append(lpips_i)\n",
        "            lpips = torch.cat(lpips_all)\n",
        "        lpips = lpips.mean().item()\n",
        "        psnr_avg /= num_ims\n",
        "        ssim_avg /= num_ims\n",
        "        out_txt = \"psnr {}\\nssim {}\\nlpips {}\".format(psnr_avg, ssim_avg, lpips)\n",
        "        with open(out_path, \"w\") as f:\n",
        "            f.write(out_txt)\n",
        "\n",
        "    for obj_path, obj_rend_path in tqdm(all_objs):\n",
        "        process_obj(obj_path, obj_rend_path)\n",
        "\n",
        "\n",
        "def run_reduce():\n",
        "    if args.multicat:\n",
        "        meta = json.load(open(osp.join(args.datadir, args.metadata), \"r\"))\n",
        "        cats = sorted(list(meta.keys()))\n",
        "        cat_description = {cat: meta[cat][\"name\"].split(\",\")[0] for cat in cats}\n",
        "\n",
        "    all_objs = []\n",
        "    objs = [x for x in os.listdir(render_root)]\n",
        "    objs = [osp.join(render_root, x) for x in objs if x[0] != \"_\"]\n",
        "    objs = [x for x in objs if osp.isdir(x)]\n",
        "    if args.dtu_sort:\n",
        "        objs = sorted(objs, key=lambda x: int(x[x.rindex(\"/\") + 5 :]))\n",
        "    else:\n",
        "        objs = sorted(objs)\n",
        "    all_objs.extend(objs)\n",
        "\n",
        "    print(\">>> PROCESSING\", len(all_objs), \"OBJECTS\")\n",
        "\n",
        "    METRIC_NAMES = [\"psnr\", \"ssim\", \"lpips\"]\n",
        "\n",
        "    out_metrics_path = osp.join(render_root, \"all_metrics.txt\")\n",
        "\n",
        "    if args.multicat:\n",
        "        cat_sz = {}\n",
        "        for cat in cats:\n",
        "            cat_sz[cat] = 0\n",
        "\n",
        "    all_metrics = {}\n",
        "    for name in METRIC_NAMES:\n",
        "        if args.multicat:\n",
        "            for cat in cats:\n",
        "                all_metrics[cat + \".\" + name] = 0.0\n",
        "        all_metrics[name] = 0.0\n",
        "\n",
        "    should_print_all_objs = len(all_objs) < 100\n",
        "\n",
        "    for obj_root in tqdm(all_objs):\n",
        "        metrics_path = osp.join(obj_root, \"metrics.txt\")\n",
        "        with open(metrics_path, \"r\") as f:\n",
        "            metrics = [line.split() for line in f.readlines()]\n",
        "        if args.multicat:\n",
        "            cat_name = osp.basename(obj_root).split(\"_\")[0]\n",
        "            cat_sz[cat_name] += 1\n",
        "            for metric, val in metrics:\n",
        "                all_metrics[cat_name + \".\" + metric] += float(val)\n",
        "\n",
        "        for metric, val in metrics:\n",
        "            all_metrics[metric] += float(val)\n",
        "        if should_print_all_objs:\n",
        "            print(obj_root, end=\" \")\n",
        "            for metric, val in metrics:\n",
        "                print(val, end=\" \")\n",
        "            print()\n",
        "\n",
        "    for name in METRIC_NAMES:\n",
        "        if args.multicat:\n",
        "            for cat in cats:\n",
        "                if cat_sz[cat] > 0:\n",
        "                    all_metrics[cat + \".\" + name] /= cat_sz[cat]\n",
        "        all_metrics[name] /= len(all_objs)\n",
        "        print(name, all_metrics[name])\n",
        "\n",
        "    metrics_txt = []\n",
        "    if args.multicat:\n",
        "        for cat in cats:\n",
        "            if cat_sz[cat] > 0:\n",
        "                cat_txt = \"{:12s}\".format(cat_description[cat])\n",
        "                for name in METRIC_NAMES:\n",
        "                    cat_txt += \" {}: {:.6f}\".format(name, all_metrics[cat + \".\" + name])\n",
        "                cat_txt += \" n_inst: {}\".format(cat_sz[cat])\n",
        "                metrics_txt.append(cat_txt)\n",
        "\n",
        "        total_txt = \"---\\n{:12s}\".format(\"total\")\n",
        "    else:\n",
        "        total_txt = \"\"\n",
        "    for name in METRIC_NAMES:\n",
        "        total_txt += \" {}: {:.6f}\".format(name, all_metrics[name])\n",
        "    metrics_txt.append(total_txt)\n",
        "\n",
        "    metrics_txt = \"\\n\".join(metrics_txt)\n",
        "    with open(out_metrics_path, \"w\") as f:\n",
        "        f.write(metrics_txt)\n",
        "    print(\"WROTE\", out_metrics_path)\n",
        "    print(metrics_txt)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    if not args.reduce_only:\n",
        "        print(\">>> Compute\")\n",
        "        run_map()\n",
        "    print(\">>> Reduce\")\n",
        "    run_reduce()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "af43ce25edbe4c2a9caab2c96e0935fd",
            "e37aa05ca1784241ae929bec4f793bb4",
            "27d967098b9e4817bf02eabf4f2c6845",
            "f8065984b011411ea9591d6e64b550b3",
            "1cf17b3601594ec9acb7cca0f12309ac",
            "0bb91f0b8c8f44439c84cfa4e00c7b23",
            "8ccad8d0e3f84e15b485cd8f270451a4",
            "714a4adca8b243aba7479f67aac9465d",
            "62a1da966caa464dbf5a5fdd019e30b4",
            "f424c0b335bf455196768083d5fda7a3",
            "ac37ed5d009d4fb09d41716d9c9ed4d9"
          ]
        },
        "id": "EAxMquQcm4rh",
        "outputId": "526dbde8-b3d4-4916-b00b-b88a11154efe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Compute\n",
            "['mvsnet_test.lst', 'mvsnet_train.lst', 'train.lst', 'val.lst', 'mvsnet_val.lst', 'test.lst', 'new_train.lst', 'new_val.lst', 'new_test.lst', 'scan98', 'scan96', 'scan99', 'scan94', 'scan95', 'scan97', 'scan9', 'scan91', 'scan90', 'scan93', 'scan89', 'scan92', 'scan85', 'scan88', 'scan87', 'scan86', 'scan76', 'scan82', 'scan83', 'scan84', 'scan8', 'scan77', 'scan74', 'scan73', 'scan75', 'scan72', 'scan69', 'scan70', 'scan71', 'scan7', 'scan66', 'scan67', 'scan65', 'scan68', 'scan64', 'scan63', 'scan60', 'scan6', 'scan61', 'scan62', 'scan57', 'scan58', 'scan56', 'scan59', 'scan55', 'scan54', 'scan51', 'scan50', 'scan52', 'scan53', 'scan49', 'scan48', 'scan5', 'scan47', 'scan42', 'scan46', 'scan45', 'scan43', 'scan41', 'scan44', 'scan38', 'scan40', 'scan4', 'scan39', 'scan37', 'scan36', 'scan33', 'scan32', 'scan34', 'scan35', 'scan29', 'scan30', 'scan3', 'scan31', 'scan27', 'scan25', 'scan26', 'scan28', 'scan21', 'scan24', 'scan22', 'scan23', 'scan2', 'scan20', 'scan18', 'scan19', 'scan16', 'scan17', 'scan128', 'scan126', 'scan15', 'scan13', 'scan14', 'scan127', 'scan124', 'scan122', 'scan123', 'scan125', 'scan12', 'scan121', 'scan120', 'scan119', 'scan118', 'scan115', 'scan113', 'scan117', 'scan114', 'scan116', 'scan111', 'scan112', 'scan110', 'scan11', 'scan107', 'scan109', 'scan106', 'scan108', 'scan104', 'scan102', 'scan105', 'scan100', 'scan103', 'scan101', 'scan1', 'scan10', 'DTU_test']\n",
            "CATEGORICAL SUMMARY\n",
            "cats: ['']\n",
            "cat \n",
            "data_root: /content/gdrive/MyDrive/Colab Notebooks/NeRF/Data/dtu_dataset/rs_dtu_4/DTU/\n",
            "cat_root: /content/gdrive/MyDrive/Colab Notebooks/NeRF/Data/dtu_dataset/rs_dtu_4/DTU/\n",
            "['DTU_test', 'mvsnet_test.lst', 'mvsnet_train.lst', 'mvsnet_val.lst', 'new_test.lst', 'new_train.lst', 'new_val.lst', 'scan1', 'scan10', 'scan100', 'scan101', 'scan102', 'scan103', 'scan104', 'scan105', 'scan106', 'scan107', 'scan108', 'scan109', 'scan11', 'scan110', 'scan111', 'scan112', 'scan113', 'scan114', 'scan115', 'scan116', 'scan117', 'scan118', 'scan119', 'scan12', 'scan120', 'scan121', 'scan122', 'scan123', 'scan124', 'scan125', 'scan126', 'scan127', 'scan128', 'scan13', 'scan14', 'scan15', 'scan16', 'scan17', 'scan18', 'scan19', 'scan2', 'scan20', 'scan21', 'scan22', 'scan23', 'scan24', 'scan25', 'scan26', 'scan27', 'scan28', 'scan29', 'scan3', 'scan30', 'scan31', 'scan32', 'scan33', 'scan34', 'scan35', 'scan36', 'scan37', 'scan38', 'scan39', 'scan4', 'scan40', 'scan41', 'scan42', 'scan43', 'scan44', 'scan45', 'scan46', 'scan47', 'scan48', 'scan49', 'scan5', 'scan50', 'scan51', 'scan52', 'scan53', 'scan54', 'scan55', 'scan56', 'scan57', 'scan58', 'scan59', 'scan6', 'scan60', 'scan61', 'scan62', 'scan63', 'scan64', 'scan65', 'scan66', 'scan67', 'scan68', 'scan69', 'scan7', 'scan70', 'scan71', 'scan72', 'scan73', 'scan74', 'scan75', 'scan76', 'scan77', 'scan8', 'scan82', 'scan83', 'scan84', 'scan85', 'scan86', 'scan87', 'scan88', 'scan89', 'scan9', 'scan90', 'scan91', 'scan92', 'scan93', 'scan94', 'scan95', 'scan96', 'scan97', 'scan98', 'scan99', 'test.lst', 'train.lst', 'val.lst']\n",
            "['scan10', 'scan103', 'scan13', 'scan18', 'scan3', 'scan30', 'scan47', 'scan63', 'scan77', 'scan99']\n",
            "objs_rend ['/content/gdrive/MyDrive/Colab Notebooks/NeRF/PixelNeRF/out/scan10', '/content/gdrive/MyDrive/Colab Notebooks/NeRF/PixelNeRF/out/scan103', '/content/gdrive/MyDrive/Colab Notebooks/NeRF/PixelNeRF/out/scan13', '/content/gdrive/MyDrive/Colab Notebooks/NeRF/PixelNeRF/out/scan18', '/content/gdrive/MyDrive/Colab Notebooks/NeRF/PixelNeRF/out/scan3', '/content/gdrive/MyDrive/Colab Notebooks/NeRF/PixelNeRF/out/scan30', '/content/gdrive/MyDrive/Colab Notebooks/NeRF/PixelNeRF/out/scan47', '/content/gdrive/MyDrive/Colab Notebooks/NeRF/PixelNeRF/out/scan63', '/content/gdrive/MyDrive/Colab Notebooks/NeRF/PixelNeRF/out/scan77', '/content/gdrive/MyDrive/Colab Notebooks/NeRF/PixelNeRF/out/scan99']\n",
            "objs ['/content/gdrive/MyDrive/Colab Notebooks/NeRF/Data/dtu_dataset/rs_dtu_4/DTU/scan10', '/content/gdrive/MyDrive/Colab Notebooks/NeRF/Data/dtu_dataset/rs_dtu_4/DTU/scan103', '/content/gdrive/MyDrive/Colab Notebooks/NeRF/Data/dtu_dataset/rs_dtu_4/DTU/scan13', '/content/gdrive/MyDrive/Colab Notebooks/NeRF/Data/dtu_dataset/rs_dtu_4/DTU/scan18', '/content/gdrive/MyDrive/Colab Notebooks/NeRF/Data/dtu_dataset/rs_dtu_4/DTU/scan3', '/content/gdrive/MyDrive/Colab Notebooks/NeRF/Data/dtu_dataset/rs_dtu_4/DTU/scan30', '/content/gdrive/MyDrive/Colab Notebooks/NeRF/Data/dtu_dataset/rs_dtu_4/DTU/scan47', '/content/gdrive/MyDrive/Colab Notebooks/NeRF/Data/dtu_dataset/rs_dtu_4/DTU/scan63', '/content/gdrive/MyDrive/Colab Notebooks/NeRF/Data/dtu_dataset/rs_dtu_4/DTU/scan77', '/content/gdrive/MyDrive/Colab Notebooks/NeRF/Data/dtu_dataset/rs_dtu_4/DTU/scan99']\n",
            "objs [('/content/gdrive/MyDrive/Colab Notebooks/NeRF/Data/dtu_dataset/rs_dtu_4/DTU/scan10', '/content/gdrive/MyDrive/Colab Notebooks/NeRF/PixelNeRF/out/scan10'), ('/content/gdrive/MyDrive/Colab Notebooks/NeRF/Data/dtu_dataset/rs_dtu_4/DTU/scan103', '/content/gdrive/MyDrive/Colab Notebooks/NeRF/PixelNeRF/out/scan103'), ('/content/gdrive/MyDrive/Colab Notebooks/NeRF/Data/dtu_dataset/rs_dtu_4/DTU/scan13', '/content/gdrive/MyDrive/Colab Notebooks/NeRF/PixelNeRF/out/scan13'), ('/content/gdrive/MyDrive/Colab Notebooks/NeRF/Data/dtu_dataset/rs_dtu_4/DTU/scan18', '/content/gdrive/MyDrive/Colab Notebooks/NeRF/PixelNeRF/out/scan18'), ('/content/gdrive/MyDrive/Colab Notebooks/NeRF/Data/dtu_dataset/rs_dtu_4/DTU/scan3', '/content/gdrive/MyDrive/Colab Notebooks/NeRF/PixelNeRF/out/scan3'), ('/content/gdrive/MyDrive/Colab Notebooks/NeRF/Data/dtu_dataset/rs_dtu_4/DTU/scan30', '/content/gdrive/MyDrive/Colab Notebooks/NeRF/PixelNeRF/out/scan30'), ('/content/gdrive/MyDrive/Colab Notebooks/NeRF/Data/dtu_dataset/rs_dtu_4/DTU/scan47', '/content/gdrive/MyDrive/Colab Notebooks/NeRF/PixelNeRF/out/scan47'), ('/content/gdrive/MyDrive/Colab Notebooks/NeRF/Data/dtu_dataset/rs_dtu_4/DTU/scan63', '/content/gdrive/MyDrive/Colab Notebooks/NeRF/PixelNeRF/out/scan63'), ('/content/gdrive/MyDrive/Colab Notebooks/NeRF/Data/dtu_dataset/rs_dtu_4/DTU/scan77', '/content/gdrive/MyDrive/Colab Notebooks/NeRF/PixelNeRF/out/scan77'), ('/content/gdrive/MyDrive/Colab Notebooks/NeRF/Data/dtu_dataset/rs_dtu_4/DTU/scan99', '/content/gdrive/MyDrive/Colab Notebooks/NeRF/PixelNeRF/out/scan99')]\n",
            "/content/gdrive/MyDrive/Colab Notebooks/NeRF/PixelNeRF/out/scan10\n",
            "True\n",
            "/content/gdrive/MyDrive/Colab Notebooks/NeRF/PixelNeRF/out/scan103\n",
            "True\n",
            "/content/gdrive/MyDrive/Colab Notebooks/NeRF/PixelNeRF/out/scan13\n",
            "True\n",
            "/content/gdrive/MyDrive/Colab Notebooks/NeRF/PixelNeRF/out/scan18\n",
            "True\n",
            "/content/gdrive/MyDrive/Colab Notebooks/NeRF/PixelNeRF/out/scan3\n",
            "True\n",
            "/content/gdrive/MyDrive/Colab Notebooks/NeRF/PixelNeRF/out/scan30\n",
            "True\n",
            "/content/gdrive/MyDrive/Colab Notebooks/NeRF/PixelNeRF/out/scan47\n",
            "True\n",
            "/content/gdrive/MyDrive/Colab Notebooks/NeRF/PixelNeRF/out/scan63\n",
            "True\n",
            "/content/gdrive/MyDrive/Colab Notebooks/NeRF/PixelNeRF/out/scan77\n",
            "True\n",
            "/content/gdrive/MyDrive/Colab Notebooks/NeRF/PixelNeRF/out/scan99\n",
            "True\n",
            " TOTAL 10 AVAILABLE 10\n",
            ">>> USING 10 OF 10 OBJECTS\n",
            "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/528M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "af43ce25edbe4c2a9caab2c96e0935fd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/vgg.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/10 [00:01<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ims: ['000000.png', '000001.png', '000002.png', '000003.png', '000004.png', '000005.png', '000006.png', '000007.png', '000008.png', '000009.png', '000010.png', '000011.png', '000012.png', '000013.png', '000014.png', '000015.png', '000016.png', '000017.png', '000018.png', '000019.png', '000020.png', '000021.png', '000022.png', '000023.png', '000024.png', '000025.png', '000026.png', '000027.png', '000028.png', '000029.png', '000030.png', '000031.png', '000032.png', '000033.png', '000034.png', '000035.png', '000036.png', '000037.png', '000038.png', '000039.png', '000040.png', '000041.png', '000042.png', '000043.png', '000044.png', '000045.png', '000046.png', '000047.png', '000048.png']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-d972a51173b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_only\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\">>> Compute\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m         \u001b[0mrun_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\">>> Reduce\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[0mrun_reduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-27-d972a51173b7>\u001b[0m in \u001b[0;36mrun_map\u001b[0;34m()\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mobj_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj_rend_path\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_objs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m         \u001b[0mprocess_obj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj_rend_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-27-d972a51173b7>\u001b[0m in \u001b[0;36mprocess_obj\u001b[0;34m(path, rend_path)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m2.0\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m                 \u001b[0mnum_ims\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m         \u001b[0mgts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: stack expects a non-empty TensorList"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "eval/eval.py"
      ],
      "metadata": {
        "id": "CXU2Ag19GhhC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class eval_param():\n",
        "    def __init__(self):\n",
        "        self.split='test'\n",
        "        self.source='64' # source views\n",
        "        self.eval_view_list=None\n",
        "        self.coarse=False\n",
        "        self.no_compare_gt=False\n",
        "        self.multicat=False\n",
        "        self.viewlist=''\n",
        "        self.output='eval'\n",
        "        self.include_src=False\n",
        "        self.scale=1.0\n",
        "        self.write_depth=False\n",
        "        self.write_compare=False\n",
        "        self.free_pose=False\n",
        "        self.resume=False#continue training\n",
        "        self.gpu_id=0\n",
        "        self.name='srn_chairs'\n",
        "        self.dataset_format='srn'\n",
        "        self.exp_group_name=''\n",
        "        self.logs_path='/content/gdrive/MyDrive/Colab Notebooks/NeRF/PixelNeRF/logs/'\n",
        "        self.checkpoints_path='/content/gdrive/MyDrive/Colab Notebooks/NeRF/PixelNeRF/checkpoints'\n",
        "        self.visual_path='/content/gdrive/MyDrive/Colab Notebooks/NeRF/PixelNeRF/visuals/'\n",
        "        self.epochs=10000000\n",
        "        self.lr=1e-4\n",
        "        self.gamma=1.00\n",
        "        self.datadir=''\n",
        "        self.ray_batch_size=50000\n",
        "        self.conf='/content/gdrive/MyDrive/Colab Notebooks/NeRF/PixelNeRF/conf/exp/srn.conf'\n",
        "\n",
        "args=eval_param()\n",
        "\n",
        "PROJECT_ROOT = '/content/gdrive/MyDrive/Colab Notebooks/NeRF/PixelNeRF/'\n",
        "EXPCONF_PATH = os.path.join(PROJECT_ROOT, \"expconf.conf\")\n",
        "expconf = ConfigFactory.parse_file(EXPCONF_PATH)\n",
        "args.scale=1.0\n",
        "args.datadir='/content/gdrive/MyDrive/Colab Notebooks/NeRF/Data/srn_cars/cars'\n",
        "args.name='srn_car'\n",
        "args.output='/content/gdrive/MyDrive/Colab Notebooks/NeRF/PixelNeRF/eval/srn_cars/'\n",
        "\n",
        "os.makedirs(os.path.join(args.checkpoints_path, args.name), exist_ok=True)\n",
        "os.makedirs(os.path.join(args.visual_path, args.name), exist_ok=True)"
      ],
      "metadata": {
        "id": "Kh2RfCXEKsVP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import imageio\n",
        "import skimage.measure\n",
        "from skimage.metrics import structural_similarity as compare_ssim\n",
        "from skimage.metrics import peak_signal_noise_ratio as compare_psnr\n",
        "import cv2\n",
        "import tqdm\n",
        "#import ipdb\n",
        "import warnings\n",
        "\n",
        "#  from pytorch_memlab import set_target_gpu\n",
        "#  set_target_gpu(9)\n",
        "\n",
        "default_conf='conf/resnet_fine_mv.conf'\n",
        "default_datadir='data'\n",
        "default_data_format='dvr'\n",
        "training=False\n",
        "\n",
        "\n",
        "if args.conf is None:\n",
        "    args.conf = expconf.get_string(\"config.\" + args.name, default_conf)\n",
        "\n",
        "if args.conf is None:\n",
        "    args.conf = expconf.get_string(\"config.\" + args.name, default_conf)\n",
        "if args.datadir is None:\n",
        "    args.datadir = expconf.get_string(\"datadir.\" + args.name, default_datadir)\n",
        "print(\"DEBUG: \",args.conf)\n",
        "conf = ConfigFactory.parse_file(args.conf)\n",
        "\n",
        "if args.dataset_format is None:\n",
        "    args.dataset_format = conf.get_string(\"data.format\", default_data_format)\n",
        "\n",
        "#args.gpu_id = list(map(int, args.gpu_id.split()))\n",
        "args.gpu_id=list([0])\n",
        "\n",
        "print(\"EXPERIMENT NAME:\", args.name)\n",
        "if training:\n",
        "    print(\"CONTINUE?\", \"yes\" if args.resume else \"no\")\n",
        "print(\"* Config file:\", args.conf)\n",
        "print(\"* Dataset format:\", args.dataset_format)\n",
        "print(\"* Dataset location:\", args.datadir)\n",
        "\n",
        "args.resume = True\n",
        "\n",
        "device = get_cuda(args.gpu_id[0])\n",
        "\n",
        "dset = get_split_dataset(\n",
        "    args.dataset_format, args.datadir, want_split=args.split, training=False\n",
        ")\n",
        "print(\"dset:\",dset)\n",
        "data_loader = torch.utils.data.DataLoader(\n",
        "    dset, batch_size=1, shuffle=False, num_workers=8, pin_memory=False\n",
        ")\n",
        "\n",
        "output_dir = args.output.strip()\n",
        "has_output = len(output_dir) > 0\n",
        "\n",
        "total_psnr = 0.0\n",
        "total_ssim = 0.0\n",
        "cnt = 0\n",
        "\n",
        "if has_output:\n",
        "    finish_path = os.path.join(output_dir, \"finish.txt\")\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    if os.path.exists(finish_path):\n",
        "        with open(finish_path, \"r\") as f:\n",
        "            lines = [x.strip().split() for x in f.readlines()]\n",
        "        lines = [x for x in lines if len(x) == 4]\n",
        "        finished = set([x[0] for x in lines])\n",
        "        total_psnr = sum((float(x[1]) for x in lines))\n",
        "        total_ssim = sum((float(x[2]) for x in lines))\n",
        "        cnt = sum((int(x[3]) for x in lines))\n",
        "        if cnt > 0:\n",
        "            print(\"resume psnr\", total_psnr / cnt, \"ssim\", total_ssim / cnt)\n",
        "        else:\n",
        "            total_psnr = 0.0\n",
        "            total_ssim = 0.0\n",
        "    else:\n",
        "        finished = set()\n",
        "\n",
        "    finish_file = open(finish_path, \"a\", buffering=1)\n",
        "    print(\"Writing images to\", output_dir)\n",
        "\n",
        "\n",
        "net = make_model(conf[\"model\"]).to(device=device).load_weights(args)\n",
        "renderer = NeRFRenderer.from_conf(\n",
        "    conf[\"renderer\"], lindisp=dset.lindisp, eval_batch_size=args.ray_batch_size\n",
        ").to(device=device)\n",
        "if args.coarse:\n",
        "    net.mlp_fine = None\n",
        "\n",
        "if renderer.n_coarse < 64:\n",
        "    # Ensure decent sampling resolution\n",
        "    renderer.n_coarse = 64\n",
        "if args.coarse:\n",
        "    renderer.n_coarse = 64\n",
        "    renderer.n_fine = 128\n",
        "    renderer.using_fine = True\n",
        "\n",
        "render_par = renderer.bind_parallel(net, args.gpu_id, simple_output=True).eval()\n",
        "\n",
        "z_near = dset.z_near\n",
        "z_far = dset.z_far\n",
        "\n",
        "use_source_lut = len(args.viewlist) > 0\n",
        "if use_source_lut:\n",
        "    print(\"Using views from list\", args.viewlist)\n",
        "    with open(args.viewlist, \"r\") as f:\n",
        "        tmp = [x.strip().split() for x in f.readlines()]\n",
        "    source_lut = {\n",
        "        x[0] + \"/\" + x[1]: torch.tensor(list(map(int, x[2:])), dtype=torch.long)\n",
        "        for x in tmp\n",
        "    }\n",
        "else:\n",
        "    print(\"args.source: \",args.source)\n",
        "    source = torch.tensor(sorted(list(map(int, args.source.split()))), dtype=torch.long)\n",
        "\n",
        "NV = dset[0][\"images\"].shape[0]\n",
        "\n",
        "if args.eval_view_list is not None:\n",
        "    with open(args.eval_view_list, \"r\") as f:\n",
        "        eval_views = torch.tensor(list(map(int, f.readline().split())))\n",
        "    target_view_mask = torch.zeros(NV, dtype=torch.bool)\n",
        "    target_view_mask[eval_views] = 1\n",
        "else:\n",
        "    target_view_mask = torch.ones(NV, dtype=torch.bool)\n",
        "target_view_mask_init = target_view_mask\n",
        "\n",
        "all_rays = None\n",
        "rays_spl = []\n",
        "\n",
        "src_view_mask = None\n",
        "total_objs = len(data_loader)\n",
        "\n",
        "with torch.no_grad():\n",
        "    for obj_idx, data in enumerate(data_loader):\n",
        "        print(\n",
        "            \"OBJECT\",\n",
        "            obj_idx,\n",
        "            \"OF\",\n",
        "            total_objs,\n",
        "            \"PROGRESS\",\n",
        "            obj_idx / total_objs * 100.0,\n",
        "            \"%\",\n",
        "            data[\"path\"][0],\n",
        "        )\n",
        "        dpath = data[\"path\"][0]\n",
        "        obj_basename = os.path.basename(dpath)\n",
        "        cat_name = os.path.basename(os.path.dirname(dpath))\n",
        "        obj_name = cat_name + \"_\" + obj_basename if args.multicat else obj_basename\n",
        "        if has_output and obj_name in finished:\n",
        "            print(\"(skip)\")\n",
        "            continue\n",
        "        images = data[\"images\"][0]  # (NV, 3, H, W)\n",
        "\n",
        "        NV, _, H, W = images.shape\n",
        "\n",
        "        if args.scale != 1.0:\n",
        "            Ht = int(H * args.scale)\n",
        "            Wt = int(W * args.scale)\n",
        "            if abs(Ht / args.scale - H) > 1e-10 or abs(Wt / args.scale - W) > 1e-10:\n",
        "                warnings.warn(\n",
        "                    \"Inexact scaling, please check {} times ({}, {}) is integral\".format(\n",
        "                        args.scale, H, W\n",
        "                    )\n",
        "                )\n",
        "            H, W = Ht, Wt\n",
        "\n",
        "        if all_rays is None or use_source_lut or args.free_pose:\n",
        "            if use_source_lut:\n",
        "                obj_id = cat_name + \"/\" + obj_basename\n",
        "                source = source_lut[obj_id]\n",
        "\n",
        "            NS = len(source)\n",
        "            src_view_mask = torch.zeros(NV, dtype=torch.bool)\n",
        "            src_view_mask[source] = 1\n",
        "\n",
        "            focal = data[\"focal\"][0]\n",
        "            if isinstance(focal, float):\n",
        "                focal = torch.tensor(focal, dtype=torch.float32)\n",
        "            focal = focal[None]\n",
        "\n",
        "            c = data.get(\"c\")\n",
        "            if c is not None:\n",
        "                c = c[0].to(device=device).unsqueeze(0)\n",
        "\n",
        "            poses = data[\"poses\"][0]  # (NV, 4, 4)\n",
        "            src_poses = poses[src_view_mask].to(device=device)  # (NS, 4, 4)\n",
        "\n",
        "            target_view_mask = target_view_mask_init.clone()\n",
        "            if not args.include_src:\n",
        "                target_view_mask *= ~src_view_mask\n",
        "\n",
        "            novel_view_idxs = target_view_mask.nonzero(as_tuple=False).reshape(-1)\n",
        "\n",
        "            poses = poses[target_view_mask]  # (NV[-NS], 4, 4)\n",
        "\n",
        "            all_rays = (\n",
        "                gen_rays(\n",
        "                    poses.reshape(-1, 4, 4),\n",
        "                    W,\n",
        "                    H,\n",
        "                    focal * args.scale,\n",
        "                    z_near,\n",
        "                    z_far,\n",
        "                    c=c * args.scale if c is not None else None,\n",
        "                )\n",
        "                .reshape(-1, 8)\n",
        "                .to(device=device)\n",
        "            )  # ((NV[-NS])*H*W, 8)\n",
        "\n",
        "            poses = None\n",
        "            focal = focal.to(device=device)\n",
        "\n",
        "        rays_spl = torch.split(all_rays, args.ray_batch_size, dim=0)  # Creates views\n",
        "\n",
        "        n_gen_views = len(novel_view_idxs)\n",
        "\n",
        "        net.encode(\n",
        "            images[src_view_mask].to(device=device).unsqueeze(0),\n",
        "            src_poses.unsqueeze(0),\n",
        "            focal,\n",
        "            c=c,\n",
        "        )\n",
        "\n",
        "        all_rgb, all_depth = [], []\n",
        "        for rays in tqdm.tqdm(rays_spl):\n",
        "            rgb, depth = render_par(rays[None])\n",
        "            rgb = rgb[0].cpu()\n",
        "            depth = depth[0].cpu()\n",
        "            all_rgb.append(rgb)\n",
        "            all_depth.append(depth)\n",
        "\n",
        "        all_rgb = torch.cat(all_rgb, dim=0)\n",
        "        all_depth = torch.cat(all_depth, dim=0)\n",
        "        all_depth = (all_depth - z_near) / (z_far - z_near)\n",
        "        all_depth = all_depth.reshape(n_gen_views, H, W).numpy()\n",
        "\n",
        "        all_rgb = torch.clamp(\n",
        "            all_rgb.reshape(n_gen_views, H, W, 3), 0.0, 1.0\n",
        "        ).numpy()  # (NV-NS, H, W, 3)\n",
        "        if has_output:\n",
        "            obj_out_dir = os.path.join(output_dir, obj_name)\n",
        "            os.makedirs(obj_out_dir, exist_ok=True)\n",
        "            for i in range(n_gen_views):\n",
        "                out_file = os.path.join(\n",
        "                    obj_out_dir, \"{:06}.png\".format(novel_view_idxs[i].item())\n",
        "                )\n",
        "                imageio.imwrite(out_file, (all_rgb[i] * 255).astype(np.uint8))\n",
        "\n",
        "                if args.write_depth:\n",
        "                    out_depth_file = os.path.join(\n",
        "                        obj_out_dir, \"{:06}_depth.exr\".format(novel_view_idxs[i].item())\n",
        "                    )\n",
        "                    out_depth_norm_file = os.path.join(\n",
        "                        obj_out_dir,\n",
        "                        \"{:06}_depth_norm.png\".format(novel_view_idxs[i].item()),\n",
        "                    )\n",
        "                    depth_cmap_norm = cmap(all_depth[i])\n",
        "                    cv2.imwrite(out_depth_file, all_depth[i])\n",
        "                    imageio.imwrite(out_depth_norm_file, depth_cmap_norm)\n",
        "\n",
        "        curr_ssim = 0.0\n",
        "        curr_psnr = 0.0\n",
        "        if not args.no_compare_gt:\n",
        "            images_0to1 = images * 0.5 + 0.5  # (NV, 3, H, W)\n",
        "            images_gt = images_0to1[target_view_mask]\n",
        "            rgb_gt_all = (\n",
        "                images_gt.permute(0, 2, 3, 1).contiguous().numpy()\n",
        "            )  # (NV-NS, H, W, 3)\n",
        "            for view_idx in range(n_gen_views):\n",
        "                print(all_rgb.shape)\n",
        "                print(rgb_gt_all.shape)\n",
        "                print(all_rgb[0])\n",
        "                print(rgb_gt_all[0])\n",
        "                ssim = compare_ssim(\n",
        "                    all_rgb[view_idx],\n",
        "                    rgb_gt_all[view_idx],\n",
        "                    multichannel=True,\n",
        "                    data_range=1,\n",
        "                )\n",
        "                psnr = compare_psnr(\n",
        "                    all_rgb[view_idx], rgb_gt_all[view_idx], data_range=1\n",
        "                )\n",
        "                curr_ssim += ssim\n",
        "                curr_psnr += psnr\n",
        "\n",
        "                if args.write_compare:\n",
        "                    out_file = os.path.join(\n",
        "                        obj_out_dir,\n",
        "                        \"{:06}_compare.png\".format(novel_view_idxs[view_idx].item()),\n",
        "                    )\n",
        "                    out_im = np.hstack((all_rgb[view_idx], rgb_gt_all[view_idx]))\n",
        "                    imageio.imwrite(out_file, (out_im * 255).astype(np.uint8))\n",
        "        curr_psnr /= n_gen_views\n",
        "        curr_ssim /= n_gen_views\n",
        "        curr_cnt = 1\n",
        "        total_psnr += curr_psnr\n",
        "        total_ssim += curr_ssim\n",
        "        cnt += curr_cnt\n",
        "        if not args.no_compare_gt:\n",
        "            print(\n",
        "                \"curr psnr\",\n",
        "                curr_psnr,\n",
        "                \"ssim\",\n",
        "                curr_ssim,\n",
        "                \"running psnr\",\n",
        "                total_psnr / cnt,\n",
        "                \"running ssim\",\n",
        "                total_ssim / cnt,\n",
        "            )\n",
        "        finish_file.write(\n",
        "            \"{} {} {} {}\\n\".format(obj_name, curr_psnr, curr_ssim, curr_cnt)\n",
        "        )\n",
        "print(\"final psnr\", total_psnr / cnt, \"ssim\", total_ssim / cnt)"
      ],
      "metadata": {
        "id": "quGBmiPKqD25",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "666a46db-a5ca-4c65-81aa-7a628df87c87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEBUG:  /content/gdrive/MyDrive/Colab Notebooks/NeRF/PixelNeRF/conf/exp/srn.conf\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ParseSyntaxException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mParseException\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;31mParseException\u001b[0m: Expected '}', found '='  (at char 759), (line:34, col:18)",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mParseSyntaxException\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-bf5c7f42918c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatadir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexpconf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"datadir.\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault_datadir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"DEBUG: \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mconf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConfigFactory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_format\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyhocon/config_parser.py\u001b[0m in \u001b[0;36mparse_file\u001b[0;34m(cls, filename, encoding, required, resolve, unresolved_value)\u001b[0m\n\u001b[1;32m    150\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m                 \u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresolve\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munresolved_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIOError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mrequired\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyhocon/config_parser.py\u001b[0m in \u001b[0;36mparse_string\u001b[0;34m(cls, content, basedir, resolve, unresolved_value)\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \"\"\"\n\u001b[0;32m--> 202\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mConfigParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasedir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresolve\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munresolved_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyhocon/config_parser.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(cls, content, basedir, resolve, unresolved_value)\u001b[0m\n\u001b[1;32m    515\u001b[0m             config_expr = ZeroOrMore(comment_eol | eol) + (list_expr | root_dict_expr | inside_root_dict_expr) + ZeroOrMore(\n\u001b[1;32m    516\u001b[0m                 comment_eol | eol_comma)\n\u001b[0;32m--> 517\u001b[0;31m             \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig_expr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparseString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparseAll\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresolve\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyparsing/core.py\u001b[0m in \u001b[0;36mparse_string\u001b[0;34m(self, instring, parse_all, parseAll)\u001b[0m\n",
            "\u001b[0;31mParseSyntaxException\u001b[0m: Expected '}', found '='  (at char 759), (line:34, col:18)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jghKMGKnU3PO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}