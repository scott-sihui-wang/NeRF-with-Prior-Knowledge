{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuClass": "premium"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q5a_4r7Pykhg",
        "outputId": "ea0d0dca-61c3-4fee-815a-c1397f61e0f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir(\"/content/gdrive/MyDrive/Colab Notebooks/NeRF/\")"
      ],
      "metadata": {
        "id": "5Opk9AS_ynxP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install timm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pE8Gl6F8z27r",
        "outputId": "a3c4f7c3-baaa-4795-c591-6643f7326356"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting timm\n",
            "  Downloading timm-0.6.12-py3-none-any.whl (549 kB)\n",
            "\u001b[K     |████████████████████████████████| 549 kB 8.2 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub\n",
            "  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
            "\u001b[K     |████████████████████████████████| 182 kB 93.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (from timm) (0.14.0+cu116)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.8/dist-packages (from timm) (6.0)\n",
            "Requirement already satisfied: torch>=1.7 in /usr/local/lib/python3.8/dist-packages (from timm) (1.13.0+cu116)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch>=1.7->timm) (4.4.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub->timm) (21.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub->timm) (3.8.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from huggingface-hub->timm) (4.64.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from huggingface-hub->timm) (2.23.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.9->huggingface-hub->timm) (3.0.9)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub->timm) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub->timm) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub->timm) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub->timm) (1.24.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torchvision->timm) (1.21.6)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision->timm) (7.1.2)\n",
            "Installing collected packages: huggingface-hub, timm\n",
            "Successfully installed huggingface-hub-0.11.1 timm-0.6.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install einops"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PI08E-lc1ug1",
        "outputId": "996e607b-a8b6-4adf-9a0c-db249587e5dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting einops\n",
            "  Downloading einops-0.6.0-py3-none-any.whl (41 kB)\n",
            "\u001b[K     |████████████████████████████████| 41 kB 554 kB/s \n",
            "\u001b[?25hInstalling collected packages: einops\n",
            "Successfully installed einops-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorboardX"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QGOEij3D3JuJ",
        "outputId": "671f0163-7649-4ede-9cf0-8a977c7efcd3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorboardX\n",
            "  Downloading tensorboardX-2.5.1-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[K     |████████████████████████████████| 125 kB 7.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf<=3.20.1,>=3.8.0 in /usr/local/lib/python3.8/dist-packages (from tensorboardX) (3.19.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from tensorboardX) (1.21.6)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parameters"
      ],
      "metadata": {
        "id": "W_qIvg-P3JL4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Parameters_gen_video:\n",
        "    def __init__(self):\n",
        "        self.config=''#config file path\n",
        "        self.expname=''#experiment name\n",
        "        self.ckptdir=''#checkpoint folder\n",
        "        self.ckpt_path=''#weights npy for coarse net\n",
        "        self.outdir=''#output vid directory\n",
        "        self.local_rank=0\n",
        "        self.data_path=''#dataset to train\n",
        "        self.img_hw=[1024,768]#img sz\n",
        "        self.focal=131.25\n",
        "        self.radius=1.3\n",
        "        self.data_index=[]\n",
        "        self.z_near=0.8\n",
        "        self.z_far=1.8\n",
        "        self.fps=12\n",
        "        self.no_reload=False\n",
        "        self.distribted=False\n",
        "        self.num_frames=40\n",
        "        self.elevation=0.0\n",
        "        self.chunk_size=128\n",
        "        self.im_feat_dim=128\n",
        "        self.mlp_feat_dim=512\n",
        "        self.freq_num=10\n",
        "        self.mlp_block_num=2\n",
        "        self.coarse_only=False\n",
        "        self.anti_alias_pooling=1\n",
        "        self.num_source_views=1\n",
        "        self.freeze_pos_embed=False\n",
        "        self.no_skip_conv=False\n",
        "        self.lrate_feature=1e-3\n",
        "        self.lrate_mlp=5e-4\n",
        "        self.lrate_decay_factor=0.5\n",
        "        self.lrate_decay_steps=50000\n",
        "        self.warmup_steps=10000\n",
        "        self.scheduler='steplr'\n",
        "        self.use_warmup=False\n",
        "        self.bbox_step=100000\n",
        "        self.N_samples=64\n",
        "        self.N_importance=128\n",
        "        self.inv_uniform=False\n",
        "        self.det=False\n",
        "        self.white_bkgd=False"
      ],
      "metadata": {
        "id": "fidUnystpfgh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Parameters_eval:\n",
        "    def __init__(self):\n",
        "        self.config=''\n",
        "        self.expname=''\n",
        "        self.ckptdir=''\n",
        "        self.ckpt_path=''\n",
        "        self.outdir=''\n",
        "        self.local_rank=0\n",
        "        self.include_src=False\n",
        "        self.data_path=''\n",
        "        self.data_type='srn'\n",
        "        self.img_hw=[1024,768]\n",
        "        self.data_range=[0,50]\n",
        "        self.data_indices=[0]\n",
        "        self.use_data_index=False\n",
        "        self.pose_index=64\n",
        "        self.no_reload=False\n",
        "        self.distributed=False\n",
        "        self.skip=1\n",
        "        self.chunk_size=128\n",
        "        self.im_feat_dim=128\n",
        "        self.mlp_feat_dim=512\n",
        "        self.freq_num=10\n",
        "        self.mlp_block_num=2\n",
        "        self.coarse_only=False\n",
        "        self.anti_alias_pooling=1\n",
        "        self.num_source_views=1\n",
        "        self.freeze_pos_embed=False\n",
        "        self.no_skip_conv=False\n",
        "        self.lrate_feature=1e-3\n",
        "        self.lrate_mlp=5e-4\n",
        "        self.lrate_decay_factor=0.5\n",
        "        self.lrate_decay_steps=50000\n",
        "        self.warmup_steps=10000\n",
        "        self.scheduler='steplr'\n",
        "        self.use_warmup=False\n",
        "        self.bbox_step=100000\n",
        "        self.N_samples=64\n",
        "        self.N_importance=64\n",
        "        self.inv_uniform=False\n",
        "        self.det=False\n",
        "        self.white_bkgd=False"
      ],
      "metadata": {
        "id": "5vvDC6KHxGfC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model"
      ],
      "metadata": {
        "id": "0HzcItP6yVuB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model/Criterion.py"
      ],
      "metadata": {
        "id": "s29f-4V1yeek"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6H0w5msKyMVe"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "TINY_NUMBER = 1e-6      # float32 only has 7 decimal digits precision\n",
        "\n",
        "def img2mse(x, y, mask=None):\n",
        "    '''\n",
        "    :param x: img 1, [(...), 3]\n",
        "    :param y: img 2, [(...), 3]\n",
        "    :param mask: optional, [(...)]\n",
        "    :return: mse score\n",
        "    '''\n",
        "    if mask is None:\n",
        "        return torch.mean((x - y) * (x - y))\n",
        "    else:\n",
        "        return torch.sum((x - y) * (x - y) * mask.unsqueeze(-1)) / (torch.sum(mask) * x.shape[-1] + TINY_NUMBER)\n",
        "\n",
        "class Criterion(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, outputs, ray_batch, scalars_to_log):\n",
        "        '''\n",
        "        training criterion\n",
        "        '''\n",
        "        pred_rgb = outputs['rgb']\n",
        "        pred_mask = outputs['mask'].float()\n",
        "        gt_rgb = ray_batch['rgb']\n",
        "\n",
        "        loss = img2mse(pred_rgb, gt_rgb, pred_mask)\n",
        "\n",
        "        return loss, scalars_to_log"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Network/Resnet_mlp.py"
      ],
      "metadata": {
        "id": "sqRn9E1VzL-k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class GaussianActivation(nn.Module):\n",
        "    def __init__(self, a=1.0):\n",
        "        super(GaussianActivation, self).__init__()\n",
        "        self.a = a\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.exp(-0.5*x**2 / self.a**2)\n",
        "\n",
        "class ResnetBlock(torch.nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, use_gaussian=False):\n",
        "        super().__init__()\n",
        "\n",
        "        if use_gaussian:\n",
        "            self.prelu_0 = GaussianActivation()\n",
        "            self.prelu_1 = GaussianActivation()\n",
        "        else:\n",
        "            self.prelu_0 = torch.nn.ReLU(inplace=True)\n",
        "            self.prelu_1 = torch.nn.ReLU(inplace=True)\n",
        "\n",
        "        self.fc_0 = torch.nn.Linear(input_size, hidden_size)\n",
        "        self.fc_1 = torch.nn.Linear(hidden_size, output_size)\n",
        "\n",
        "        self.shortcut = (\n",
        "            torch.nn.Linear(input_size, output_size, bias=False)\n",
        "            if input_size != output_size else None)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = self.fc_1(self.prelu_1(self.fc_0(self.prelu_0(x))))\n",
        "        shortcut = x if self.shortcut is None else self.shortcut(x)\n",
        "        return residual + shortcut\n",
        "\n",
        "\n",
        "class PosEncodeResnet(torch.nn.Module):\n",
        "    def __init__(self, args, pos_size, x_size,\n",
        "                hidden_size, output_size, block_num, freq_factor=np.pi, use_gaussian=False):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            pos_size: size of positional encodings\n",
        "            x_size: size of input vector\n",
        "            hidden_size: hidden channels\n",
        "            output_size: output channels\n",
        "            freq_num: how many frequency bases\n",
        "            block_num: how many resnet blocks\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.args = args\n",
        "        self.freq_factor = freq_factor\n",
        "\n",
        "        input_size = (\n",
        "            pos_size * (2 * self.args.freq_num + 1)\n",
        "            + x_size\n",
        "        )\n",
        "\n",
        "        self.input_layer = torch.nn.Linear(input_size, hidden_size)\n",
        "        self.blocks = torch.nn.ModuleList(\n",
        "            [ResnetBlock(hidden_size, hidden_size, hidden_size, use_gaussian=use_gaussian)\n",
        "             for i in range(block_num)]\n",
        "        )\n",
        "        if use_gaussian:\n",
        "            self.output_prelu = GaussianActivation()\n",
        "        else:\n",
        "            self.output_prelu = torch.nn.ReLU(inplace=True)\n",
        "        self.output_layer = torch.nn.Linear(hidden_size, output_size)\n",
        "        self.softplus = torch.nn.Softplus()\n",
        "        self.sigmoid = torch.nn.Sigmoid()\n",
        "\n",
        "    def posenc(self, x):\n",
        "        freq_multiplier = (\n",
        "            self.freq_factor * 2 ** torch.arange(\n",
        "                                        self.args.freq_num,\n",
        "                                        device=x.device\n",
        "                                    )\n",
        "        ).view(1, 1, 1, -1)\n",
        "        x_expand = x.unsqueeze(-1)\n",
        "        sin_val = torch.sin(x_expand * freq_multiplier)\n",
        "        cos_val = torch.cos(x_expand * freq_multiplier)\n",
        "        return torch.cat(\n",
        "            [x_expand, sin_val, cos_val], -1\n",
        "        ).view(x.shape[:2] + (-1,))\n",
        "\n",
        "    def forward(self, pos_x, in_x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            pos_x: input to be encoded with positional encodings\n",
        "            in_x: input NOT to be encoded with positional encodings\n",
        "        \"\"\"\n",
        "        x = self.posenc(pos_x)\n",
        "        x = torch.cat([x, in_x], axis=-1)\n",
        "        x = self.input_layer(x)\n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "        out = self.output_layer(self.output_prelu(x))\n",
        "        out = torch.cat([self.sigmoid(out[..., :-1]), self.softplus(out[..., -1:])], -1)\n",
        "        return out"
      ],
      "metadata": {
        "id": "Sknr3hCnzHAV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Network/Vit.py"
      ],
      "metadata": {
        "id": "NHa1PuepzbEi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import types\n",
        "import math\n",
        "\n",
        "import timm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                     padding=dilation, groups=groups, bias=False, dilation=dilation, padding_mode='reflect')\n",
        "\n",
        "def conv1x1(in_planes, out_planes, stride=1):\n",
        "    \"\"\"1x1 convolution\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False, padding_mode='reflect')\n",
        "\n",
        "\n",
        "class Identity(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Identity, self).__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None, norm_layer=None):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn1 = norm_layer(planes, track_running_stats=False, affine=True)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = norm_layer(planes, track_running_stats=False, affine=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "activations = {}\n",
        "\n",
        "def get_activation(name):\n",
        "    def hook(model, input, output):\n",
        "        activations[name] = output\n",
        "\n",
        "    return hook\n",
        "\n",
        "\n",
        "class Slice(nn.Module):\n",
        "    def __init__(self, start_index=1):\n",
        "        super(Slice, self).__init__()\n",
        "        self.start_index = start_index\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x[:, self.start_index :]\n",
        "\n",
        "\n",
        "class AddReadout(nn.Module):\n",
        "    def __init__(self, start_index=1):\n",
        "        super(AddReadout, self).__init__()\n",
        "        self.start_index = start_index\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.start_index == 2:\n",
        "            readout = (x[:, 0] + x[:, 1]) / 2\n",
        "        else:\n",
        "            readout = x[:, 0]\n",
        "        return x[:, self.start_index :] + readout.unsqueeze(1)\n",
        "\n",
        "\n",
        "class ProjectReadout(nn.Module):\n",
        "    def __init__(self, in_features, start_index=1):\n",
        "        super(ProjectReadout, self).__init__()\n",
        "        self.start_index = start_index\n",
        "\n",
        "        self.project = nn.Sequential(nn.Linear(2 * in_features, in_features), nn.GELU())\n",
        "\n",
        "    def forward(self, x):\n",
        "        readout = x[:, 0].unsqueeze(1).expand_as(x[:, self.start_index :])\n",
        "        features = torch.cat((x[:, self.start_index :], readout), -1)\n",
        "\n",
        "        return self.project(features)\n",
        "\n",
        "\n",
        "class Transpose(nn.Module):\n",
        "    def __init__(self, dim0, dim1):\n",
        "        super(Transpose, self).__init__()\n",
        "        self.dim0 = dim0\n",
        "        self.dim1 = dim1\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.transpose(self.dim0, self.dim1)\n",
        "        return x\n",
        "\n",
        "def forward_vit(pretrained, x):\n",
        "    b, c, h, w = x.shape\n",
        "\n",
        "    glob = pretrained.model.forward_flex(x)\n",
        "\n",
        "    layer_1 = pretrained.activations[\"1\"]\n",
        "    layer_2 = pretrained.activations[\"2\"]\n",
        "    layer_3 = pretrained.activations[\"3\"]\n",
        "    layer_4 = pretrained.activations[\"4\"]\n",
        "\n",
        "    layer_1 = pretrained.act_postprocess1[0:2](layer_1)\n",
        "    layer_2 = pretrained.act_postprocess2[0:2](layer_2)\n",
        "    layer_3 = pretrained.act_postprocess3[0:2](layer_3)\n",
        "    layer_4 = pretrained.act_postprocess4[0:2](layer_4)\n",
        "\n",
        "    unflatten = nn.Sequential(\n",
        "        nn.Unflatten(\n",
        "            2,\n",
        "            torch.Size(\n",
        "                [\n",
        "                    h // pretrained.model.patch_size[1],\n",
        "                    w // pretrained.model.patch_size[0],\n",
        "                ]\n",
        "            ),\n",
        "        )\n",
        "    )\n",
        "\n",
        "    if layer_1.ndim == 3:\n",
        "        layer_1 = unflatten(layer_1)\n",
        "    if layer_2.ndim == 3:\n",
        "        layer_2 = unflatten(layer_2)\n",
        "    if layer_3.ndim == 3:\n",
        "        layer_3 = unflatten(layer_3)\n",
        "    if layer_4.ndim == 3:\n",
        "        layer_4 = unflatten(layer_4)\n",
        "\n",
        "    layer_1 = pretrained.act_postprocess1[3 : len(pretrained.act_postprocess1)](layer_1)\n",
        "    layer_2 = pretrained.act_postprocess2[3 : len(pretrained.act_postprocess2)](layer_2)\n",
        "    layer_3 = pretrained.act_postprocess3[3 : len(pretrained.act_postprocess3)](layer_3)\n",
        "    layer_4 = pretrained.act_postprocess4[3 : len(pretrained.act_postprocess4)](layer_4)\n",
        "\n",
        "    return layer_1, layer_2, layer_3, layer_4\n",
        "\n",
        "def _resize_pos_embed(self, posemb, gs_h, gs_w):\n",
        "    posemb_tok, posemb_grid = (\n",
        "        posemb[:, : self.start_index],\n",
        "        posemb[0, self.start_index :],\n",
        "    )\n",
        "\n",
        "    gs_old = int(math.sqrt(len(posemb_grid)))\n",
        "\n",
        "    posemb_grid = posemb_grid.reshape(1, gs_old, gs_old, -1).permute(0, 3, 1, 2)\n",
        "    posemb_grid = F.interpolate(posemb_grid, size=(gs_h, gs_w), mode=\"bilinear\")\n",
        "    posemb_grid = posemb_grid.permute(0, 2, 3, 1).reshape(1, gs_h * gs_w, -1)\n",
        "\n",
        "    posemb = torch.cat([posemb_tok, posemb_grid], dim=1)\n",
        "\n",
        "    return posemb\n",
        "\n",
        "def forward_flex(self, x):\n",
        "    b, c, h, w = x.shape\n",
        "\n",
        "    pos_embed = self._resize_pos_embed(\n",
        "        self.pos_embed, h // self.patch_size[1], w // self.patch_size[0]\n",
        "    )\n",
        "\n",
        "    B = x.shape[0]\n",
        "\n",
        "    if hasattr(self.patch_embed, \"backbone\"):\n",
        "        x = self.patch_embed.backbone(x)\n",
        "        if isinstance(x, (list, tuple)):\n",
        "            x = x[-1]  # last feature if backbone outputs list/tuple of features\n",
        "\n",
        "    x = self.patch_embed.proj(x).flatten(2).transpose(1, 2)\n",
        "\n",
        "    if getattr(self, \"dist_token\", None) is not None:\n",
        "        cls_tokens = self.cls_token.expand(\n",
        "            B, -1, -1\n",
        "        )  # stole cls_tokens impl from Phil Wang, thanks\n",
        "        dist_token = self.dist_token.expand(B, -1, -1)\n",
        "        x = torch.cat((cls_tokens, dist_token, x), dim=1)\n",
        "    else:\n",
        "        cls_tokens = self.cls_token.expand(\n",
        "            B, -1, -1\n",
        "        )  # stole cls_tokens impl from Phil Wang, thanks\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "\n",
        "    x = x + pos_embed\n",
        "    x = self.pos_drop(x)\n",
        "\n",
        "    for blk in self.blocks:\n",
        "        x = blk(x)\n",
        "\n",
        "    x = self.norm(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "def get_readout_oper(vit_features, features, use_readout, start_index=1):\n",
        "    if use_readout == \"ignore\":\n",
        "        readout_oper = [Slice(start_index)] * len(features)\n",
        "    elif use_readout == \"add\":\n",
        "        readout_oper = [AddReadout(start_index)] * len(features)\n",
        "    elif use_readout == \"project\":\n",
        "        readout_oper = [\n",
        "            ProjectReadout(vit_features, start_index) for out_feat in features\n",
        "        ]\n",
        "    else:\n",
        "        assert (\n",
        "            False\n",
        "        ), \"wrong operation for readout token, use_readout can be 'ignore', 'add', or 'project'\"\n",
        "\n",
        "    return readout_oper\n",
        "\n",
        "def _make_vit_b16_backbone(\n",
        "    model,\n",
        "    features=[96, 192, 384, 768],\n",
        "    size=[384, 384],\n",
        "    hooks=[2, 5, 8, 11],\n",
        "    vit_features=768,\n",
        "    use_readout=\"ignore\",\n",
        "    start_index=1,\n",
        "):\n",
        "    pretrained = nn.Module()\n",
        "\n",
        "    pretrained.model = model\n",
        "    pretrained.model.blocks[hooks[0]].register_forward_hook(get_activation(\"1\"))\n",
        "    pretrained.model.blocks[hooks[1]].register_forward_hook(get_activation(\"2\"))\n",
        "    pretrained.model.blocks[hooks[2]].register_forward_hook(get_activation(\"3\"))\n",
        "    pretrained.model.blocks[hooks[3]].register_forward_hook(get_activation(\"4\"))\n",
        "\n",
        "    pretrained.activations = activations\n",
        "\n",
        "\n",
        "    readout_oper = get_readout_oper(vit_features, features, use_readout, start_index)\n",
        "\n",
        "    # 32, 48, 136, 384\n",
        "    pretrained.act_postprocess1 = nn.Sequential(\n",
        "        readout_oper[0],\n",
        "        Transpose(1, 2),\n",
        "        nn.Unflatten(2, torch.Size([size[0] // 16, size[1] // 16])),\n",
        "        nn.Conv2d(\n",
        "            in_channels=vit_features,\n",
        "            out_channels=features[0],\n",
        "            kernel_size=1,\n",
        "            stride=1,\n",
        "            padding=0,\n",
        "        ),\n",
        "        nn.ConvTranspose2d(\n",
        "            in_channels=features[0],\n",
        "            out_channels=features[0],\n",
        "            kernel_size=4,\n",
        "            stride=4,\n",
        "            padding=0,\n",
        "            bias=True,\n",
        "            dilation=1,\n",
        "            groups=1,\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    pretrained.act_postprocess2 = nn.Sequential(\n",
        "        readout_oper[1],\n",
        "        Transpose(1, 2),\n",
        "        nn.Unflatten(2, torch.Size([size[0] // 16, size[1] // 16])),\n",
        "        nn.Conv2d(\n",
        "            in_channels=vit_features,\n",
        "            out_channels=features[1],\n",
        "            kernel_size=1,\n",
        "            stride=1,\n",
        "            padding=0,\n",
        "        ),\n",
        "        nn.ConvTranspose2d(\n",
        "            in_channels=features[1],\n",
        "            out_channels=features[1],\n",
        "            kernel_size=2,\n",
        "            stride=2,\n",
        "            padding=0,\n",
        "            bias=True,\n",
        "            dilation=1,\n",
        "            groups=1,\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    pretrained.act_postprocess3 = nn.Sequential(\n",
        "        readout_oper[2],\n",
        "        Transpose(1, 2),\n",
        "        nn.Unflatten(2, torch.Size([size[0] // 16, size[1] // 16])),\n",
        "        nn.Conv2d(\n",
        "            in_channels=vit_features,\n",
        "            out_channels=features[2],\n",
        "            kernel_size=1,\n",
        "            stride=1,\n",
        "            padding=0,\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    pretrained.act_postprocess4 = nn.Sequential(\n",
        "        readout_oper[3],\n",
        "        Transpose(1, 2),\n",
        "        nn.Unflatten(2, torch.Size([size[0] // 16, size[1] // 16])),\n",
        "        nn.Conv2d(\n",
        "            in_channels=vit_features,\n",
        "            out_channels=features[3],\n",
        "            kernel_size=1,\n",
        "            stride=1,\n",
        "            padding=0,\n",
        "        ),\n",
        "        nn.Conv2d(\n",
        "            in_channels=features[3],\n",
        "            out_channels=features[3],\n",
        "            kernel_size=3,\n",
        "            stride=2,\n",
        "            padding=1,\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    pretrained.model.start_index = start_index\n",
        "    pretrained.model.patch_size = [16, 16]\n",
        "\n",
        "    # We inject this function into the VisionTransformer instances so that\n",
        "    # we can use it with interpolated position embeddings without modifying the library source.\n",
        "    pretrained.model.forward_flex = types.MethodType(forward_flex, pretrained.model)\n",
        "    pretrained.model._resize_pos_embed = types.MethodType(\n",
        "        _resize_pos_embed, pretrained.model\n",
        "    )\n",
        "\n",
        "    return pretrained\n",
        "\n",
        "def _make_pretrained_vitb16_128(\n",
        "    pretrained, use_readout=\"ignore\", hooks=None\n",
        "):\n",
        "    model = timm.create_model(\"vit_base_patch16_224\", img_size=128, pretrained=pretrained)\n",
        "\n",
        "    hooks = [2, 5, 8, 11] if hooks == None else hooks\n",
        "    return _make_vit_b16_backbone(\n",
        "        model,\n",
        "        features=[96, 192, 384, 768],\n",
        "        hooks=hooks,\n",
        "        use_readout=use_readout\n",
        "    )\n",
        "\n",
        "def _make_encoder(\n",
        "    backbone,\n",
        "    features,\n",
        "    use_pretrained,\n",
        "    groups=1,\n",
        "    expand=False,\n",
        "    hooks=None,\n",
        "    use_readout=\"ignore\"\n",
        "):\n",
        "    if backbone == \"vitb16_128\":\n",
        "        pretrained = _make_pretrained_vitb16_128(\n",
        "            use_pretrained,\n",
        "            hooks=hooks,\n",
        "            use_readout=use_readout\n",
        "        )\n",
        "        scratch = _make_scratch(\n",
        "            [96, 192, 384, 768], features, groups=groups, expand=expand\n",
        "        )  # ViT-B/16 - 84.6% Top1 (backbone)\n",
        "    else:\n",
        "        print(f\"Backbone '{backbone}' not implemented\")\n",
        "        assert False\n",
        "\n",
        "    return pretrained, scratch\n",
        "\n",
        "def _make_scratch(in_shape, out_shape, groups=1, expand=False):\n",
        "    scratch = nn.Module()\n",
        "\n",
        "    out_shape1 = out_shape\n",
        "    out_shape2 = out_shape\n",
        "    out_shape3 = out_shape\n",
        "    out_shape4 = out_shape\n",
        "    if expand == True:\n",
        "        out_shape1 = out_shape\n",
        "        out_shape2 = out_shape * 2\n",
        "        out_shape3 = out_shape * 4\n",
        "        out_shape4 = out_shape * 8\n",
        "\n",
        "    scratch.layer1_rn = nn.Conv2d(\n",
        "        in_shape[0],\n",
        "        out_shape1,\n",
        "        kernel_size=3,\n",
        "        stride=1,\n",
        "        padding=1,\n",
        "        bias=False,\n",
        "        groups=groups,\n",
        "    )\n",
        "    scratch.layer2_rn = nn.Conv2d(\n",
        "        in_shape[1],\n",
        "        out_shape2,\n",
        "        kernel_size=3,\n",
        "        stride=1,\n",
        "        padding=1,\n",
        "        bias=False,\n",
        "        groups=groups,\n",
        "    )\n",
        "    scratch.layer3_rn = nn.Conv2d(\n",
        "        in_shape[2],\n",
        "        out_shape3,\n",
        "        kernel_size=3,\n",
        "        stride=1,\n",
        "        padding=1,\n",
        "        bias=False,\n",
        "        groups=groups,\n",
        "    )\n",
        "    scratch.layer4_rn = nn.Conv2d(\n",
        "        in_shape[3],\n",
        "        out_shape4,\n",
        "        kernel_size=3,\n",
        "        stride=1,\n",
        "        padding=1,\n",
        "        bias=False,\n",
        "        groups=groups,\n",
        "    )\n",
        "\n",
        "    return scratch\n",
        "\n",
        "class VIT(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        features=256,\n",
        "        backbone=\"vitb16_128\",\n",
        "        readout=\"project\",\n",
        "        channels_last=False,\n",
        "        train_pos_embed=True,\n",
        "        norm_layer=None,\n",
        "        use_skip_conv=True,\n",
        "    ):\n",
        "\n",
        "        super(VIT, self).__init__()\n",
        "\n",
        "        self.channels_last = channels_last\n",
        "\n",
        "        hooks = {\n",
        "            \"vitb16_128\": [2, 5, 8, 11],\n",
        "        }\n",
        "\n",
        "        # Instantiate backbone and reassemble blocks\n",
        "        self.pretrained, self.scratch = _make_encoder(\n",
        "            backbone,\n",
        "            features,\n",
        "            True,  # Set to true of you want to train from scratch, uses ImageNet weights\n",
        "            groups=1,\n",
        "            expand=False,\n",
        "            hooks=hooks[backbone],\n",
        "            use_readout=readout\n",
        "        )\n",
        "\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        self._norm_layer = norm_layer\n",
        "\n",
        "        self.use_skip_conv = use_skip_conv\n",
        "        if use_skip_conv:\n",
        "            self.scratch.output_conv = nn.Sequential(\n",
        "                nn.ReLU(True),\n",
        "                nn.Conv2d(4*features, features, kernel_size=3, stride=1, padding=1),\n",
        "                nn.ReLU(True),\n",
        "                nn.Conv2d(features, features // 2, kernel_size=3, stride=1, padding=1),\n",
        "            )\n",
        "\n",
        "            downsample = nn.Sequential(\n",
        "                conv1x1(64, features//2, 1),\n",
        "                norm_layer(features//2, track_running_stats=False, affine=True),\n",
        "            ) # HACK\n",
        "\n",
        "            self.scratch.skip_conv = nn.Sequential(\n",
        "                nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False, padding_mode='reflect'),\n",
        "                norm_layer(64, track_running_stats=False, affine=True),\n",
        "                nn.ReLU(inplace=True),\n",
        "                BasicBlock(64, features//2, 1, downsample, norm_layer),\n",
        "                BasicBlock(features//2, features//2, 1, None, norm_layer),\n",
        "                BasicBlock(features//2, features//2, 1, None, norm_layer),\n",
        "            )\n",
        "        else:\n",
        "            self.scratch.output_conv = nn.Sequential(\n",
        "                nn.ReLU(True),\n",
        "                nn.Conv2d(4*features, features, kernel_size=3, stride=1, padding=1),\n",
        "            )\n",
        "\n",
        "        self.pretrained.model.pos_embed.requires_grad = train_pos_embed\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.channels_last == True:\n",
        "            x.contiguous(memory_format=torch.channels_last)\n",
        "\n",
        "        layer_1, layer_2, layer_3, layer_4 = forward_vit(self.pretrained, x)\n",
        "\n",
        "        layer_1_rn = self.scratch.layer1_rn(layer_1)\n",
        "        layer_2_rn = self.scratch.layer2_rn(layer_2)\n",
        "        layer_3_rn = self.scratch.layer3_rn(layer_3)\n",
        "        layer_4_rn = self.scratch.layer4_rn(layer_4)\n",
        "\n",
        "        if self.use_skip_conv:\n",
        "            skip_x = self.scratch.skip_conv(x)\n",
        "            sz = skip_x.shape[-2:]\n",
        "        else:\n",
        "            sz = layer_1.shape[-2:]\n",
        "\n",
        "        new_latents = []\n",
        "        new_latents.append(F.interpolate(\n",
        "            layer_1_rn,\n",
        "            sz,\n",
        "            mode='bilinear',\n",
        "            align_corners=True,\n",
        "        ))\n",
        "\n",
        "        new_latents.append(F.interpolate(\n",
        "            layer_2_rn,\n",
        "            sz,\n",
        "            mode='bilinear',\n",
        "            align_corners=True,\n",
        "        ))\n",
        "\n",
        "        new_latents.append(F.interpolate(\n",
        "            layer_3_rn,\n",
        "            sz,\n",
        "            mode='bilinear',\n",
        "            align_corners=True,\n",
        "        ))\n",
        "\n",
        "        new_latents.append(F.interpolate(\n",
        "            layer_4_rn,\n",
        "            sz,\n",
        "            mode='bilinear',\n",
        "            align_corners=True,\n",
        "        ))\n",
        "\n",
        "        new_latents = torch.cat(new_latents, 1)\n",
        "\n",
        "        out = self.scratch.output_conv(new_latents)\n",
        "\n",
        "        # out = self.scratch.output_conv(path_1)\n",
        "        if self.use_skip_conv:\n",
        "            out = torch.cat([out, skip_x], 1)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "KD7Dly8WzWvl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model/Model.py"
      ],
      "metadata": {
        "id": "rOZgnbBOymAl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "#from network.resnet_mlp import PosEncodeResnet\n",
        "#from network.vit import VIT\n",
        "\n",
        "def de_parallel(model):\n",
        "    return model.module if hasattr(model, 'module') else model\n",
        "\n",
        "\n",
        "class VisionNerfModel(object):\n",
        "    def __init__(self, args, load_opt=True, load_scheduler=True):\n",
        "        self.args = args\n",
        "        device = torch.device('cuda:{}'.format(args.local_rank))\n",
        "\n",
        "        self.freq_factor = np.pi\n",
        "\n",
        "        # create coarse network\n",
        "        pos_c = 3\n",
        "        in_c = args.im_feat_dim + 3 + 3\n",
        "        # create coarse network\n",
        "        self.net_coarse = PosEncodeResnet(args, pos_c, in_c, args.mlp_feat_dim,\n",
        "                                          4, args.mlp_block_num).to(device)\n",
        "        if args.coarse_only:\n",
        "            self.net_fine = None\n",
        "        else:\n",
        "            # create fine network\n",
        "            self.net_fine = PosEncodeResnet(args, pos_c, in_c, args.mlp_feat_dim,\n",
        "                                            4, args.mlp_block_num).to(device)\n",
        "\n",
        "\n",
        "        im_feat = args.im_feat_dim\n",
        "        # create feature extraction network\n",
        "        self.feature_net = VIT(im_feat,\n",
        "                               train_pos_embed=not args.freeze_pos_embed,\n",
        "                               use_skip_conv=not args.no_skip_conv).cuda()\n",
        "\n",
        "        # optimizer and learning rate scheduler\n",
        "        learnable_params = list(self.net_coarse.parameters())\n",
        "        learnable_params += list(self.feature_net.parameters())\n",
        "        if self.net_fine is not None:\n",
        "            learnable_params += list(self.net_fine.parameters())\n",
        "\n",
        "        params = [\n",
        "                {'params': self.net_coarse.parameters()},\n",
        "                {'params': self.feature_net.parameters(), 'lr': args.lrate_feature},\n",
        "            ]\n",
        "\n",
        "        if self.net_fine is not None:\n",
        "            params.append({'params': self.net_fine.parameters()})\n",
        "\n",
        "        self.optimizer = torch.optim.Adam(params, lr=args.lrate_mlp)\n",
        "\n",
        "        if args.scheduler == 'steplr':\n",
        "            self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer,\n",
        "                                                            step_size=args.lrate_decay_steps,\n",
        "                                                            gamma=args.lrate_decay_factor)\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "\n",
        "        if args.use_warmup:\n",
        "            self.warmup_scheduler = torch.optim.lr_scheduler.LambdaLR(self.optimizer,\n",
        "                        lr_lambda=lambda step: np.clip((step+1), 0, args.warmup_steps) / args.warmup_steps)\n",
        "        else:\n",
        "            self.warmup_scheduler = None\n",
        "\n",
        "        out_folder = os.path.join(args.ckptdir, args.expname)\n",
        "        self.start_step = self.load_from_ckpt(out_folder,\n",
        "                                              load_opt=load_opt,\n",
        "                                              load_scheduler=load_scheduler)\n",
        "\n",
        "        if args.distributed:\n",
        "\n",
        "            self.net_coarse = torch.nn.parallel.DistributedDataParallel(\n",
        "                self.net_coarse,\n",
        "                device_ids=[args.local_rank],\n",
        "                output_device=args.local_rank\n",
        "            )\n",
        "\n",
        "            self.feature_net = torch.nn.parallel.DistributedDataParallel(\n",
        "                self.feature_net,\n",
        "                device_ids=[args.local_rank],\n",
        "                output_device=args.local_rank,\n",
        "                find_unused_parameters=True\n",
        "            )\n",
        "\n",
        "            if self.net_fine is not None:\n",
        "                self.net_fine = torch.nn.parallel.DistributedDataParallel(\n",
        "                    self.net_fine,\n",
        "                    device_ids=[args.local_rank],\n",
        "                    output_device=args.local_rank\n",
        "                )\n",
        "\n",
        "    def encode(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: input tensor [b, v, h, w, c]\n",
        "        Returns:\n",
        "            Extracted feature maps [b, out_c, h, w]\n",
        "        \"\"\"\n",
        "        b, v, h, w, c = x.shape\n",
        "        x = x*2. - 1. # Normalization for transformer\n",
        "        feat_maps = self.feature_net(x.reshape([-1, h, w, c]).permute(0, 3, 1, 2))\n",
        "        _, nc, nh, nw = feat_maps.shape\n",
        "        feat_maps = feat_maps.reshape([b, v, nc, nh, nw])\n",
        "        return feat_maps\n",
        "\n",
        "    def posenc(self, x):\n",
        "        freq_multiplier = (\n",
        "            self.freq_factor * 2 ** torch.arange(\n",
        "                                        self.args.freq_num,\n",
        "                                        device=f\"cuda:{self.args.local_rank}\"\n",
        "                                    )\n",
        "        ).view(1, 1, 1, -1)\n",
        "        x_expand = x.unsqueeze(-1)\n",
        "        sin_val = torch.sin(x_expand * freq_multiplier)\n",
        "        cos_val = torch.cos(x_expand * freq_multiplier)\n",
        "        return torch.cat(\n",
        "            [x_expand, sin_val, cos_val], -1\n",
        "        ).view(x.shape[:2] + (-1,))\n",
        "\n",
        "    def switch_to_eval(self):\n",
        "        self.net_coarse.eval()\n",
        "        if self.net_fine is not None:\n",
        "            self.net_fine.eval()\n",
        "        self.feature_net.eval()\n",
        "\n",
        "    def switch_to_train(self):\n",
        "        self.net_coarse.train()\n",
        "        if self.net_fine is not None:\n",
        "            self.net_fine.train()\n",
        "        self.feature_net.train()\n",
        "\n",
        "    def save_model(self, filename):\n",
        "        to_save = {'optimizer': self.optimizer.state_dict(),\n",
        "                   'scheduler': self.scheduler.state_dict(),\n",
        "                   'net_coarse': de_parallel(self.net_coarse).state_dict(),\n",
        "                   'feature_net': de_parallel(self.feature_net).state_dict(),\n",
        "                   }\n",
        "\n",
        "        if self.net_fine is not None:\n",
        "            to_save['net_fine'] = de_parallel(self.net_fine).state_dict()\n",
        "\n",
        "        torch.save(to_save, filename)\n",
        "\n",
        "    def load_model(self, filename, load_opt=True, load_scheduler=True):\n",
        "        if self.args.distributed:\n",
        "            to_load = torch.load(filename, map_location='cuda:{}'.format(self.args.local_rank))\n",
        "        else:\n",
        "            to_load = torch.load(filename)\n",
        "\n",
        "        if load_opt:\n",
        "            self.optimizer.load_state_dict(to_load['optimizer'])\n",
        "        if load_scheduler:\n",
        "            self.scheduler.load_state_dict(to_load['scheduler'])\n",
        "\n",
        "        self.net_coarse.load_state_dict(to_load['net_coarse'])\n",
        "        self.feature_net.load_state_dict(to_load['feature_net'])\n",
        "\n",
        "        if self.net_fine is not None and 'net_fine' in to_load.keys():\n",
        "            self.net_fine.load_state_dict(to_load['net_fine'])\n",
        "\n",
        "    def load_from_ckpt(self, out_folder,\n",
        "                       load_opt=True,\n",
        "                       load_scheduler=True,\n",
        "                       force_latest_ckpt=False):\n",
        "        '''Load model from existing checkpoints and return the current step\n",
        "\n",
        "        Args:\n",
        "            out_folder: the directory that stores ckpts\n",
        "        Returns:\n",
        "            The current starting step\n",
        "        '''\n",
        "\n",
        "        # all existing ckpts\n",
        "        ckpts = []\n",
        "        if os.path.exists(out_folder):\n",
        "            ckpts = [os.path.join(out_folder, f)\n",
        "                     for f in sorted(os.listdir(out_folder)) if f.endswith('.pth')]\n",
        "\n",
        "        if self.args.ckpt_path is not None and not force_latest_ckpt:\n",
        "            if os.path.isfile(self.args.ckpt_path):  # load the specified ckpt\n",
        "                ckpts = [self.args.ckpt_path]\n",
        "\n",
        "        if len(ckpts) > 0 and not self.args.no_reload:\n",
        "            fpath = ckpts[-1]\n",
        "            self.load_model(fpath, load_opt, load_scheduler)\n",
        "            step = int(fpath[-10:-4])\n",
        "            print('Reloading from {}, starting at step={}'.format(fpath, step))\n",
        "        else:\n",
        "            print('No ckpts found, training from scratch...')\n",
        "            step = 0\n",
        "\n",
        "        return step"
      ],
      "metadata": {
        "id": "3OZ0fO-HylPi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model/Projection.py"
      ],
      "metadata": {
        "id": "DItFVlAI1it2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from einops import rearrange, repeat\n",
        "\n",
        "def divide_safe(num, denom):\n",
        "    eps = 1e-8\n",
        "    tmp = denom + eps * torch.le(denom, 1e-20).to(torch.float)\n",
        "    return num / tmp\n",
        "\n",
        "def meshgrid_pinhole(h, w,\n",
        "                    is_homogenous=True, device=None):\n",
        "    '''Create a meshgrid for image coordinate\n",
        "    Args:\n",
        "        h: grid height\n",
        "        w: grid width\n",
        "        is_homogenous: return homogenous or not\n",
        "    Returns:\n",
        "        Image coordinate meshgrid [height, width, 2 (3 if homogenous)]\n",
        "    '''\n",
        "    xs = torch.linspace(0, w-1, steps=w, device=device)\n",
        "    ys = torch.linspace(0, h-1, steps=h, device=device)\n",
        "    new_y, new_x = torch.meshgrid(ys, xs)\n",
        "    grid = (new_x, new_y)\n",
        "\n",
        "    if is_homogenous:\n",
        "        ones = torch.ones_like(new_x)\n",
        "        grid = torch.stack(grid + (ones, ), 2)\n",
        "    else:\n",
        "        grid = torch.stack(grid, 2)\n",
        "    return grid\n",
        "\n",
        "def normalize(pixel_locations, h, w):\n",
        "    resize_factor = torch.tensor([w-1., h-1.], device=pixel_locations.device).view([1, 1, 1, 1, 2])\n",
        "    normalized_pixel_locations = 2 * pixel_locations / resize_factor - 1.  # [n_views, n_points, 2]\n",
        "    return normalized_pixel_locations\n",
        "\n",
        "\n",
        "class Projector():\n",
        "    def __init__(self, device):\n",
        "        self.device = device\n",
        "\n",
        "    def normalize(self, pixel_locations, h, w):\n",
        "        resize_factor = torch.tensor([w-1., h-1.], device=pixel_locations.device).view([1, 1, 1, 1, 2])\n",
        "        normalized_pixel_locations = 2 * pixel_locations / resize_factor - 1.  # [n_views, n_points, 2]\n",
        "        return normalized_pixel_locations\n",
        "\n",
        "    def normalize_pts(self, xyz, bbox3d):\n",
        "        near = bbox3d[:, None, None, None, :, 1]\n",
        "        far = bbox3d[:, None, None, None, :, 0]\n",
        "        normalized_voxel_locations = 2 * (xyz - near) / (far-near) - 1.\n",
        "        return normalized_voxel_locations\n",
        "\n",
        "    def compute_projections(self, xyz, train_ints, train_exts):\n",
        "        '''Project 3D points into cameras\n",
        "        Args:\n",
        "            xyz: [batch, N_rays, N_samples, 3]\n",
        "            train_ints: intrinsics [batch, num_views, 4, 4]\n",
        "            train_exts: extrinsics [batch, num_views, 4, 4]\n",
        "        Returns:\n",
        "            Pixel locations [batch, #views, N_rays, N_samples, 2], xyz_c [batch, #views, N_rays*N_samples, 4]\n",
        "        '''\n",
        "        batch, N_rays, N_samples, _ = xyz.shape\n",
        "        xyz = xyz.reshape(batch, -1, 3)  # [batch, n_points, 3]\n",
        "        num_views = train_ints.shape[1]\n",
        "        train_intrinsics = train_ints  # [batch, n_views, 4, 4]\n",
        "        train_poses = train_exts  # [batch, n_views, 4, 4]\n",
        "        xyz_h = torch.cat([xyz, torch.ones_like(xyz[..., :1])], dim=-1)  # [batch, n_points, 4]\n",
        "\n",
        "        xyz_c = torch.inverse(train_poses) @ (xyz_h.permute([0, 2, 1])[:, None].repeat(1, num_views, 1, 1)) # camera_coodrinates\n",
        "        projections = train_intrinsics @ xyz_c # [batch, n_views, 4, n_points]\n",
        "        projections = projections.permute(0, 1, 3, 2)  # [batch, n_views, n_points, 4]\n",
        "        pixel_locations = projections[..., :2] / torch.clamp(projections[..., 2:3], min=1e-8)  # [batch, n_views, n_points, 2]\n",
        "        pixel_locations = torch.clamp(pixel_locations, min=-1e6, max=1e6)\n",
        "        return pixel_locations.reshape((batch, num_views, N_rays, N_samples, 2)), \\\n",
        "               xyz_c.permute(0, 1, 3, 2).reshape((batch, num_views, N_rays, N_samples, 4))\n",
        "\n",
        "    def compute_directions(self, dir, train_exts):\n",
        "        '''Transform view directions from world to camera coordinates\n",
        "        Args:\n",
        "            dir: [batch, N_rays, N_samples, 3]\n",
        "            train_exts: extrinsics [batch, num_views, 4, 4]\n",
        "        Returns:\n",
        "            Viewing direction in camera coordinates [batch, #views, N_rays*N_samples, 3]\n",
        "        '''\n",
        "        _, N_rays, N_samples, _ = dir.shape\n",
        "        num_views = train_exts.shape[1]\n",
        "        dir = repeat(dir, 'b nr ns c -> b nv c (nr ns)', nv=num_views)\n",
        "        train_poses = train_exts[..., :3, :3]  # [batch, n_views, 4, 4]\n",
        "        dir_c = torch.inverse(train_poses) @ (dir)\n",
        "        dir_c = rearrange(dir_c, 'b nv c (nr ns) -> b nv nr ns c', nr=N_rays, ns=N_samples)\n",
        "        return dir_c\n",
        "\n",
        "    def compute_pixel(self,  xyz, train_imgs, train_ints, train_exts, featmaps):\n",
        "        '''Original pixelNeRF projection (2D -> samples)\n",
        "        Args:\n",
        "            xyz: [batch, n_rays, n_samples, 3]\n",
        "            train_imgs: [batch, n_views, h, w, 3]\n",
        "            train_ints: [batch, n_views, 4, 4]\n",
        "            train_exts: [batch, n_views, 4, 4]\n",
        "            featmaps: [batch, n_views, c, h, w]\n",
        "        Returns: rgb_feat_sampled: [batch, n_rays, n_samples, n_views, 3+n_feat],\n",
        "                 xyz_c: [batch, n_views, n_rays, n_samples, 4]\n",
        "        '''\n",
        "        _, views, h, w = train_imgs.shape[:-1]\n",
        "\n",
        "        train_imgs = train_imgs.permute(0, 1, 4, 2, 3)  # [batch, n_views, 3, h, w]\n",
        "        train_imgs = train_imgs * 2. - 1. # normalization\n",
        "\n",
        "        # compute the projection of the query points to each reference image\n",
        "        pixel_locations, xyz_c = self.compute_projections(xyz, train_ints, train_exts)\n",
        "        normalized_pixel_locations = self.normalize(pixel_locations, h, w)   # [batch, n_views, n_rays, n_samples, 2]\n",
        "        N_rays, N_samples = normalized_pixel_locations.shape[2:4]\n",
        "\n",
        "        # rgb sampling\n",
        "        rgbs_sampled = F.grid_sample(train_imgs.flatten(0, 1), normalized_pixel_locations.flatten(0, 1), align_corners=True)\n",
        "        rgb_sampled = rearrange(rgbs_sampled, '(b v) c nr ns -> b nr ns v c', v=views)\n",
        "\n",
        "        # deep feature sampling\n",
        "        feat_sampled = F.grid_sample(featmaps.flatten(0, 1), normalized_pixel_locations.flatten(0, 1), align_corners=True)\n",
        "        feat_sampled = rearrange(feat_sampled, '(b v) c nr ns -> b nr ns v c', v=views)\n",
        "        rgb_feat_sampled = torch.cat([rgb_sampled, feat_sampled], dim=-1)   # [batch, n_rays, n_samples, n_views, c+3]\n",
        "\n",
        "        return rgb_feat_sampled, xyz_c"
      ],
      "metadata": {
        "id": "brvBO-Lwy0q-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model/Render_ray.py"
      ],
      "metadata": {
        "id": "ObOsO4-s2HlF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import OrderedDict\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from einops import rearrange, repeat\n",
        "\n",
        "########################################################################################################################\n",
        "# helper functions for nerf ray rendering\n",
        "########################################################################################################################\n",
        "\n",
        "\n",
        "def sample_pdf(bins, weights, N_samples, det=False):\n",
        "    '''\n",
        "    Args:\n",
        "        bins: tensor of shape [batch, N_rays, M+1], M is the number of bins\n",
        "        weights: tensor of shape [batch, N_rays, M]\n",
        "        N_samples: number of samples along each ray\n",
        "        det: if True, will perform deterministic sampling\n",
        "    Returns: [batch, N_rays, N_samples]\n",
        "    '''\n",
        "\n",
        "    batch = bins.shape[0]\n",
        "    M = weights.shape[-1]\n",
        "    weights += 1e-5\n",
        "    # Get pdf\n",
        "    pdf = weights / torch.sum(weights, dim=-1, keepdim=True)    # [batch, N_rays, M]\n",
        "    cdf = torch.cumsum(pdf, dim=-1)  # [batch, N_rays, M]\n",
        "    cdf = torch.cat([torch.zeros_like(cdf[..., 0:1]), cdf], dim=-1) # [batch, N_rays, M+1]\n",
        "\n",
        "    # Take uniform samples\n",
        "    if det:\n",
        "        u = torch.linspace(0., 1., N_samples, device=bins.device)\n",
        "        u = u[None, None, :].repeat(bins.shape[:2] + (1,))       # [batch, N_rays, N_samples]\n",
        "    else:\n",
        "        u = torch.rand(batch, bins.shape[1], N_samples, device=bins.device)\n",
        "\n",
        "    # Invert CDF\n",
        "    above_inds = torch.zeros_like(u, dtype=torch.long)       # [batch, N_rays, N_samples]\n",
        "    for i in range(M):\n",
        "        above_inds += (u >= cdf[..., i:i+1]).long()\n",
        "\n",
        "    # random sample inside each bin\n",
        "    below_inds = torch.clamp(above_inds-1, min=0)\n",
        "    inds_g = torch.stack((below_inds, above_inds), dim=-1)     # [batch, N_rays, N_samples, 2]\n",
        "\n",
        "    cdf = cdf.unsqueeze(2).repeat(1, 1, N_samples, 1)  # [batch, N_rays, N_samples, M+1]\n",
        "    cdf_g = torch.gather(input=cdf, dim=-1, index=inds_g)  # [batch, N_rays, N_samples, 2]\n",
        "\n",
        "    bins = bins.unsqueeze(2).repeat(1, 1, N_samples, 1)  # [batch, N_rays, N_samples, M+1]\n",
        "    bins_g = torch.gather(input=bins, dim=-1, index=inds_g)  # [batch, N_rays, N_samples, 2]\n",
        "\n",
        "    # t = (u-cdf_g[:, :, 0]) / (cdf_g[:, :, 1] - cdf_g[:, :, 0] + TINY_NUMBER)  # [N_rays, N_samples]\n",
        "    # fix numeric issue\n",
        "    denom = cdf_g[..., 1] - cdf_g[..., 0]      # [batch, N_rays, N_samples]\n",
        "    denom = torch.where(denom < 1e-5, torch.ones_like(denom), denom)\n",
        "    t = (u - cdf_g[..., 0]) / denom\n",
        "\n",
        "    samples = bins_g[..., 0] + t * (bins_g[..., 1]-bins_g[..., 0])\n",
        "\n",
        "    return samples\n",
        "\n",
        "def sample_along_camera_ray(ray_o, ray_d, depth_range,\n",
        "                            N_samples,\n",
        "                            inv_uniform=False,\n",
        "                            det=False):\n",
        "    '''\n",
        "    :param ray_o: origin of the ray in scene coordinate system; tensor of shape [N_rays, 3] or [Batch, N_rays, 3]\n",
        "    :param ray_d: homogeneous ray direction vectors in scene coordinate system; tensor of shape [N_rays, 3] or [Batch, N_rays, 3]\n",
        "    :param depth_range: [B, 2] (near_depth, far_depth)\n",
        "    :param inv_uniform: if True, uniformly sampling inverse depth\n",
        "    :param det: if True, will perform deterministic sampling\n",
        "    :return: tensor of shape [Batch, N_rays, N_samples, 3]\n",
        "    '''\n",
        "\n",
        "    if ray_o.ndim == 2:\n",
        "        ray_o = ray_o[None, :]\n",
        "    if ray_d.ndim == 2:\n",
        "        ray_d = ray_d[None, :]\n",
        "\n",
        "    # will sample inside [near_depth, far_depth]\n",
        "    # assume the nearest possible depth is at least (min_ratio * depth)\n",
        "    near_depth_value = depth_range[:, 0]\n",
        "    far_depth_value = depth_range[:, 1]\n",
        "    assert torch.all(near_depth_value > 0) and torch.all(far_depth_value > 0) and torch.all(far_depth_value > near_depth_value)\n",
        "\n",
        "    near_depth = near_depth_value[..., None] * torch.ones_like(ray_d[..., 0])\n",
        "\n",
        "    far_depth = far_depth_value[..., None] * torch.ones_like(ray_d[..., 0])\n",
        "    if inv_uniform:\n",
        "        start = 1. / near_depth     # [Batch, N_rays,]\n",
        "        step = (1. / far_depth - start) / (N_samples-1)\n",
        "        inv_z_vals = torch.stack([start+i*step for i in range(N_samples)], dim=-1)  # [Batch, N_rays, N_samples]\n",
        "        z_vals = 1. / inv_z_vals\n",
        "    else:\n",
        "        start = near_depth\n",
        "        step = (far_depth - near_depth) / (N_samples-1)\n",
        "        z_vals = torch.stack([start+i*step for i in range(N_samples)], dim=-1)  # [Batch, N_rays, N_samples]\n",
        "\n",
        "    if not det:\n",
        "        # get intervals between samples\n",
        "        mids = .5 * (z_vals[..., 1:] + z_vals[..., :-1])\n",
        "        upper = torch.cat([mids, z_vals[..., -1:]], dim=-1)\n",
        "        lower = torch.cat([z_vals[..., 0:1], mids], dim=-1)\n",
        "        # uniform samples in those intervals\n",
        "        t_rand = torch.rand_like(z_vals)\n",
        "        z_vals = lower + (upper - lower) * t_rand   # [N_rays, N_samples]\n",
        "\n",
        "    ray_d = ray_d.unsqueeze(2).repeat(1, 1, N_samples, 1)  # [N_rays, N_samples, 3]\n",
        "    ray_o = ray_o.unsqueeze(2).repeat(1, 1, N_samples, 1)\n",
        "    pts = z_vals.unsqueeze(-1) * ray_d + ray_o       # [N_rays, N_samples, 3]\n",
        "    return pts, z_vals\n",
        "\n",
        "\n",
        "########################################################################################################################\n",
        "# ray rendering of nerf\n",
        "########################################################################################################################\n",
        "\n",
        "def raw2outputs(raw, z_vals, white_bkgd=False):\n",
        "    '''\n",
        "    Args:\n",
        "        raw: raw network output; tensor of shape [batch, N_rays, N_samples, 4]\n",
        "        z_vals: depth of point samples along rays; tensor of shape [batch, N_rays, N_samples]\n",
        "    Returns:\n",
        "        {'rgb': [batch, N_rays, 3], 'depth': batch, [N_rays,], 'weights': [batch, N_rays,]}\n",
        "    '''\n",
        "    rgb = raw[..., :3]     # [batch, N_rays, N_samples, 3]\n",
        "    sigma = raw[..., 3]    # [batch, N_rays, N_samples]\n",
        "\n",
        "    # Changed to include dists to imitate pixelnerf\n",
        "    sigma2alpha = lambda sigma, dists: 1. - torch.exp(-dists * torch.relu(sigma))\n",
        "\n",
        "    # point samples are ordered with increasing depth\n",
        "    # interval between samples\n",
        "    dists = z_vals[..., 1:] - z_vals[..., :-1]\n",
        "    dists = torch.cat((dists, dists[..., -1:]), dim=-1)  # [batch, N_rays, N_samples]\n",
        "\n",
        "    alpha = sigma2alpha(sigma, dists)  # [batch, N_rays, N_samples]\n",
        "\n",
        "    # Eq. (3): T\n",
        "    T = torch.cumprod(1. - alpha + 1e-10, dim=-1)[..., :-1]   # [batch, N_rays, N_samples-1]\n",
        "    T = torch.cat((torch.ones_like(T[..., 0:1]), T), dim=-1)  # [batch, N_rays, N_samples]\n",
        "\n",
        "    # maths show weights, and summation of weights along a ray, are always inside [0, 1]\n",
        "    weights = alpha * T     # [N_rays, N_samples]\n",
        "    rgb_map = torch.sum(weights.unsqueeze(-1) * rgb, dim=2)  # [N_rays, 3]\n",
        "\n",
        "    if white_bkgd:\n",
        "        rgb_map = rgb_map + (1. - torch.sum(weights, dim=-1, keepdim=True))\n",
        "\n",
        "    depth_map = torch.sum(weights * z_vals, dim=-1)     # [N_rays,]\n",
        "\n",
        "    ret = OrderedDict([('rgb', rgb_map),\n",
        "                       ('depth', depth_map),\n",
        "                       ('weights', weights),                # used for importance sampling of fine samples\n",
        "                       ('mask', torch.ones_like(rgb_map[..., 0])),\n",
        "                       ('alpha', alpha),\n",
        "                       ('z_vals', z_vals)\n",
        "                       ])\n",
        "\n",
        "    return ret\n",
        "\n",
        "\n",
        "def render_rays(ray_batch,\n",
        "                model,\n",
        "                featmaps,\n",
        "                projector,\n",
        "                N_samples,\n",
        "                inv_uniform=False,\n",
        "                N_importance=0,\n",
        "                det=False,\n",
        "                white_bkgd=False):\n",
        "    '''\n",
        "    Args:\n",
        "        ray_batch: {'ray_o': [batch, N_rays, 3] , 'ray_d': [batch, N_rays, 3], 'view_dir': [batch, N_rays, 2]}\n",
        "        model:  {'net_coarse':  , 'net_fine': }\n",
        "        featmaps: feature maps for inference [b, c, h, w] or [b, c, d, h, w]\n",
        "        projector: projector object\n",
        "        N_samples: samples along each ray (for both coarse and fine model)\n",
        "        inv_uniform: if True, uniformly sample inverse depth for coarse model\n",
        "        det: if True, will deterministicly sample depths\n",
        "        white_bkgd: if True, assume background is white\n",
        "    Return:\n",
        "        {'outputs_coarse': {}, 'outputs_fine': {}}\n",
        "    '''\n",
        "    ret = {'outputs_coarse': None,\n",
        "           'outputs_fine': None}\n",
        "\n",
        "    # pts: [batch, N_rays, N_samples, 3]\n",
        "    # z_vals: [batch, N_rays, N_samples]\n",
        "    pts, z_vals = sample_along_camera_ray(ray_o=ray_batch['ray_o'],\n",
        "                                          ray_d=ray_batch['ray_d'],\n",
        "                                          depth_range=ray_batch['depth_range'],\n",
        "                                          N_samples=N_samples, inv_uniform=inv_uniform, det=det)\n",
        "    batch, N_rays, N_samples = pts.shape[:3]\n",
        "\n",
        "    rgb_feat, xyz_c = projector.compute_pixel(pts, ray_batch['src_rgbs'],\n",
        "                                                ray_batch['src_intrinsics'],\n",
        "                                                ray_batch['src_c2w_mats'],\n",
        "                                                featmaps=featmaps)  # [batch, N_rays, N_samples, N_views, x]\n",
        "\n",
        "    xyz_c = xyz_c[:, 0, ..., :3] # HACK only use the first view\n",
        "    rgb_feat = rgb_feat.squeeze(3) # HACK consider only one camera now so remove the axis\n",
        "    dir = repeat(ray_batch['ray_d'], 'b nr c -> b nr ns c', ns=N_samples)\n",
        "    dir_c = projector.compute_directions(dir, ray_batch['src_c2w_mats'])\n",
        "    dir_c = dir_c[:, 0, ..., :3] # HACK only use the first view\n",
        "\n",
        "    feat = torch.cat([rgb_feat, dir_c], -1)\n",
        "    raw_coarse = model.net_coarse(xyz_c.flatten(0, 1), feat.flatten(0, 1))   # [batch*N_rays*N_samples, 4]\n",
        "    raw_coarse = raw_coarse.reshape([batch, N_rays, N_samples, 4])\n",
        "    outputs_coarse = raw2outputs(raw_coarse, z_vals,\n",
        "                                 white_bkgd=white_bkgd)\n",
        "    ret['outputs_coarse'] = outputs_coarse\n",
        "\n",
        "    if N_importance > 0:\n",
        "        assert model.net_fine is not None\n",
        "        # detach since we would like to decouple the coarse and fine networks\n",
        "        weights = outputs_coarse['weights'].clone().detach()            # [batch, N_rays, N_samples]\n",
        "        if inv_uniform:\n",
        "            inv_z_vals = 1. / z_vals\n",
        "            inv_z_vals_mid = .5 * (inv_z_vals[..., 1:] + inv_z_vals[..., :-1])   # [batch, N_rays, N_samples-1]\n",
        "            weights = weights[..., 1:-1]      # [batch, N_rays, N_samples-2]\n",
        "            inv_z_vals = sample_pdf(bins=torch.flip(inv_z_vals_mid, dims=[-1]),\n",
        "                                    weights=torch.flip(weights, dims=[-1]),\n",
        "                                    N_samples=N_importance, det=det)  # [batch, N_rays, N_importance]\n",
        "            z_samples = 1. / inv_z_vals\n",
        "        else:\n",
        "            # take mid-points of depth samples\n",
        "            z_vals_mid = .5 * (z_vals[..., 1:] + z_vals[..., :-1])   # [batch, N_rays, N_samples-1]\n",
        "            weights = weights[..., 1:-1]      # [N_rays, N_samples-2]\n",
        "            z_samples = sample_pdf(bins=z_vals_mid, weights=weights,\n",
        "                                   N_samples=N_importance, det=det)  # [batch, N_rays, N_importance]\n",
        "\n",
        "        z_vals = torch.cat((z_vals, z_samples), dim=-1)  # [batch, N_rays, N_samples + N_importance]\n",
        "\n",
        "        # samples are sorted with increasing depth\n",
        "        z_vals, _ = torch.sort(z_vals, dim=-1)\n",
        "        N_total_samples = N_samples + N_importance\n",
        "\n",
        "        viewdirs = ray_batch['ray_d'][:, :, None].expand([-1, -1, N_total_samples, -1])\n",
        "        ray_o = ray_batch['ray_o'][:, :, None].repeat(1, 1, N_total_samples, 1)\n",
        "        pts = z_vals.unsqueeze(-1) * viewdirs + ray_o  # [batch, N_rays, N_samples + N_importance, 3]\n",
        "\n",
        "        rgb_feat_sampled, xyz_c = projector.compute_pixel(pts, ray_batch['src_rgbs'],\n",
        "                                                    ray_batch['src_intrinsics'],\n",
        "                                                    ray_batch['src_c2w_mats'],\n",
        "                                                    featmaps=featmaps)  # [batch, N_rays, N_samples, N_views, x]\n",
        "\n",
        "        xyz_c = xyz_c[:, 0, ..., :3] # HACK only use the first view\n",
        "        rgb_feat_sampled = rgb_feat_sampled.squeeze(3) # HACK consider only one camera now so remove the axis\n",
        "        dir = repeat(ray_batch['ray_d'], 'b nr c -> b nr ns c', ns=N_total_samples)\n",
        "        dir_c = projector.compute_directions(dir, ray_batch['src_c2w_mats'])\n",
        "        dir_c = dir_c[:, 0, ..., :3] # HACK only use the first view\n",
        "\n",
        "        feat = torch.cat([rgb_feat_sampled, dir_c], -1)\n",
        "        raw_fine = model.net_fine(xyz_c.flatten(0, 1), feat.flatten(0, 1))   # [batch*N_rays*N_samples, 4]\n",
        "        raw_fine = raw_fine.reshape([batch, N_rays, N_total_samples, 4])\n",
        "        outputs_fine = raw2outputs(raw_fine, z_vals,\n",
        "                                   white_bkgd=white_bkgd)\n",
        "        ret['outputs_fine'] = outputs_fine\n",
        "\n",
        "    return ret"
      ],
      "metadata": {
        "id": "9nJNVHeh2HRa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model/Render_image.py"
      ],
      "metadata": {
        "id": "NuxRo4iW17vv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import OrderedDict\n",
        "\n",
        "import torch\n",
        "\n",
        "\n",
        "def render_single_image(ray_sampler,\n",
        "                        ray_batch,\n",
        "                        model,\n",
        "                        featmaps,\n",
        "                        projector,\n",
        "                        chunk_size,\n",
        "                        N_samples,\n",
        "                        inv_uniform=False,\n",
        "                        N_importance=0,\n",
        "                        det=False,\n",
        "                        white_bkgd=False,\n",
        "                        render_stride=1):\n",
        "    '''\n",
        "    Args:\n",
        "        ray_sampler: RaySamplingSingleImage for this view\n",
        "        ray_batch: {'ray_o': [N_rays, 3] , 'ray_d': [N_rays, 3], 'view_dir': [N_rays, 2]}\n",
        "        model:  {'net_coarse': , 'net_fine': , ...}\n",
        "        chunk_size: number of rays in a chunk\n",
        "        N_samples: samples along each ray (for both coarse and fine model)\n",
        "        inv_uniform: if True, uniformly sample inverse depth for coarse model\n",
        "        det: if True, use deterministic sampling\n",
        "        white_bkgd: if True, assume background is white\n",
        "        render_stride: stride for rendering\n",
        "        featmaps: feature maps for inference [b, c, h, w] or [b, c, d, h, w]\n",
        "    Return:\n",
        "        {'outputs_coarse': {'rgb': numpy, 'depth': numpy, ...}, 'outputs_fine': {}}\n",
        "    '''\n",
        "\n",
        "    all_ret = OrderedDict([('outputs_coarse', OrderedDict()),\n",
        "                           ('outputs_fine', OrderedDict())])\n",
        "\n",
        "    N_rays = ray_batch['ray_o'].shape[0]\n",
        "\n",
        "    for i in range(0, N_rays, chunk_size):\n",
        "        chunk = OrderedDict()\n",
        "        for k in ray_batch:\n",
        "            if k in ['intrinsics', 'c2w_mat', 'depth_range',\n",
        "                     'src_rgbs', 'src_intrinsics', 'src_c2w_mats']:\n",
        "                chunk[k] = ray_batch[k]\n",
        "            elif ray_batch[k] is not None:\n",
        "                chunk[k] = ray_batch[k][None, i:i+chunk_size]\n",
        "            else:\n",
        "                chunk[k] = None\n",
        "\n",
        "        ret = render_rays(chunk, model, featmaps,\n",
        "                          projector=projector,\n",
        "                          N_samples=N_samples,\n",
        "                          inv_uniform=inv_uniform,\n",
        "                          N_importance=N_importance,\n",
        "                          det=det,\n",
        "                          white_bkgd=white_bkgd)\n",
        "\n",
        "        # handle both coarse and fine outputs\n",
        "        # cache chunk results on cpu\n",
        "        if i == 0:\n",
        "            for k in ret['outputs_coarse']:\n",
        "                all_ret['outputs_coarse'][k] = []\n",
        "\n",
        "            if ret['outputs_fine'] is None:\n",
        "                all_ret['outputs_fine'] = None\n",
        "            else:\n",
        "                for k in ret['outputs_fine']:\n",
        "                    all_ret['outputs_fine'][k] = []\n",
        "\n",
        "        for k in ret['outputs_coarse']:\n",
        "            all_ret['outputs_coarse'][k].append(ret['outputs_coarse'][k].squeeze(0).cpu())\n",
        "\n",
        "        if ret['outputs_fine'] is not None:\n",
        "            for k in ret['outputs_fine']:\n",
        "                all_ret['outputs_fine'][k].append(ret['outputs_fine'][k].squeeze(0).cpu())\n",
        "\n",
        "    rgb_strided = torch.ones(ray_sampler.H, ray_sampler.W, 3)[::render_stride, ::render_stride, :]\n",
        "    # merge chunk results and reshape\n",
        "    for k in all_ret['outputs_coarse']:\n",
        "        if k == 'random_sigma':\n",
        "            continue\n",
        "        tmp = torch.cat(all_ret['outputs_coarse'][k], dim=0).reshape((rgb_strided.shape[0],\n",
        "                                                                      rgb_strided.shape[1], -1))\n",
        "        all_ret['outputs_coarse'][k] = tmp.squeeze()\n",
        "\n",
        "    if all_ret['outputs_fine'] is not None:\n",
        "        for k in all_ret['outputs_fine']:\n",
        "            if k == 'random_sigma':\n",
        "                continue\n",
        "            tmp = torch.cat(all_ret['outputs_fine'][k], dim=0).reshape((rgb_strided.shape[0],\n",
        "                                                                        rgb_strided.shape[1], -1))\n",
        "\n",
        "            all_ret['outputs_fine'][k] = tmp.squeeze()\n",
        "\n",
        "    return all_ret"
      ],
      "metadata": {
        "id": "8bNh01QA1lMu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model/Sample_ray.py"
      ],
      "metadata": {
        "id": "uNGGx_WQ2TMV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "rng = np.random.RandomState(234)\n",
        "\n",
        "\n",
        "def bbox_sample(bboxes, N_rand):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        bboxes: bounding box value (xmin, ymin, xmax, ymax) [batch, 4]\n",
        "        N_rand: number of pixels to sample\n",
        "    Returns:\n",
        "        Pixel indices to sample from\n",
        "    \"\"\"\n",
        "    x = (\n",
        "        torch.rand(N_rand) * (bboxes[2] + 1 - bboxes[0])\n",
        "        + bboxes[0]\n",
        "    ).long()\n",
        "    y = (\n",
        "        torch.rand(N_rand) * (bboxes[3] + 1 - bboxes[1])\n",
        "        + bboxes[1]\n",
        "    ).long()\n",
        "    return y, x\n",
        "\n",
        "def bbox_sample_full(bboxes, N_rand, h=128, w=128, prob=0.8):\n",
        "    \"\"\"Bounding box sampling but includes other parts of the images\n",
        "    Args:\n",
        "        bboxes: bounding box value (xmin, ymin, xmax, ymax) [batch, 4]\n",
        "        N_rand: number of pixels to sample\n",
        "        h: image height\n",
        "        w: image width\n",
        "        prob: probability of choosing samples inside the bbox\n",
        "    Returns:\n",
        "        Pixel indices to sample from\n",
        "    \"\"\"\n",
        "    N_in = int(N_rand * prob)\n",
        "    N_out = N_rand - N_in\n",
        "\n",
        "    x = (\n",
        "        torch.rand(N_in) * (bboxes[2] + 1 - bboxes[0])\n",
        "        + bboxes[0]\n",
        "    ).long()\n",
        "    y = (\n",
        "        torch.rand(N_in) * (bboxes[3] + 1 - bboxes[1])\n",
        "        + bboxes[1]\n",
        "    ).long()\n",
        "\n",
        "\n",
        "    x_out = (\n",
        "        torch.rand(N_out) * w\n",
        "    ).long()\n",
        "\n",
        "    y_out = (\n",
        "        torch.rand(N_out) * h\n",
        "    ).long()\n",
        "\n",
        "    y = torch.cat([y, y_out])\n",
        "    x = torch.cat([x, x_out])\n",
        "\n",
        "    return y, x\n",
        "\n",
        "########################################################################################################################\n",
        "# ray batch sampling\n",
        "########################################################################################################################\n",
        "\n",
        "class RaySamplerSingleImage(object):\n",
        "    def __init__(self, data, device, resize_factor=1, render_stride=1):\n",
        "        super().__init__()\n",
        "        self.render_stride = render_stride\n",
        "        self.rgb = data['tgt_rgb'] if 'tgt_rgb' in data.keys() else None\n",
        "        self.intrinsics = data['tgt_intrinsic']\n",
        "        self.c2w_mat = data['tgt_c2w_mat']\n",
        "        self.rgb_path = data['rgb_path']\n",
        "        self.depth_range = data['depth_range']\n",
        "        self.device = device\n",
        "        self.batch_size = len(self.intrinsics)\n",
        "\n",
        "        self.H = int(data['img_hw'][0])\n",
        "        self.W = int(data['img_hw'][1])\n",
        "\n",
        "        # half-resolution output\n",
        "        if resize_factor != 1:\n",
        "            self.W = int(self.W * resize_factor)\n",
        "            self.H = int(self.H * resize_factor)\n",
        "            self.intrinsics[:, :2, :3] *= resize_factor\n",
        "            if self.rgb is not None:\n",
        "                self.rgb = F.interpolate(self.rgb.permute(0, 3, 1, 2), scale_factor=resize_factor).permute(0, 2, 3, 1)\n",
        "\n",
        "        self.rays_o, self.rays_d = self.get_rays_single_image(self.H, self.W, self.intrinsics, self.c2w_mat)\n",
        "        if self.rgb is not None:\n",
        "            self.rgb = self.rgb.reshape(-1, 3)\n",
        "\n",
        "        if 'src_rgbs' in data.keys():\n",
        "            self.src_rgbs = data['src_rgbs']\n",
        "        else:\n",
        "            self.src_rgbs = None\n",
        "        if 'src_masks' in data.keys():\n",
        "            self.src_masks = data['src_masks']\n",
        "        else:\n",
        "            self.src_masks = None\n",
        "        if 'src_intrinsics' in data.keys():\n",
        "            self.src_intrinsics = data['src_intrinsics']\n",
        "        else:\n",
        "            self.src_intrinsics = None\n",
        "        if 'src_c2w_mats' in data.keys():\n",
        "            self.src_c2w_mats = data['src_c2w_mats']\n",
        "        else:\n",
        "            self.src_c2w_mats = None\n",
        "        if 'tgt_bbox' in data.keys():\n",
        "            self.tgt_bbox = data['tgt_bbox']\n",
        "\n",
        "    def get_rays_single_image(self, H, W, intrinsics, c2w):\n",
        "        '''Generate rays for a single image (batch size = 1).\n",
        "\n",
        "        Args:\n",
        "            H: image height\n",
        "            W: image width\n",
        "            intrinsics: 4 by 4 intrinsic matrix\n",
        "            c2w: 4 by 4 camera to world extrinsic matrix\n",
        "        Returns:\n",
        "            Tensors of ray origin and direction.\n",
        "        '''\n",
        "        u, v = np.meshgrid(np.arange(W)[::self.render_stride], np.arange(H)[::self.render_stride])\n",
        "        u = u.reshape(-1).astype(dtype=np.float32)  # + 0.5    # add half pixel\n",
        "        v = v.reshape(-1).astype(dtype=np.float32)  # + 0.5\n",
        "        pixels = np.stack((u, v, np.ones_like(u)), axis=0)  # (3, H*W)\n",
        "        pixels = torch.from_numpy(pixels)\n",
        "        batched_pixels = pixels.unsqueeze(0).repeat(self.batch_size, 1, 1)\n",
        "\n",
        "        rays_d = (c2w[:, :3, :3].bmm(torch.inverse(intrinsics[:, :3, :3])).bmm(batched_pixels)).transpose(1, 2)\n",
        "        rays_d = rays_d.reshape(-1, 3)\n",
        "        rays_o = c2w[:, :3, 3].unsqueeze(1).repeat(1, rays_d.shape[0], 1).reshape(-1, 3)  # B x HW x 3\n",
        "        return rays_o, rays_d\n",
        "\n",
        "    def get_all(self):\n",
        "        ret = {'ray_o': self.rays_o.cuda(),\n",
        "               'ray_d': self.rays_d.cuda(),\n",
        "               'depth_range': self.depth_range.cuda(),\n",
        "               'intrinsics': self.intrinsics.cuda(),\n",
        "               'c2w_mat': self.c2w_mat.cuda(),\n",
        "               'rgb': self.rgb.cuda() if self.rgb is not None else None,\n",
        "               'src_rgbs': self.src_rgbs.cuda() if self.src_rgbs is not None else None,\n",
        "               'src_intrinsics': self.src_intrinsics.cuda() if self.src_intrinsics is not None else None,\n",
        "               'src_c2w_mats': self.src_c2w_mats.cuda() if self.src_c2w_mats is not None else None,\n",
        "               'src_masks': self.src_masks.cuda() if self.src_masks is not None else None,\n",
        "        }\n",
        "        return ret\n",
        "\n",
        "    def sample_random_pixel(self, N_rand, sample_mode, center_ratio=0.8):\n",
        "        if sample_mode == 'center':\n",
        "            border_H = int(self.H * (1 - center_ratio) / 2.)\n",
        "            border_W = int(self.W * (1 - center_ratio) / 2.)\n",
        "\n",
        "            # pixel coordinates\n",
        "            u, v = np.meshgrid(np.arange(border_H, self.H - border_H),\n",
        "                               np.arange(border_W, self.W - border_W))\n",
        "            u = u.reshape(-1)\n",
        "            v = v.reshape(-1)\n",
        "\n",
        "            select_inds = rng.choice(u.shape[0], size=(N_rand,), replace=False)\n",
        "            select_inds = v[select_inds] + self.W * u[select_inds]\n",
        "\n",
        "        elif sample_mode == 'uniform':\n",
        "            # Random from one image\n",
        "            select_inds = rng.choice(self.H*self.W, size=(N_rand,), replace=False)\n",
        "        else:\n",
        "            raise Exception(\"unknown sample mode!\")\n",
        "\n",
        "        return select_inds\n",
        "\n",
        "    def random_sample(self, N_rand, sample_mode, center_ratio=0.8):\n",
        "        '''Generate a bundle of randomly sampled rays.\n",
        "        Args:\n",
        "            N_rand: number of rays to be casted\n",
        "        Returns:\n",
        "            A dictionary of ray information.\n",
        "        '''\n",
        "\n",
        "        select_inds = self.sample_random_pixel(N_rand, sample_mode, center_ratio)\n",
        "\n",
        "        rays_o = self.rays_o[select_inds]\n",
        "        rays_d = self.rays_d[select_inds]\n",
        "\n",
        "        if self.rgb is not None:\n",
        "            rgb = self.rgb[select_inds]\n",
        "        else:\n",
        "            rgb = None\n",
        "\n",
        "        ret = {'ray_o': rays_o.cuda(),\n",
        "               'ray_d': rays_d.cuda(),\n",
        "               'intrinsics': self.intrinsics.cuda(),\n",
        "               'c2w_mat': self.c2w_mat.cuda(),\n",
        "               'depth_range': self.depth_range.cuda(),\n",
        "               'rgb': rgb.cuda() if rgb is not None else None,\n",
        "               'src_rgbs': self.src_rgbs.cuda() if self.src_rgbs is not None else None,\n",
        "               'src_intrinsics': self.src_intrinsics.cuda() if self.src_intrinsics is not None else None,\n",
        "               'src_c2w_mats': self.src_c2w_mats.cuda() if self.src_c2w_mats is not None else None,\n",
        "               'selected_inds': select_inds,\n",
        "               'src_masks': self.src_masks.cuda() if self.src_masks is not None else None,\n",
        "        }\n",
        "        return ret\n",
        "\n",
        "class RaySamplerMultipleImages(object):\n",
        "    \"\"\"Ray sampler for multiple images (batch size > 1)\n",
        "    \"\"\"\n",
        "    def __init__(self, data, device, cur_step, resize_factor=1, render_stride=1, bbox_steps=100000):\n",
        "        super().__init__()\n",
        "        self.render_stride = render_stride\n",
        "        self.rgb = data['tgt_rgb'] if 'tgt_rgb' in data.keys() else None # [b, h, w, 3]\n",
        "        self.intrinsics = data['tgt_intrinsic'] # [b, 4, 4]\n",
        "        self.c2w_mat = data['tgt_c2w_mat'] # [b, 4, 4]\n",
        "        self.rgb_path = data['rgb_path']\n",
        "        self.depth_range = data['depth_range'] # [b, 2]\n",
        "        self.device = device\n",
        "        self.batch_size = len(self.intrinsics)\n",
        "        self.cur_step = cur_step\n",
        "        self.bbox_steps = bbox_steps\n",
        "\n",
        "        self.H = int(data['img_hw'][0][0])\n",
        "        self.W = int(data['img_hw'][1][0])\n",
        "\n",
        "        # half-resolution output\n",
        "        if resize_factor != 1:\n",
        "            self.W = int(self.W * resize_factor)\n",
        "            self.H = int(self.H * resize_factor)\n",
        "            self.intrinsics[:, :2, :3] *= resize_factor\n",
        "            if self.rgb is not None:\n",
        "                self.rgb = F.interpolate(self.rgb.permute(0, 3, 1, 2), scale_factor=resize_factor).permute(0, 2, 3, 1)\n",
        "\n",
        "        self.rays_o, self.rays_d = self.get_rays_multiple_images(self.H, self.W, self.intrinsics, self.c2w_mat)\n",
        "        if self.rgb is not None:\n",
        "            self.rgb = self.rgb.reshape(self.batch_size, -1, 3)\n",
        "\n",
        "        if 'src_rgbs' in data.keys():\n",
        "            self.src_rgbs = data['src_rgbs']\n",
        "        else:\n",
        "            self.src_rgbs = None\n",
        "        if 'src_masks' in data.keys():\n",
        "            self.src_masks = data['src_masks']\n",
        "        else:\n",
        "            self.src_masks = None\n",
        "        if 'src_intrinsics' in data.keys():\n",
        "            self.src_intrinsics = data['src_intrinsics']\n",
        "        else:\n",
        "            self.src_intrinsics = None\n",
        "        if 'src_c2w_mats' in data.keys():\n",
        "            self.src_c2w_mats = data['src_c2w_mats']\n",
        "        else:\n",
        "            self.src_c2w_mats = None\n",
        "        if 'tgt_bbox' in data.keys():\n",
        "            self.tgt_bbox = data['tgt_bbox']\n",
        "\n",
        "    def get_rays_multiple_images(self, H, W, intrinsics, c2w):\n",
        "        '''Generate rays for multiple images (batch size > 1).\n",
        "        Args:\n",
        "            H: image height\n",
        "            W: image width\n",
        "            intrinsics: 4 by 4 intrinsic matrix\n",
        "            c2w: 4 by 4 camera to world extrinsic matrix\n",
        "        Returns:\n",
        "            Tensors of ray origin and direction.\n",
        "        '''\n",
        "        u, v = np.meshgrid(np.arange(W)[::self.render_stride], np.arange(H)[::self.render_stride])\n",
        "        u = u.reshape(-1).astype(dtype=np.float32)  # + 0.5    # add half pixel\n",
        "        v = v.reshape(-1).astype(dtype=np.float32)  # + 0.5\n",
        "        pixels = np.stack((u, v, np.ones_like(u)), axis=0)  # (3, H*W)\n",
        "        pixels = torch.from_numpy(pixels)\n",
        "        batched_pixels = pixels.unsqueeze(0).repeat(self.batch_size, 1, 1)\n",
        "\n",
        "        rays_d = (c2w[:, :3, :3].bmm(torch.inverse(intrinsics[:, :3, :3])).bmm(batched_pixels)).transpose(1, 2) # B x HW x 3\n",
        "        rays_o = c2w[:, :3, 3].unsqueeze(1).repeat(1, rays_d.shape[1], 1)  # B x HW x 3\n",
        "\n",
        "        return rays_o, rays_d\n",
        "\n",
        "    def get_all(self):\n",
        "        ret = {'ray_o': self.rays_o.cuda(), # [b, h*w, 3]\n",
        "               'ray_d': self.rays_d.cuda(), # [b, h*w, 3]\n",
        "               'depth_range': self.depth_range.cuda(), # [b, 2]\n",
        "               'intrinsics': self.intrinsics.cuda(), # [b, 4, 4]\n",
        "               'c2w_mat': self.c2w_mat.cuda(), # [b, 4, 4]\n",
        "               'rgb': self.rgb.cuda() if self.rgb is not None else None, # [b, h*w, 3]\n",
        "               'src_rgbs': self.src_rgbs.cuda() if self.src_rgbs is not None else None, # [b, v, h, w, 3]\n",
        "               'src_intrinsics': self.src_intrinsics.cuda() if self.src_intrinsics is not None else None, # [b, v, 4, 4]\n",
        "               'src_c2w_mats': self.src_c2w_mats.cuda() if self.src_c2w_mats is not None else None, # [b, v, 4, 4]\n",
        "               'src_masks': self.src_masks.cuda() if self.src_masks is not None else None, # [b, v, h, w, 1]\n",
        "        }\n",
        "        return ret\n",
        "\n",
        "    def sample_random_pixel(self, N_rand, sample_mode, batch_idx, center_ratio=0.8):\n",
        "        if sample_mode == 'center':\n",
        "            border_H = int(self.H * (1 - center_ratio) / 2.)\n",
        "            border_W = int(self.W * (1 - center_ratio) / 2.)\n",
        "\n",
        "            # pixel coordinates\n",
        "            u, v = np.meshgrid(np.arange(border_H, self.H - border_H),\n",
        "                               np.arange(border_W, self.W - border_W))\n",
        "            u = u.reshape(-1)\n",
        "            v = v.reshape(-1)\n",
        "\n",
        "            select_inds = rng.choice(u.shape[0], size=(N_rand,), replace=False)\n",
        "            select_inds = v[select_inds] + self.W * u[select_inds]\n",
        "\n",
        "        elif sample_mode == 'uniform' or (sample_mode == 'bbox' and self.cur_step > self.bbox_steps):\n",
        "            # Random from one image\n",
        "            select_inds = rng.choice(self.H*self.W, size=(N_rand,), replace=False)\n",
        "\n",
        "        elif sample_mode == 'bbox':\n",
        "            u, v = bbox_sample(self.tgt_bbox[batch_idx], N_rand)\n",
        "            select_inds = v + self.W * u\n",
        "        elif sample_mode == 'bbox_sample_full':\n",
        "            u, v = bbox_sample_full(self.tgt_bbox[batch_idx], N_rand, h=self.H, w=self.W, prob=0.8)\n",
        "            select_inds = v + self.W * u\n",
        "        else:\n",
        "            raise Exception(\"unknown sample mode!\")\n",
        "\n",
        "        return select_inds\n",
        "\n",
        "    def random_sample(self, N_rand, sample_mode, center_ratio=0.8):\n",
        "        '''Generate a bundle of randomly sampled rays.\n",
        "        Args:\n",
        "            N_rand: number of rays to be casted\n",
        "        Returns:\n",
        "            A dictionary of ray information.\n",
        "        '''\n",
        "\n",
        "        select_inds = []\n",
        "        for x in range(self.batch_size):\n",
        "            select_inds.append(\n",
        "                self.sample_random_pixel(N_rand, sample_mode, x, center_ratio)\n",
        "            )\n",
        "        select_inds = np.stack(select_inds, 0)\n",
        "\n",
        "        rays_o = [self.rays_o[i, select_inds[i]] for i in range(self.batch_size)]\n",
        "        rays_d = [self.rays_d[i, select_inds[i]] for i in range(self.batch_size)]\n",
        "        rays_o = torch.stack(rays_o, 0)\n",
        "        rays_d = torch.stack(rays_d, 0)\n",
        "\n",
        "        if self.rgb is not None:\n",
        "            rgb = [self.rgb[i, select_inds[i]] for i in range(self.batch_size)]\n",
        "            rgb = torch.stack(rgb, 0)\n",
        "        else:\n",
        "            rgb = None\n",
        "\n",
        "        ret = {'ray_o': rays_o.cuda(),\n",
        "               'ray_d': rays_d.cuda(),\n",
        "               'intrinsics': self.intrinsics.cuda(),\n",
        "               'c2w_mat': self.c2w_mat.cuda(),\n",
        "               'depth_range': self.depth_range.cuda(),\n",
        "               'rgb': rgb.cuda() if rgb is not None else None,\n",
        "               'src_rgbs': self.src_rgbs.cuda() if self.src_rgbs is not None else None,\n",
        "               'src_intrinsics': self.src_intrinsics.cuda() if self.src_intrinsics is not None else None,\n",
        "               'src_c2w_mats': self.src_c2w_mats.cuda() if self.src_c2w_mats is not None else None,\n",
        "               'selected_inds': select_inds,\n",
        "               'src_masks': self.src_masks.cuda() if self.src_masks is not None else None,\n",
        "        }\n",
        "        return ret"
      ],
      "metadata": {
        "id": "P0RHYQdV2ATy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Utils.py"
      ],
      "metadata": {
        "id": "oTQ3FDy627O0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "import torch\n",
        "import numpy as np\n",
        "from matplotlib.backends.backend_agg import FigureCanvasAgg\n",
        "from matplotlib.figure import Figure\n",
        "import matplotlib as mpl\n",
        "from matplotlib import cm\n",
        "import cv2\n",
        "import os\n",
        "from datetime import datetime\n",
        "import shutil\n",
        "\n",
        "HUGE_NUMBER = 1e10\n",
        "TINY_NUMBER = 1e-6      # float32 only has 7 decimal digits precision\n",
        "\n",
        "img_HWC2CHW = lambda x: x.permute(2, 0, 1)\n",
        "gray2rgb = lambda x: x.unsqueeze(2).repeat(1, 1, 3)\n",
        "\n",
        "\n",
        "to8b = lambda x: (255 * np.clip(x, 0, 1)).astype(np.uint8)\n",
        "mse2psnr = lambda x: -10. * np.log(x+TINY_NUMBER) / np.log(10.)\n",
        "\n",
        "\n",
        "def get_single(data_dict):\n",
        "    new_dict = {}\n",
        "    for key in data_dict:\n",
        "        if key == 'img_hw':\n",
        "            new_dict[key] = [data_dict[key][0][:1], data_dict[key][1][:1]]\n",
        "        else:\n",
        "            datum = data_dict[key][0]\n",
        "            if isinstance(datum, torch.Tensor):\n",
        "                new_dict[key] = datum.unsqueeze(0)\n",
        "            else:\n",
        "                new_dict[key] = [datum]\n",
        "    return new_dict\n",
        "\n",
        "def get_views(data_dict, src_indices, tgt_indices):\n",
        "    \"\"\"Acquire certain source/target views from a given sample\n",
        "\n",
        "    Args:\n",
        "        data_dict: sample from eval dataset\n",
        "        src_indices: source view indices [#views]\n",
        "        tgt_indices: target view indices [#views]\n",
        "\n",
        "    Returns:\n",
        "        An array of data_dict's\n",
        "    \"\"\"\n",
        "    samples = []\n",
        "\n",
        "    for i in range(len(src_indices)):\n",
        "        sample = {\n",
        "            'rgb_path': data_dict['rgb_path'],\n",
        "            'img_id': data_dict['img_id'],\n",
        "            'img_hw': data_dict['img_hw'],\n",
        "            'depth_range': data_dict['depth_range'],\n",
        "            'tgt_bbox': data_dict['bbox'][:, tgt_indices[i]],\n",
        "            'tgt_mask': data_dict['masks'][:, tgt_indices[i]],\n",
        "            'tgt_rgb': data_dict['rgbs'][:, tgt_indices[i]],\n",
        "            'tgt_c2w_mat': data_dict['c2w_mats'][:, tgt_indices[i]],\n",
        "            'tgt_intrinsic': data_dict['intrinsics'][:, tgt_indices[i]],\n",
        "            'src_masks': data_dict['masks'][:, src_indices[i]][None, :], # HACK assuming one view\n",
        "            'src_rgbs': data_dict['rgbs'][:, src_indices[i]][None, :],\n",
        "            'src_c2w_mats': data_dict['c2w_mats'][:, src_indices[i]][None, :],\n",
        "            'src_intrinsics': data_dict['intrinsics'][:, src_indices[i]][None, :],\n",
        "        }\n",
        "\n",
        "        samples.append(sample)\n",
        "\n",
        "    return samples\n",
        "\n",
        "def get_views_single(data_dict, src_indices, tgt_indices):\n",
        "    \"\"\"Acquire certain source/target views from a given sample\n",
        "\n",
        "    Args:\n",
        "        data_dict: sample from eval dataset\n",
        "        src_indices: source view indices [#views]\n",
        "        tgt_indices: target view indices [#views]\n",
        "\n",
        "    Returns:\n",
        "        An array of data_dict's\n",
        "    \"\"\"\n",
        "    samples = []\n",
        "\n",
        "    for i in range(len(src_indices)):\n",
        "        sample = {\n",
        "            'rgb_path': data_dict['rgb_path'],\n",
        "            'img_id': data_dict['img_id'],\n",
        "            'img_hw': data_dict['img_hw'],\n",
        "            'depth_range': data_dict['depth_range'],\n",
        "            'tgt_bbox': data_dict['bbox'][tgt_indices[i]],\n",
        "            'tgt_mask': data_dict['masks'][tgt_indices[i]],\n",
        "            'tgt_rgb': data_dict['rgbs'][tgt_indices[i]],\n",
        "            'tgt_c2w_mat': data_dict['c2w_mats'][tgt_indices[i]],\n",
        "            'tgt_intrinsic': data_dict['intrinsics'][tgt_indices[i]],\n",
        "            'src_masks': data_dict['masks'][src_indices[i]][None, :], # HACK assuming one view\n",
        "            'src_rgbs': data_dict['rgbs'][src_indices[i]][None, :],\n",
        "            'src_c2w_mats': data_dict['c2w_mats'][src_indices[i]][None, :],\n",
        "            'src_intrinsics': data_dict['intrinsics'][src_indices[i]][None, :],\n",
        "        }\n",
        "\n",
        "        samples.append(sample)\n",
        "\n",
        "    return samples\n",
        "\n",
        "def save_current_code(outdir):\n",
        "    now = datetime.now()  # current date and time\n",
        "    date_time = now.strftime(\"%m_%d-%H:%M:%S\")\n",
        "    src_dir = '.'\n",
        "    dst_dir = os.path.join(outdir, 'code_{}'.format(date_time))\n",
        "    shutil.copytree(src_dir, dst_dir,\n",
        "                    ignore=shutil.ignore_patterns('data*', 'pretrained*', 'logs*', 'out*', '*.png', '*.mp4',\n",
        "                                                  '*__pycache__*', '*.git*', '*.idea*', '*.zip', '*.jpg'))\n",
        "\n",
        "# Get git commit hash\n",
        "def get_git_revision_hash():\n",
        "    return subprocess.check_output(['git', 'rev-parse', 'HEAD'])\n",
        "\n",
        "def img2mse(x, y, mask=None):\n",
        "    '''\n",
        "    :param x: img 1, [(...), 3]\n",
        "    :param y: img 2, [(...), 3]\n",
        "    :param mask: optional, [(...)]\n",
        "    :return: mse score\n",
        "    '''\n",
        "    if mask is None:\n",
        "        return torch.mean((x - y) * (x - y))\n",
        "    else:\n",
        "        return torch.sum((x - y) * (x - y) * mask.unsqueeze(-1)) / (torch.sum(mask) * x.shape[-1] + TINY_NUMBER)\n",
        "\n",
        "\n",
        "def img2psnr(x, y, mask=None):\n",
        "    return mse2psnr(img2mse(x, y, mask).item())\n",
        "\n",
        "\n",
        "def cycle(iterable):\n",
        "    while True:\n",
        "        for x in iterable:\n",
        "            yield x\n",
        "\n",
        "\n",
        "def get_vertical_colorbar(h, vmin, vmax, cmap_name='jet', label=None, cbar_precision=2):\n",
        "    '''\n",
        "    :param w: pixels\n",
        "    :param h: pixels\n",
        "    :param vmin: min value\n",
        "    :param vmax: max value\n",
        "    :param cmap_name:\n",
        "    :param label\n",
        "    :return:\n",
        "    '''\n",
        "    fig = Figure(figsize=(2, 8), dpi=100)\n",
        "    fig.subplots_adjust(right=1.5)\n",
        "    canvas = FigureCanvasAgg(fig)\n",
        "\n",
        "    # Do some plotting.\n",
        "    ax = fig.add_subplot(111)\n",
        "    cmap = cm.get_cmap(cmap_name)\n",
        "    norm = mpl.colors.Normalize(vmin=vmin, vmax=vmax)\n",
        "\n",
        "    tick_cnt = 6\n",
        "    tick_loc = np.linspace(vmin, vmax, tick_cnt)\n",
        "    cb1 = mpl.colorbar.ColorbarBase(ax, cmap=cmap,\n",
        "                                    norm=norm,\n",
        "                                    ticks=tick_loc,\n",
        "                                    orientation='vertical')\n",
        "\n",
        "    tick_label = [str(np.round(x, cbar_precision)) for x in tick_loc]\n",
        "    if cbar_precision == 0:\n",
        "        tick_label = [x[:-2] for x in tick_label]\n",
        "\n",
        "    cb1.set_ticklabels(tick_label)\n",
        "\n",
        "    cb1.ax.tick_params(labelsize=18, rotation=0)\n",
        "\n",
        "    if label is not None:\n",
        "        cb1.set_label(label)\n",
        "\n",
        "    fig.tight_layout()\n",
        "\n",
        "    canvas.draw()\n",
        "    s, (width, height) = canvas.print_to_buffer()\n",
        "\n",
        "    im = np.frombuffer(s, np.uint8).reshape((height, width, 4))\n",
        "\n",
        "    im = im[:, :, :3].astype(np.float32) / 255.\n",
        "    if h != im.shape[0]:\n",
        "        w = int(im.shape[1] / im.shape[0] * h)\n",
        "        im = cv2.resize(im, (w, h), interpolation=cv2.INTER_AREA)\n",
        "\n",
        "    return im\n",
        "\n",
        "\n",
        "def colorize_np(x, cmap_name='jet', mask=None, range=None, append_cbar=False, cbar_in_image=False, cbar_precision=2):\n",
        "    '''\n",
        "    turn a grayscale image into a color image\n",
        "    :param x: input grayscale, [H, W]\n",
        "    :param cmap_name: the colorization method\n",
        "    :param mask: the mask image, [H, W]\n",
        "    :param range: the range for scaling, automatic if None, [min, max]\n",
        "    :param append_cbar: if append the color bar\n",
        "    :param cbar_in_image: put the color bar inside the image to keep the output image the same size as the input image\n",
        "    :return: colorized image, [H, W]\n",
        "    '''\n",
        "    if range is not None:\n",
        "        vmin, vmax = range\n",
        "    elif mask is not None:\n",
        "        # vmin, vmax = np.percentile(x[mask], (2, 100))\n",
        "        vmin = np.min(x[mask][np.nonzero(x[mask])])\n",
        "        vmax = np.max(x[mask])\n",
        "        # vmin = vmin - np.abs(vmin) * 0.01\n",
        "        x[np.logical_not(mask)] = vmin\n",
        "        # print(vmin, vmax)\n",
        "    else:\n",
        "        vmin, vmax = np.percentile(x, (1, 100))\n",
        "        vmax += TINY_NUMBER\n",
        "\n",
        "    x = np.clip(x, vmin, vmax)\n",
        "    x = (x - vmin) / (vmax - vmin)\n",
        "    # x = np.clip(x, 0., 1.)\n",
        "\n",
        "    cmap = cm.get_cmap(cmap_name)\n",
        "    x_new = cmap(x)[:, :, :3]\n",
        "\n",
        "    if mask is not None:\n",
        "        mask = np.float32(mask[:, :, np.newaxis])\n",
        "        x_new = x_new * mask + np.ones_like(x_new) * (1. - mask)\n",
        "\n",
        "    cbar = get_vertical_colorbar(h=x.shape[0], vmin=vmin, vmax=vmax, cmap_name=cmap_name, cbar_precision=cbar_precision)\n",
        "\n",
        "    if append_cbar:\n",
        "        if cbar_in_image:\n",
        "            x_new[:, -cbar.shape[1]:, :] = cbar\n",
        "        else:\n",
        "            x_new = np.concatenate((x_new, np.zeros_like(x_new[:, :5, :]), cbar), axis=1)\n",
        "        return x_new\n",
        "    else:\n",
        "        return x_new\n",
        "\n",
        "\n",
        "# tensor\n",
        "def colorize(x, cmap_name='jet', mask=None, range=None, append_cbar=False, cbar_in_image=False):\n",
        "    device = x.device\n",
        "    x = x.cpu().numpy()\n",
        "    if mask is not None:\n",
        "        mask = mask.cpu().numpy() > 0.99\n",
        "        kernel = np.ones((3, 3), np.uint8)\n",
        "        mask = cv2.erode(mask.astype(np.uint8), kernel, iterations=1).astype(bool)\n",
        "\n",
        "    x = colorize_np(x, cmap_name, mask, range, append_cbar, cbar_in_image)\n",
        "    x = torch.from_numpy(x).to(device)\n",
        "    return x"
      ],
      "metadata": {
        "id": "4618GeCp2WKW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "data/data_utils.py"
      ],
      "metadata": {
        "id": "THnA6YI24TnL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import math\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "import torch\n",
        "from scipy.spatial.transform import Rotation as R\n",
        "\n",
        "rng = np.random.RandomState(234)\n",
        "_EPS = np.finfo(float).eps * 4.0\n",
        "TINY_NUMBER = 1e-6      # float32 only has 7 decimal digits precision\n",
        "\n",
        "\n",
        "def vector_norm(data, axis=None, out=None):\n",
        "    \"\"\"Return length, i.e. eucledian norm, of ndarray along axis.\n",
        "    \"\"\"\n",
        "    data = np.array(data, dtype=np.float64, copy=True)\n",
        "    if out is None:\n",
        "        if data.ndim == 1:\n",
        "            return math.sqrt(np.dot(data, data))\n",
        "        data *= data\n",
        "        out = np.atleast_1d(np.sum(data, axis=axis))\n",
        "        np.sqrt(out, out)\n",
        "        return out\n",
        "    else:\n",
        "        data *= data\n",
        "        np.sum(data, axis=axis, out=out)\n",
        "        np.sqrt(out, out)\n",
        "\n",
        "\n",
        "def quaternion_about_axis(angle, axis):\n",
        "    \"\"\"Return quaternion for rotation about axis.\n",
        "    \"\"\"\n",
        "    quaternion = np.zeros((4, ), dtype=np.float64)\n",
        "    quaternion[:3] = axis[:3]\n",
        "    qlen = vector_norm(quaternion)\n",
        "    if qlen > _EPS:\n",
        "        quaternion *= math.sin(angle/2.0) / qlen\n",
        "    quaternion[3] = math.cos(angle/2.0)\n",
        "    return quaternion\n",
        "\n",
        "\n",
        "def quaternion_matrix(quaternion):\n",
        "    \"\"\"Return homogeneous rotation matrix from quaternion.\n",
        "    \"\"\"\n",
        "    q = np.array(quaternion[:4], dtype=np.float64, copy=True)\n",
        "    nq = np.dot(q, q)\n",
        "    if nq < _EPS:\n",
        "        return np.identity(4)\n",
        "    q *= math.sqrt(2.0 / nq)\n",
        "    q = np.outer(q, q)\n",
        "    return np.array((\n",
        "        (1.0-q[1, 1]-q[2, 2],     q[0, 1]-q[2, 3],     q[0, 2]+q[1, 3], 0.0),\n",
        "        (    q[0, 1]+q[2, 3], 1.0-q[0, 0]-q[2, 2],     q[1, 2]-q[0, 3], 0.0),\n",
        "        (    q[0, 2]-q[1, 3],     q[1, 2]+q[0, 3], 1.0-q[0, 0]-q[1, 1], 0.0),\n",
        "        (                0.0,                 0.0,                 0.0, 1.0)\n",
        "        ), dtype=np.float64)\n",
        "\n",
        "\n",
        "def rectify_inplane_rotation(src_pose, tar_pose, src_img, th=40):\n",
        "    relative = np.linalg.inv(tar_pose).dot(src_pose)\n",
        "    relative_rot = relative[:3, :3]\n",
        "    r = R.from_matrix(relative_rot)\n",
        "    euler = r.as_euler('zxy', degrees=True)\n",
        "    euler_z = euler[0]\n",
        "    if np.abs(euler_z) < th:\n",
        "        return src_pose, src_img\n",
        "\n",
        "    R_rectify = R.from_euler('z', -euler_z, degrees=True).as_matrix()\n",
        "    src_R_rectified = src_pose[:3, :3].dot(R_rectify)\n",
        "    out_pose = np.eye(4)\n",
        "    out_pose[:3, :3] = src_R_rectified\n",
        "    out_pose[:3, 3:4] = src_pose[:3, 3:4]\n",
        "    h, w = src_img.shape[:2]\n",
        "    center = ((w - 1.) / 2., (h - 1.) / 2.)\n",
        "    M = cv2.getRotationMatrix2D(center, -euler_z, 1)\n",
        "    src_img = np.clip((255*src_img).astype(np.uint8), a_max=255, a_min=0)\n",
        "    rotated = cv2.warpAffine(src_img, M, (w, h), borderValue=(255, 255, 255), flags=cv2.INTER_LANCZOS4)\n",
        "    rotated = rotated.astype(np.float32) / 255.\n",
        "    return out_pose, rotated\n",
        "\n",
        "\n",
        "def random_crop(rgb, camera, src_rgbs, src_cameras, size=(400, 600), center=None):\n",
        "    h, w = rgb.shape[:2]\n",
        "    out_h, out_w = size[0], size[1]\n",
        "    if out_w >= w or out_h >= h:\n",
        "        return rgb, camera, src_rgbs, src_cameras\n",
        "\n",
        "    if center is not None:\n",
        "        center_h, center_w = center\n",
        "    else:\n",
        "        center_h = np.random.randint(low=out_h // 2 + 1, high=h - out_h // 2 - 1)\n",
        "        center_w = np.random.randint(low=out_w // 2 + 1, high=w - out_w // 2 - 1)\n",
        "\n",
        "    rgb_out = rgb[center_h - out_h // 2:center_h + out_h // 2, center_w - out_w // 2:center_w + out_w // 2, :]\n",
        "    src_rgbs = np.array(src_rgbs)\n",
        "    src_rgbs = src_rgbs[:, center_h - out_h // 2:center_h + out_h // 2,\n",
        "               center_w - out_w // 2:center_w + out_w // 2, :]\n",
        "    camera[0] = out_h\n",
        "    camera[1] = out_w\n",
        "    camera[4] -= center_w - out_w // 2\n",
        "    camera[8] -= center_h - out_h // 2\n",
        "    src_cameras[:, 4] -= center_w - out_w // 2\n",
        "    src_cameras[:, 8] -= center_h - out_h // 2\n",
        "    src_cameras[:, 0] = out_h\n",
        "    src_cameras[:, 1] = out_w\n",
        "    return rgb_out, camera, src_rgbs, src_cameras\n",
        "\n",
        "\n",
        "def random_flip(rgb, camera, src_rgbs, src_cameras):\n",
        "    h, w = rgb.shape[:2]\n",
        "    h_r, w_r = src_rgbs.shape[1:3]\n",
        "    rgb_out = np.flip(rgb, axis=1).copy()\n",
        "    src_rgbs = np.flip(src_rgbs, axis=-2).copy()\n",
        "    camera[2] *= -1\n",
        "    camera[4] = w - 1. - camera[4]\n",
        "    src_cameras[:, 2] *= -1\n",
        "    src_cameras[:, 4] = w_r - 1. - src_cameras[:, 4]\n",
        "    return rgb_out, camera, src_rgbs, src_cameras\n",
        "\n",
        "\n",
        "def get_color_jitter_params(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2):\n",
        "    color_jitter = transforms.ColorJitter(brightness=brightness, contrast=contrast, saturation=saturation, hue=hue)\n",
        "    transform = transforms.ColorJitter.get_params(color_jitter.brightness,\n",
        "                                                  color_jitter.contrast,\n",
        "                                                  color_jitter.saturation,\n",
        "                                                  color_jitter.hue)\n",
        "    return transform\n",
        "\n",
        "\n",
        "def color_jitter(img, transform):\n",
        "    '''\n",
        "    Args:\n",
        "        img: np.float32 [h, w, 3]\n",
        "        transform:\n",
        "    Returns: transformed np.float32\n",
        "    '''\n",
        "    img = Image.fromarray((255.*img).astype(np.uint8))\n",
        "    img_trans = transform(img)\n",
        "    img_trans = np.array(img_trans).astype(np.float32) / 255.\n",
        "    return img_trans\n",
        "\n",
        "\n",
        "def color_jitter_all_rgbs(rgb, ref_rgbs, brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2):\n",
        "    transform = get_color_jitter_params(brightness, contrast, saturation, hue)\n",
        "    rgb_trans = color_jitter(rgb, transform)\n",
        "    ref_rgbs_trans = []\n",
        "    for ref_rgb in ref_rgbs:\n",
        "        ref_rgbs_trans.append(color_jitter(ref_rgb, transform))\n",
        "\n",
        "    ref_rgbs_trans = np.array(ref_rgbs_trans)\n",
        "    return rgb_trans, ref_rgbs_trans\n",
        "\n",
        "\n",
        "def deepvoxels_parse_intrinsics(filepath, trgt_sidelength, invert_y=False):\n",
        "    # Get camera intrinsics\n",
        "    with open(filepath, 'r') as file:\n",
        "        f, cx, cy = list(map(float, file.readline().split()))[:3]\n",
        "        grid_barycenter = torch.Tensor(list(map(float, file.readline().split())))\n",
        "        near_plane = float(file.readline())\n",
        "        scale = float(file.readline())\n",
        "        height, width = map(float, file.readline().split())\n",
        "\n",
        "        try:\n",
        "            world2cam_poses = int(file.readline())\n",
        "        except ValueError:\n",
        "            world2cam_poses = None\n",
        "\n",
        "    if world2cam_poses is None:\n",
        "        world2cam_poses = False\n",
        "\n",
        "    world2cam_poses = bool(world2cam_poses)\n",
        "\n",
        "    cx = cx / width * trgt_sidelength\n",
        "    cy = cy / height * trgt_sidelength\n",
        "    f = trgt_sidelength / height * f\n",
        "\n",
        "    fx = f\n",
        "    if invert_y:\n",
        "        fy = -f\n",
        "    else:\n",
        "        fy = f\n",
        "\n",
        "    # Build the intrinsic matrices\n",
        "    full_intrinsic = np.array([[fx, 0., cx, 0.],\n",
        "                               [0., fy, cy, 0],\n",
        "                               [0., 0, 1, 0],\n",
        "                               [0, 0, 0, 1]])\n",
        "\n",
        "    return full_intrinsic, grid_barycenter, scale, near_plane, world2cam_poses\n",
        "\n",
        "\n",
        "def angular_dist_between_2_vectors(vec1, vec2):\n",
        "    vec1_unit = vec1 / (np.linalg.norm(vec1, axis=1, keepdims=True) + TINY_NUMBER)\n",
        "    vec2_unit = vec2 / (np.linalg.norm(vec2, axis=1, keepdims=True) + TINY_NUMBER)\n",
        "    angular_dists = np.arccos(np.clip(np.sum(vec1_unit*vec2_unit, axis=-1), -1.0, 1.0))\n",
        "    return angular_dists\n",
        "\n",
        "\n",
        "def batched_angular_dist_rot_matrix(R1, R2):\n",
        "    '''\n",
        "    calculate the angular distance between two rotation matrices (batched)\n",
        "    :param R1: the first rotation matrix [N, 3, 3]\n",
        "    :param R2: the second rotation matrix [N, 3, 3]\n",
        "    :return: angular distance in radiance [N, ]\n",
        "    '''\n",
        "    assert R1.shape[-1] == 3 and R2.shape[-1] == 3 and R1.shape[-2] == 3 and R2.shape[-2] == 3\n",
        "    return np.arccos(np.clip((np.trace(np.matmul(R2.transpose(0, 2, 1), R1), axis1=1, axis2=2) - 1) / 2.,\n",
        "                             a_min=-1 + TINY_NUMBER, a_max=1 - TINY_NUMBER))\n",
        "\n",
        "\n",
        "def get_nearest_pose_ids(tar_pose, ref_poses, num_select, tar_id=-1, angular_dist_method='vector',\n",
        "                         scene_center=(0, 0, 0)):\n",
        "    '''\n",
        "    Args:\n",
        "        tar_pose: target pose [4, 4]\n",
        "        ref_poses: reference poses [N, 4, 4]\n",
        "        num_select: the number of nearest views to select\n",
        "    Returns: the selected indices\n",
        "    '''\n",
        "    num_cams = len(ref_poses)\n",
        "    num_select = min(num_select, num_cams-1)\n",
        "    batched_tar_pose = tar_pose[None, ...].repeat(num_cams, 0)\n",
        "\n",
        "    if angular_dist_method == 'matrix':\n",
        "        dists = batched_angular_dist_rot_matrix(batched_tar_pose[:, :3, :3], ref_poses[:, :3, :3])\n",
        "    elif angular_dist_method == 'vector':\n",
        "        tar_cam_locs = batched_tar_pose[:, :3, 3]\n",
        "        ref_cam_locs = ref_poses[:, :3, 3]\n",
        "        scene_center = np.array(scene_center)[None, ...]\n",
        "        tar_vectors = tar_cam_locs - scene_center\n",
        "        ref_vectors = ref_cam_locs - scene_center\n",
        "        dists = angular_dist_between_2_vectors(tar_vectors, ref_vectors)\n",
        "    elif angular_dist_method == 'dist':\n",
        "        tar_cam_locs = batched_tar_pose[:, :3, 3]\n",
        "        ref_cam_locs = ref_poses[:, :3, 3]\n",
        "        dists = np.linalg.norm(tar_cam_locs - ref_cam_locs, axis=1)\n",
        "    else:\n",
        "        raise Exception('unknown angular distance calculation method!')\n",
        "\n",
        "    if tar_id >= 0:\n",
        "        assert tar_id < num_cams\n",
        "        dists[tar_id] = 1e3  # make sure not to select the target id itself\n",
        "\n",
        "    sorted_ids = np.argsort(dists)\n",
        "    selected_ids = sorted_ids[:num_select]\n",
        "    # print(angular_dists[selected_ids] * 180 / np.pi)\n",
        "    return selected_ids"
      ],
      "metadata": {
        "id": "ytJew5Tt4Xwd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "data/DVR.py"
      ],
      "metadata": {
        "id": "nIV4jS_d4c8-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "\n",
        "import imageio\n",
        "import numpy as np\n",
        "import tqdm\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import  torchvision.transforms as T\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "def parse_pose(path, num_views):\n",
        "    cameras = np.load(path)\n",
        "\n",
        "    intrinsics = []\n",
        "    c2w_mats = []\n",
        "\n",
        "    for i in range(num_views):\n",
        "        # ShapeNet\n",
        "        wmat_inv_key = \"world_mat_inv_\" + str(i)\n",
        "        wmat_key = \"world_mat_\" + str(i)\n",
        "        kmat_key = \"camera_mat_\" + str(i)\n",
        "        if wmat_inv_key in cameras:\n",
        "            c2w_mat = cameras[wmat_inv_key]\n",
        "        else:\n",
        "            w2c_mat = cameras[wmat_key]\n",
        "            if w2c_mat.shape[0] == 3:\n",
        "                w2c_mat = np.vstack((w2c_mat, np.array([0, 0, 0, 1])))\n",
        "            c2w_mat = np.linalg.inv(w2c_mat)\n",
        "\n",
        "        intrinsics.append(cameras[kmat_key])\n",
        "        c2w_mats.append(c2w_mat)\n",
        "\n",
        "    intrinsics = np.stack(intrinsics, 0)\n",
        "    c2w_mats = np.stack(c2w_mats, 0)\n",
        "\n",
        "    return intrinsics, c2w_mats\n",
        "\n",
        "class DVRDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset from DVR (Niemeyer et al. 2020)\n",
        "    Provides 3D-R2N2 and NMR renderings\n",
        "    \"\"\"\n",
        "    def __init__(self, args, mode,\n",
        "                **kwargs):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            args.data_path: path to data directory\n",
        "            args.img_hw: image size (resize if needed)\n",
        "            mode: train | test | val mode\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.base_path = args.data_path\n",
        "        self.dataset_name = os.path.basename(args.data_path)\n",
        "        assert os.path.exists(self.base_path)\n",
        "\n",
        "        cats = [x for x in glob.glob(os.path.join(args.data_path, \"*\")) if os.path.isdir(x)]\n",
        "\n",
        "        list_prefix = \"softras_\" # Train on all categories and eval on them\n",
        "\n",
        "        if mode == \"train\":\n",
        "            file_lists = [os.path.join(x, list_prefix + \"train.lst\") for x in cats]\n",
        "        elif mode == \"val\":\n",
        "            file_lists = [os.path.join(x, list_prefix + \"val.lst\") for x in cats]\n",
        "        elif mode == \"test\":\n",
        "            file_lists = [os.path.join(x, list_prefix + \"test.lst\") for x in cats]\n",
        "\n",
        "        print(\"Loading NMR dataset\", self.base_path, \"name:\", self.dataset_name, \"mode:\", mode)\n",
        "\n",
        "        self.mode = mode\n",
        "\n",
        "        all_objs = []\n",
        "        for file_list in file_lists:\n",
        "            if not os.path.exists(file_list):\n",
        "                continue\n",
        "            base_dir = os.path.dirname(file_list)\n",
        "            cat = os.path.basename(base_dir)\n",
        "            with open(file_list, \"r\") as f:\n",
        "                objs = [(cat, os.path.join(base_dir, x.strip())) for x in f.readlines()]\n",
        "            all_objs.extend(objs)\n",
        "\n",
        "        self.all_objs = all_objs\n",
        "\n",
        "        if args.debug:\n",
        "            self.all_objs = self.all_objs[:1]\n",
        "\n",
        "\n",
        "        self.intrinsics = []\n",
        "        self.poses = []\n",
        "        self.rgb_paths = []\n",
        "        for _, path in tqdm.tqdm(self.all_objs):\n",
        "            curr_paths = sorted(glob.glob(os.path.join(path, \"image\", \"*\")))\n",
        "            self.rgb_paths.append(curr_paths)\n",
        "\n",
        "            pose_path = os.path.join(path, 'cameras.npz')\n",
        "            intrinsics, c2w_mats = parse_pose(pose_path, len(curr_paths))\n",
        "\n",
        "            self.poses.append(c2w_mats)\n",
        "            self.intrinsics.append(intrinsics)\n",
        "\n",
        "        self.rgb_paths = np.array(self.rgb_paths)\n",
        "        self.poses = np.stack(self.poses, 0)\n",
        "        self.intrinsics = np.array(self.intrinsics)\n",
        "\n",
        "        assert(len(self.rgb_paths) == len(self.poses))\n",
        "\n",
        "        self.define_transforms()\n",
        "        self.img_hw = args.img_hw\n",
        "\n",
        "        self.num_views = args.num_source_views\n",
        "        self.closest_n_views = args.closest_n_views\n",
        "\n",
        "        # Default near/far plane depth\n",
        "        self.z_near = 1.2\n",
        "        self.z_far = 4.0\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.intrinsics)\n",
        "\n",
        "    def define_transforms(self):\n",
        "        self.img_transforms = T.Compose(\n",
        "            [T.ToTensor(), T.Normalize((0.0, 0.0, 0.0), (1.0, 1.0, 1.0))]\n",
        "        )\n",
        "        self.mask_transforms = T.Compose(\n",
        "            [T.ToTensor(), T.Normalize((0.0,), (1.0,))]\n",
        "        )\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        train_poses = self.poses[index]\n",
        "\n",
        "        render_idx = np.random.choice(len(train_poses), 1, replace=False)[0]\n",
        "\n",
        "        intrinsic = self.intrinsics[index][render_idx].copy()\n",
        "\n",
        "        rgb_path = self.rgb_paths[index, render_idx]\n",
        "        render_pose = train_poses[render_idx]\n",
        "        if self.closest_n_views > 0:\n",
        "            nearest_pose_ids = get_nearest_pose_ids(render_pose,\n",
        "                                                    train_poses,\n",
        "                                                    self.closest_n_views,\n",
        "                                                    tar_id=render_idx,\n",
        "                                                    angular_dist_method='vector')\n",
        "        else:\n",
        "            nearest_pose_ids = np.arange(len(train_poses))\n",
        "            nearest_pose_ids = np.delete(nearest_pose_ids, render_idx)\n",
        "        nearest_pose_ids = np.random.choice(nearest_pose_ids, self.num_views, replace=False)\n",
        "\n",
        "        # Read target RGB\n",
        "        img = imageio.imread(rgb_path)[..., :3]\n",
        "        mask = (img.sum(axis=-1) != 255*3)[..., None].astype(np.uint8) * 255\n",
        "        tgt_rgb = self.img_transforms(img)\n",
        "        tgt_mask = self.mask_transforms(mask)\n",
        "\n",
        "        intrinsic[0, 0] *= img.shape[1] / 2.0\n",
        "        intrinsic[1, 1] *= img.shape[0] / 2.0\n",
        "        intrinsic[0, 2] = img.shape[1] / 2.0\n",
        "        intrinsic[1, 2] = img.shape[0] / 2.0\n",
        "\n",
        "        h, w = tgt_rgb.shape[-2:]\n",
        "        if (h != self.img_hw[0]) or (w != self.img_hw[1]):\n",
        "            scale = self.img_hw[-1] / img.shape[1]\n",
        "            intrinsic[:2] *= scale\n",
        "\n",
        "            tgt_rgb = F.interpolate(tgt_rgb[None, :], size=self.img_hw, mode=\"area\")[0]\n",
        "            tgt_mask = F.interpolate(tgt_mask[None, :], size=self.img_hw, mode=\"area\")[0]\n",
        "\n",
        "        yy = torch.any(tgt_mask, axis=2)\n",
        "        xx = torch.any(tgt_mask, axis=1)\n",
        "        ynz = torch.nonzero(yy)[:, 1]\n",
        "        xnz = torch.nonzero(xx)[:, 1]\n",
        "        ymin, ymax = ynz[[0, -1]]\n",
        "        xmin, xmax = xnz[[0, -1]]\n",
        "        tgt_bbox = torch.FloatTensor([xmin, ymin, xmax, ymax])\n",
        "\n",
        "        # Read source RGB\n",
        "        src_rgb_paths = [self.rgb_paths[index][x] for x in nearest_pose_ids]\n",
        "        src_c2w_mats = np.array([train_poses[x] for x in nearest_pose_ids])\n",
        "        src_intrinsics = np.array(self.intrinsics[index][nearest_pose_ids])\n",
        "\n",
        "        src_intrinsics[..., 0, 0] *= img.shape[1] / 2.0\n",
        "        src_intrinsics[..., 1, 1] *= img.shape[0] / 2.0\n",
        "        src_intrinsics[..., 0, 2] = img.shape[1] / 2.0\n",
        "        src_intrinsics[..., 1, 2] = img.shape[0] / 2.0\n",
        "\n",
        "        src_rgbs = []\n",
        "        src_masks = []\n",
        "        for i, rgb_path in enumerate(src_rgb_paths):\n",
        "            img = imageio.imread(rgb_path)[..., :3]\n",
        "            mask = (img.sum(axis=-1) != 255*3)[..., None].astype(np.uint8) * 255\n",
        "            rgb = self.img_transforms(img)\n",
        "            mask = self.mask_transforms(mask)\n",
        "\n",
        "            h, w = rgb.shape[-2:]\n",
        "            if (h != self.img_hw[0]) or (w != self.img_hw[1]):\n",
        "                scale = self.img_hw[-1] / w\n",
        "                src_intrinsics[i, :2] *= scale\n",
        "\n",
        "                rgb = F.interpolate(rgb[None, :], size=self.img_hw, mode=\"area\")[0]\n",
        "                mask = F.interpolate(mask[None, :], size=self.img_hw, mode=\"area\")[0]\n",
        "\n",
        "            src_rgbs.append(rgb)\n",
        "            src_masks.append(mask)\n",
        "\n",
        "        depth_range = np.array([self.z_near, self.z_far])\n",
        "\n",
        "        return {\n",
        "            \"rgb_path\": rgb_path,\n",
        "            \"img_id\": index,\n",
        "            \"img_hw\": self.img_hw,\n",
        "            \"tgt_mask\": tgt_mask.permute([1, 2, 0]).float(),\n",
        "            \"tgt_rgb\": tgt_rgb.permute([1, 2, 0]).float(),\n",
        "            \"tgt_c2w_mat\": torch.FloatTensor(render_pose),\n",
        "            \"tgt_intrinsic\": torch.FloatTensor(intrinsic),\n",
        "            \"tgt_bbox\": tgt_bbox,\n",
        "            \"src_masks\": torch.stack(src_masks).permute([0, 2, 3, 1]).float(),\n",
        "            \"src_rgbs\": torch.stack(src_rgbs).permute([0, 2, 3, 1]).float(),\n",
        "            \"src_c2w_mats\": torch.FloatTensor(src_c2w_mats),\n",
        "            \"src_intrinsics\": torch.FloatTensor(src_intrinsics),\n",
        "            \"depth_range\": torch.FloatTensor(depth_range)\n",
        "        }"
      ],
      "metadata": {
        "id": "AWz4x0FN4dWc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "data/DVR_eval.py"
      ],
      "metadata": {
        "id": "tZjmy_aa4sbd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "\n",
        "import imageio\n",
        "import numpy as np\n",
        "import tqdm\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import  torchvision.transforms as T\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "def parse_pose(path, num_views):\n",
        "    cameras = np.load(path)\n",
        "\n",
        "    intrinsics = []\n",
        "    c2w_mats = []\n",
        "\n",
        "    for i in range(num_views):\n",
        "        # ShapeNet\n",
        "        wmat_inv_key = \"world_mat_inv_\" + str(i)\n",
        "        wmat_key = \"world_mat_\" + str(i)\n",
        "        kmat_key = \"camera_mat_\" + str(i)\n",
        "        if wmat_inv_key in cameras:\n",
        "            c2w_mat = cameras[wmat_inv_key]\n",
        "        else:\n",
        "            w2c_mat = cameras[wmat_key]\n",
        "            if w2c_mat.shape[0] == 3:\n",
        "                w2c_mat = np.vstack((w2c_mat, np.array([0, 0, 0, 1])))\n",
        "            c2w_mat = np.linalg.inv(w2c_mat)\n",
        "\n",
        "        intrinsics.append(cameras[kmat_key])\n",
        "        c2w_mats.append(c2w_mat)\n",
        "\n",
        "    intrinsics = np.stack(intrinsics, 0)\n",
        "    c2w_mats = np.stack(c2w_mats, 0)\n",
        "\n",
        "    return intrinsics, c2w_mats\n",
        "\n",
        "class DVREvalDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset from DVR (Niemeyer et al. 2020)\n",
        "    Provides 3D-R2N2 and NMR renderings\n",
        "    \"\"\"\n",
        "    def __init__(self, args, mode,\n",
        "                **kwargs):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            args.data_path: path to data directory\n",
        "            args.img_hw: image size (resize if needed)\n",
        "            mode: train | test | val mode\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.base_path = args.data_path\n",
        "        self.dataset_name = os.path.basename(args.data_path)\n",
        "        assert os.path.exists(self.base_path)\n",
        "\n",
        "        cats = [x for x in glob.glob(os.path.join(args.data_path, \"*\")) if os.path.isdir(x)]\n",
        "\n",
        "        list_prefix = \"gen_\"\n",
        "\n",
        "        if mode == \"train\":\n",
        "            file_lists = [os.path.join(x, list_prefix + \"train.lst\") for x in cats]\n",
        "        elif mode == \"val\":\n",
        "            file_lists = [os.path.join(x, list_prefix + \"val.lst\") for x in cats]\n",
        "        elif mode == \"test\":\n",
        "            file_lists = [os.path.join(x, list_prefix + \"test.lst\") for x in cats]\n",
        "\n",
        "        print(\"Loading NMR dataset\", self.base_path, \"name:\", self.dataset_name, \"mode:\", mode)\n",
        "        self.mode = mode\n",
        "\n",
        "        all_objs = []\n",
        "        for file_list in file_lists:\n",
        "            if not os.path.exists(file_list):\n",
        "                continue\n",
        "            base_dir = os.path.dirname(file_list)\n",
        "            cat = os.path.basename(base_dir)\n",
        "            with open(file_list, \"r\") as f:\n",
        "                objs = [(cat, os.path.join(base_dir, x.strip())) for x in f.readlines()]\n",
        "            all_objs.extend(objs)\n",
        "\n",
        "        self.all_objs = all_objs\n",
        "\n",
        "        if args.debug:\n",
        "            self.all_objs = self.all_objs[:1]\n",
        "\n",
        "        if mode == \"val\" or mode == \"test\":\n",
        "            self.all_objs = self.all_objs[:100] # HACK to avoid reading too much things\n",
        "\n",
        "        self.intrinsics = []\n",
        "        self.poses = []\n",
        "        self.rgb_paths = []\n",
        "        for _, path in tqdm.tqdm(self.all_objs):\n",
        "            curr_paths = sorted(glob.glob(os.path.join(path, \"image\", \"*\")))\n",
        "            self.rgb_paths.append(curr_paths)\n",
        "\n",
        "            pose_path = os.path.join(path, 'cameras.npz')\n",
        "            intrinsics, c2w_mats = parse_pose(pose_path, len(curr_paths))\n",
        "\n",
        "            self.poses.append(c2w_mats)\n",
        "            self.intrinsics.append(intrinsics)\n",
        "\n",
        "        self.rgb_paths = np.array(self.rgb_paths)\n",
        "        self.poses = np.stack(self.poses, 0)\n",
        "        self.intrinsics = np.array(self.intrinsics)\n",
        "\n",
        "        assert(len(self.rgb_paths) == len(self.poses))\n",
        "\n",
        "        self.define_transforms()\n",
        "        self.img_hw = args.img_hw\n",
        "\n",
        "        self.num_views = args.num_source_views\n",
        "        self.closest_n_views = args.closest_n_views\n",
        "\n",
        "        # default near/far plane depth\n",
        "        self.z_near = 1.2\n",
        "        self.z_far = 4.0\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.intrinsics)\n",
        "\n",
        "    def define_transforms(self):\n",
        "        self.img_transforms = T.Compose(\n",
        "            [T.ToTensor(), T.Normalize((0.0, 0.0, 0.0), (1.0, 1.0, 1.0))]\n",
        "        )\n",
        "        self.mask_transforms = T.Compose(\n",
        "            [T.ToTensor(), T.Normalize((0.0,), (1.0,))]\n",
        "        )\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        rgb_paths = self.rgb_paths[index]\n",
        "        c2w_mats = np.array(self.poses[index])\n",
        "        intrinsics = np.array(self.intrinsics[index])\n",
        "\n",
        "        rgbs = []\n",
        "        masks = []\n",
        "        bboxes = []\n",
        "\n",
        "        # Read all RGB\n",
        "        for i in range(len(rgb_paths)):\n",
        "            img = imageio.imread(rgb_paths[i])[..., :3]\n",
        "            mask = (img.sum(axis=-1) != 255*3)[..., None].astype(np.uint8) * 255\n",
        "            rgb = self.img_transforms(img)\n",
        "            mask = self.mask_transforms(mask)\n",
        "\n",
        "            intrinsics[i, 0, 0] *= img.shape[1] / 2.0\n",
        "            intrinsics[i, 1, 1] *= img.shape[0] / 2.0\n",
        "            intrinsics[i, 0, 2] = img.shape[1] / 2.0\n",
        "            intrinsics[i, 1, 2] = img.shape[0] / 2.0\n",
        "\n",
        "            h, w = rgb.shape[-2:]\n",
        "            if (h != self.img_hw[0]) or (w != self.img_hw[1]):\n",
        "                scale = self.img_hw[-1] / w\n",
        "                intrinsics[i, :2] *= scale\n",
        "\n",
        "                rgb = F.interpolate(rgb[None, :], size=self.img_hw, mode=\"area\")[0]\n",
        "                mask = F.interpolate(mask[None, :], size=self.img_hw, mode=\"area\")[0]\n",
        "\n",
        "            rgbs.append(rgb)\n",
        "            masks.append(mask)\n",
        "\n",
        "            yy = torch.any(mask, axis=2)\n",
        "            xx = torch.any(mask, axis=1)\n",
        "            ynz = torch.nonzero(yy)[:, 1]\n",
        "            xnz = torch.nonzero(xx)[:, 1]\n",
        "            ymin, ymax = ynz[[0, -1]]\n",
        "            xmin, xmax = xnz[[0, -1]]\n",
        "            bbox = torch.FloatTensor([xmin, ymin, xmax, ymax])\n",
        "\n",
        "            bboxes.append(bbox)\n",
        "\n",
        "        depth_range = np.array([self.z_near, self.z_far])\n",
        "\n",
        "        return {\n",
        "            \"rgb_path\": rgb_paths[0],\n",
        "            \"img_id\": index,\n",
        "            \"img_hw\": self.img_hw,\n",
        "            \"bbox\": torch.stack(bboxes, 0),\n",
        "            \"masks\": torch.stack(masks).permute([0, 2, 3, 1]).float(),\n",
        "            \"rgbs\": torch.stack(rgbs).permute([0, 2, 3, 1]).float(),\n",
        "            \"c2w_mats\": torch.FloatTensor(c2w_mats),\n",
        "            \"intrinsics\": torch.FloatTensor(intrinsics),\n",
        "            \"depth_range\": torch.FloatTensor(depth_range)\n",
        "        }"
      ],
      "metadata": {
        "id": "RFWXcwec4wzk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "data/srn.py"
      ],
      "metadata": {
        "id": "27yBN7WG41N7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "\n",
        "import imageio\n",
        "import numpy as np\n",
        "import tqdm\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import  torchvision.transforms as T\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "def parse_intrinsic(path):\n",
        "    with open(path, \"r\") as f:\n",
        "        lines = f.readlines()\n",
        "        focal, cx, cy, _ = map(float, lines[0].split())\n",
        "    intrinsic = np.array([[focal, 0, cx, 0],\n",
        "                          [0, focal, cy, 0],\n",
        "                          [0,     0,  1, 0],\n",
        "                          [0,     0,  0, 1]])\n",
        "    return intrinsic\n",
        "\n",
        "def parse_pose(path):\n",
        "    return np.loadtxt(path, dtype=np.float32).reshape(4, 4)\n",
        "\n",
        "class SRNDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset from SRN (V. Sitzmann et al. 2020)\n",
        "    \"\"\"\n",
        "    def __init__(self, args, mode, **kwargs):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            args.data_path: path to data directory\n",
        "            args.img_hw: image size (resize if needed)\n",
        "            mode: train | test | val mode\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.base_path = args.data_path + \"_\" + mode\n",
        "        self.dataset_name = os.path.basename(args.data_path)\n",
        "\n",
        "        print(\"Loading SRN dataset\", self.base_path, \"name:\", self.dataset_name)\n",
        "        self.mode = mode\n",
        "        assert os.path.exists(self.base_path)\n",
        "\n",
        "        is_chair = \"chair\" in self.dataset_name\n",
        "        if is_chair and mode == \"train\":\n",
        "            # Ugly thing from SRN's public dataset\n",
        "            tmp = os.path.join(self.base_path, \"chairs_2.0_train\")\n",
        "            if os.path.exists(tmp):\n",
        "                self.base_path = tmp\n",
        "\n",
        "        intrinsic_paths = sorted(\n",
        "            glob.glob(os.path.join(self.base_path, \"*\", \"intrinsics.txt\"))\n",
        "        )\n",
        "\n",
        "        if args.debug:\n",
        "            intrinsic_paths = intrinsic_paths[:1]\n",
        "\n",
        "        self.intrinsics = []\n",
        "        self.poses = []\n",
        "        self.rgb_paths = []\n",
        "        for path in tqdm.tqdm(intrinsic_paths):\n",
        "            dir = os.path.dirname(path)\n",
        "            curr_paths = sorted(glob.glob(os.path.join(dir, \"rgb\", \"*\")))\n",
        "            self.rgb_paths.append(curr_paths)\n",
        "\n",
        "            pose_paths = [f.replace('rgb', 'pose').replace('png', 'txt') for f in curr_paths]\n",
        "            c2w_mats = [parse_pose(x) for x in\n",
        "                    pose_paths]\n",
        "            self.poses.append(c2w_mats)\n",
        "\n",
        "            self.intrinsics.append(parse_intrinsic(path))\n",
        "\n",
        "        self.rgb_paths = np.array(self.rgb_paths)\n",
        "        self.poses = np.stack(self.poses, 0)\n",
        "        self.intrinsics = np.array(self.intrinsics)\n",
        "\n",
        "        assert(len(self.rgb_paths) == len(self.poses))\n",
        "\n",
        "        self.define_transforms()\n",
        "        self.img_hw = args.img_hw\n",
        "\n",
        "        self.num_views = args.num_source_views\n",
        "        self.closest_n_views = args.closest_n_views\n",
        "\n",
        "        # Default near/far plane depth\n",
        "        if is_chair:\n",
        "            self.z_near = 1.25\n",
        "            self.z_far = 2.75\n",
        "        else:\n",
        "            self.z_near = 0.8\n",
        "            self.z_far = 1.8\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.intrinsics)\n",
        "\n",
        "    def define_transforms(self):\n",
        "        self.img_transforms = T.Compose(\n",
        "            [T.ToTensor(), T.Normalize((0.0, 0.0, 0.0), (1.0, 1.0, 1.0))]\n",
        "        )\n",
        "        self.mask_transforms = T.Compose(\n",
        "            [T.ToTensor(), T.Normalize((0.0,), (1.0,))]\n",
        "        )\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        intrinsic = self.intrinsics[index].copy()\n",
        "\n",
        "        train_poses = self.poses[index]\n",
        "\n",
        "        render_idx = np.random.choice(len(train_poses), 1, replace=False)[0]\n",
        "        rgb_path = self.rgb_paths[index, render_idx]\n",
        "        render_pose = train_poses[render_idx]\n",
        "        if self.closest_n_views > 0:\n",
        "            nearest_pose_ids = get_nearest_pose_ids(render_pose,\n",
        "                                                    train_poses,\n",
        "                                                    self.closest_n_views,\n",
        "                                                    tar_id=render_idx,\n",
        "                                                    angular_dist_method='vector')\n",
        "        else:\n",
        "            nearest_pose_ids = np.arange(len(train_poses))\n",
        "            nearest_pose_ids = np.delete(nearest_pose_ids, render_idx)\n",
        "        nearest_pose_ids = np.random.choice(nearest_pose_ids, self.num_views, replace=False)\n",
        "\n",
        "        # Read target RGB\n",
        "        img = imageio.imread(rgb_path)[..., :3]\n",
        "        mask = (img.sum(axis=-1) != 255*3)[..., None].astype(np.uint8) * 255\n",
        "        tgt_rgb = self.img_transforms(img)\n",
        "        tgt_mask = self.mask_transforms(mask)\n",
        "\n",
        "        h, w = tgt_rgb.shape[-2:]\n",
        "        if (h != self.img_hw[0]) or (w != self.img_hw[1]):\n",
        "            scale = self.img_hw[-1] / img.shape[1]\n",
        "            intrinsic[:2] *= scale\n",
        "\n",
        "            tgt_rgb = F.interpolate(tgt_rgb[None, :], size=self.img_hw, mode=\"area\")[0]\n",
        "            tgt_mask = F.interpolate(tgt_mask[None, :], size=self.img_hw, mode=\"area\")[0]\n",
        "\n",
        "        yy = torch.any(tgt_mask, axis=2)\n",
        "        xx = torch.any(tgt_mask, axis=1)\n",
        "        ynz = torch.nonzero(yy)[:, 1]\n",
        "        xnz = torch.nonzero(xx)[:, 1]\n",
        "        ymin, ymax = ynz[[0, -1]]\n",
        "        xmin, xmax = xnz[[0, -1]]\n",
        "        tgt_bbox = torch.FloatTensor([xmin, ymin, xmax, ymax])\n",
        "\n",
        "        # Read source RGB\n",
        "        src_rgb_paths = [self.rgb_paths[index][x] for x in nearest_pose_ids]\n",
        "        src_c2w_mats = np.array([train_poses[x] for x in nearest_pose_ids])\n",
        "        src_intrinsics = np.array([self.intrinsics[index]] * len(nearest_pose_ids))\n",
        "\n",
        "        src_rgbs = []\n",
        "        src_masks = []\n",
        "        for i, rgb_path in enumerate(src_rgb_paths):\n",
        "            img = imageio.imread(rgb_path)[..., :3]\n",
        "            mask = (img.sum(axis=-1) != 255*3)[..., None].astype(np.uint8) * 255\n",
        "            rgb = self.img_transforms(img)\n",
        "            mask = self.mask_transforms(mask)\n",
        "\n",
        "            h, w = rgb.shape[-2:]\n",
        "            if (h != self.img_hw[0]) or (w != self.img_hw[1]):\n",
        "                scale = self.img_hw[-1] / w\n",
        "                src_intrinsics[i, :2] *= scale\n",
        "\n",
        "                rgb = F.interpolate(rgb[None, :], size=self.img_hw, mode=\"area\")[0]\n",
        "                mask = F.interpolate(mask[None, :], size=self.img_hw, mode=\"area\")[0]\n",
        "\n",
        "            src_rgbs.append(rgb)\n",
        "            src_masks.append(mask)\n",
        "\n",
        "        depth_range = np.array([self.z_near, self.z_far])\n",
        "\n",
        "        return {\n",
        "            \"rgb_path\": rgb_path,\n",
        "            \"img_id\": index,\n",
        "            \"img_hw\": self.img_hw,\n",
        "            \"tgt_mask\": tgt_mask.permute([1, 2, 0]).float(),\n",
        "            \"tgt_rgb\": tgt_rgb.permute([1, 2, 0]).float(),\n",
        "            \"tgt_c2w_mat\": torch.FloatTensor(render_pose),\n",
        "            \"tgt_intrinsic\": torch.FloatTensor(intrinsic),\n",
        "            \"tgt_bbox\": tgt_bbox,\n",
        "            \"src_masks\": torch.stack(src_masks).permute([0, 2, 3, 1]).float(),\n",
        "            \"src_rgbs\": torch.stack(src_rgbs).permute([0, 2, 3, 1]).float(),\n",
        "            \"src_c2w_mats\": torch.FloatTensor(src_c2w_mats),\n",
        "            \"src_intrinsics\": torch.FloatTensor(src_intrinsics),\n",
        "            \"depth_range\": torch.FloatTensor(depth_range)\n",
        "        }"
      ],
      "metadata": {
        "id": "QlfeE9p843PK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "data/srn_eval.py"
      ],
      "metadata": {
        "id": "TEdcUZm4495o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "\n",
        "import imageio\n",
        "import numpy as np\n",
        "import tqdm\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import  torchvision.transforms as T\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "def parse_intrinsic(path):\n",
        "    with open(path, \"r\") as f:\n",
        "        lines = f.readlines()\n",
        "        focal, cx, cy, _ = map(float, lines[0].split())\n",
        "    intrinsic = np.array([[focal, 0, cx, 0],\n",
        "                          [0, focal, cy, 0],\n",
        "                          [0,     0,  1, 0],\n",
        "                          [0,     0,  0, 1]])\n",
        "    return intrinsic\n",
        "\n",
        "def parse_pose(path):\n",
        "    return np.loadtxt(path, dtype=np.float32).reshape(4, 4)\n",
        "\n",
        "class SRNEvalDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset from SRN (V. Sitzmann et al. 2020)\n",
        "    \"\"\"\n",
        "    def __init__(self, args, mode, **kwargs):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            args.data_path: path to data directory\n",
        "            args.img_hw: image size (resize if needed)\n",
        "            mode: train | test | val mode\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.base_path = args.data_path + \"_\" + mode\n",
        "        self.dataset_name = os.path.basename(args.data_path)\n",
        "\n",
        "        print(\"Loading SRN dataset\", self.base_path, \"name:\", self.dataset_name)\n",
        "        self.mode = mode\n",
        "        assert os.path.exists(self.base_path)\n",
        "\n",
        "        is_chair = \"chair\" in self.dataset_name\n",
        "        if is_chair and mode == \"train\":\n",
        "            # Ugly thing from SRN's public dataset\n",
        "            tmp = os.path.join(self.base_path, \"chairs_2.0_train\")\n",
        "            if os.path.exists(tmp):\n",
        "                self.base_path = tmp\n",
        "\n",
        "        intrinsic_paths = sorted(\n",
        "            glob.glob(os.path.join(self.base_path, \"*\", \"intrinsics.txt\"))\n",
        "        )\n",
        "\n",
        "        if args.debug:\n",
        "            intrinsic_paths = intrinsic_paths[:10]\n",
        "\n",
        "        self.intrinsics = []\n",
        "        self.poses = []\n",
        "        self.rgb_paths = []\n",
        "        for path in tqdm.tqdm(intrinsic_paths):\n",
        "            dir = os.path.dirname(path)\n",
        "            curr_paths = sorted(glob.glob(os.path.join(dir, \"rgb\", \"*\")))\n",
        "            self.rgb_paths.append(curr_paths)\n",
        "\n",
        "            pose_paths = [f.replace('rgb', 'pose').replace('png', 'txt') for f in curr_paths]\n",
        "            c2w_mats = [parse_pose(x) for x in\n",
        "                    pose_paths]\n",
        "            self.poses.append(c2w_mats)\n",
        "\n",
        "            self.intrinsics.append(parse_intrinsic(path))\n",
        "\n",
        "        self.rgb_paths = np.array(self.rgb_paths)\n",
        "        self.poses = np.stack(self.poses, 0)\n",
        "        self.intrinsics = np.array(self.intrinsics)\n",
        "\n",
        "        assert(len(self.rgb_paths) == len(self.poses))\n",
        "\n",
        "        self.define_transforms()\n",
        "        self.img_hw = args.img_hw\n",
        "\n",
        "        self.num_views = args.num_source_views\n",
        "        self.closest_n_views = args.closest_n_views\n",
        "\n",
        "        # Default near/far plane depth\n",
        "        if is_chair:\n",
        "            self.z_near = 1.25\n",
        "            self.z_far = 2.75\n",
        "        else:\n",
        "            self.z_near = 0.8\n",
        "            self.z_far = 1.8\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.intrinsics)\n",
        "\n",
        "    def define_transforms(self):\n",
        "        self.img_transforms = T.Compose(\n",
        "            [T.ToTensor(), T.Normalize((0.0, 0.0, 0.0), (1.0, 1.0, 1.0))]\n",
        "        )\n",
        "        self.mask_transforms = T.Compose(\n",
        "            [T.ToTensor(), T.Normalize((0.0,), (1.0,))]\n",
        "        )\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        rgb_paths = self.rgb_paths[index]\n",
        "        c2w_mats = np.array(self.poses[index])\n",
        "        intrinsics = np.array([self.intrinsics[index]] * len(rgb_paths))\n",
        "\n",
        "        rgbs = []\n",
        "        masks = []\n",
        "        bboxes = []\n",
        "\n",
        "        # Read all RGB\n",
        "        for i in range(len(rgb_paths)):\n",
        "\n",
        "            img = imageio.imread(rgb_paths[i])[..., :3]\n",
        "            mask = (img.sum(axis=-1) != 255*3)[..., None].astype(np.uint8) * 255\n",
        "            rgb = self.img_transforms(img)\n",
        "            mask = self.mask_transforms(mask)\n",
        "\n",
        "            h, w = rgb.shape[-2:]\n",
        "            if (h != self.img_hw[0]) or (w != self.img_hw[1]):\n",
        "                scale = self.img_hw[-1] / w\n",
        "                intrinsics[i, :2] *= scale\n",
        "\n",
        "                rgb = F.interpolate(rgb[None, :], size=self.img_hw, mode=\"area\")[0]\n",
        "                mask = F.interpolate(mask[None, :], size=self.img_hw, mode=\"area\")[0]\n",
        "\n",
        "            rgbs.append(rgb)\n",
        "            masks.append(mask)\n",
        "\n",
        "            yy = torch.any(mask, axis=2)\n",
        "            xx = torch.any(mask, axis=1)\n",
        "            ynz = torch.nonzero(yy)[:, 1]\n",
        "            xnz = torch.nonzero(xx)[:, 1]\n",
        "            ymin, ymax = ynz[[0, -1]]\n",
        "            xmin, xmax = xnz[[0, -1]]\n",
        "            bbox = torch.FloatTensor([xmin, ymin, xmax, ymax])\n",
        "            bboxes.append(bbox)\n",
        "\n",
        "        depth_range = np.array([self.z_near, self.z_far])\n",
        "\n",
        "        return {\n",
        "            \"rgb_path\": rgb_paths[0],\n",
        "            \"img_id\": index,\n",
        "            \"img_hw\": self.img_hw,\n",
        "            \"bbox\": torch.stack(bboxes, 0),\n",
        "            \"masks\": torch.stack(masks).permute([0, 2, 3, 1]).float(),\n",
        "            \"rgbs\": torch.stack(rgbs).permute([0, 2, 3, 1]).float(),\n",
        "            \"c2w_mats\": torch.FloatTensor(c2w_mats),\n",
        "            \"intrinsics\": torch.FloatTensor(intrinsics),\n",
        "            \"depth_range\": torch.FloatTensor(depth_range)\n",
        "        }"
      ],
      "metadata": {
        "id": "0Hav2Xy04-R6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "data/_init.py"
      ],
      "metadata": {
        "id": "PhRcfVQK5QKn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_dict = {\n",
        "    'srn': SRNDataset,\n",
        "    'dvr': DVRDataset,\n",
        "}\n",
        "\n",
        "eval_dataset_dict = {\n",
        "    'srn': SRNEvalDataset,\n",
        "    'dvr': DVREvalDataset,\n",
        "}"
      ],
      "metadata": {
        "id": "B5vt9rIR5Pkc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "data/create_training_dataset.py"
      ],
      "metadata": {
        "id": "uxWwawOr4HLG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Optional\n",
        "from operator import itemgetter\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, Sampler\n",
        "from torch.utils.data import DistributedSampler, WeightedRandomSampler\n",
        "\n",
        "\n",
        "class DatasetFromSampler(Dataset):\n",
        "    \"\"\"Dataset to create indexes from `Sampler`.\n",
        "    Args:\n",
        "        sampler: PyTorch sampler\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, sampler: Sampler):\n",
        "        \"\"\"Initialisation for DatasetFromSampler.\"\"\"\n",
        "        self.sampler = sampler\n",
        "        self.sampler_list = None\n",
        "\n",
        "    def __getitem__(self, index: int):\n",
        "        \"\"\"Gets element of the dataset.\n",
        "        Args:\n",
        "            index: index of the element in the dataset\n",
        "        Returns:\n",
        "            Single element by index\n",
        "        \"\"\"\n",
        "        if self.sampler_list is None:\n",
        "            self.sampler_list = list(self.sampler)\n",
        "        return self.sampler_list[index]\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        \"\"\"\n",
        "        Returns:\n",
        "            int: length of the dataset\n",
        "        \"\"\"\n",
        "        return len(self.sampler)\n",
        "\n",
        "\n",
        "class DistributedSamplerWrapper(DistributedSampler):\n",
        "    \"\"\"\n",
        "    Wrapper over `Sampler` for distributed training.\n",
        "    Allows you to use any sampler in distributed mode.\n",
        "    It is especially useful in conjunction with\n",
        "    `torch.nn.parallel.DistributedDataParallel`. In such case, each\n",
        "    process can pass a DistributedSamplerWrapper instance as a DataLoader\n",
        "    sampler, and load a subset of subsampled data of the original dataset\n",
        "    that is exclusive to it.\n",
        "    .. note::\n",
        "        Sampler is assumed to be of constant size.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        sampler,\n",
        "        num_replicas: Optional[int] = None,\n",
        "        rank: Optional[int] = None,\n",
        "        shuffle: bool = True,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            sampler: Sampler used for subsampling\n",
        "            num_replicas (int, optional): Number of processes participating in\n",
        "              distributed training\n",
        "            rank (int, optional): Rank of the current process\n",
        "              within ``num_replicas``\n",
        "            shuffle (bool, optional): If true (default),\n",
        "              sampler will shuffle the indices\n",
        "        \"\"\"\n",
        "        super(DistributedSamplerWrapper, self).__init__(\n",
        "            DatasetFromSampler(sampler),\n",
        "            num_replicas=num_replicas,\n",
        "            rank=rank,\n",
        "            shuffle=shuffle,\n",
        "        )\n",
        "        self.sampler = sampler\n",
        "\n",
        "    def __iter__(self):\n",
        "        self.dataset = DatasetFromSampler(self.sampler)\n",
        "        indexes_of_indexes = super().__iter__()\n",
        "        subsampler_indexes = self.dataset\n",
        "        return iter(itemgetter(*indexes_of_indexes)(subsampler_indexes))\n",
        "\n",
        "\n",
        "def create_training_dataset(args):\n",
        "    # parse args.data_type, \"+\" indicates that multiple datasets are used, for example \"ibrnet_collect+llff+spaces\"\n",
        "    # otherwise only one dataset is used\n",
        "    # args.dataset_weights should be a list representing the resampling rate for each dataset, and should sum up to 1\n",
        "\n",
        "    print('training dataset: {}'.format(args.data_type))\n",
        "    mode = 'train'\n",
        "    if '+' not in args.data_type:\n",
        "        train_dataset = dataset_dict[args.data_type](args, mode)\n",
        "        train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset) if args.distributed else None\n",
        "    else:\n",
        "        train_dataset_names = args.data_type.split('+')\n",
        "        weights = args.dataset_weights\n",
        "        assert len(train_dataset_names) == len(weights)\n",
        "        assert np.abs(np.sum(weights) - 1.) < 1e-6\n",
        "        print('weights:{}'.format(weights))\n",
        "        train_datasets = []\n",
        "        train_weights_samples = []\n",
        "        for training_dataset_name, weight in zip(train_dataset_names, weights):\n",
        "            train_dataset = dataset_dict[training_dataset_name](args, mode,\n",
        "                                                                scenes=args.train_scenes,\n",
        "                                                                )\n",
        "            train_datasets.append(train_dataset)\n",
        "            num_samples = len(train_dataset)\n",
        "            weight_each_sample = weight / num_samples\n",
        "            train_weights_samples.extend([weight_each_sample]*num_samples)\n",
        "\n",
        "        train_dataset = torch.utils.data.ConcatDataset(train_datasets)\n",
        "        train_weights = torch.from_numpy(np.array(train_weights_samples))\n",
        "        sampler = WeightedRandomSampler(train_weights, len(train_weights))\n",
        "        train_sampler = DistributedSamplerWrapper(sampler) if args.distributed else sampler\n",
        "\n",
        "    return train_dataset, train_sampler"
      ],
      "metadata": {
        "id": "YieGzcar4Hjx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "gen_real.py"
      ],
      "metadata": {
        "id": "MH5xfWh_6_oy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import shutil\n",
        "\n",
        "import tqdm\n",
        "import imageio\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def parse_intrinsic(focal, cx, cy):\n",
        "    intrinsic = np.array([[focal, 0, cx, 0],\n",
        "                          [0, focal, cy, 0],\n",
        "                          [0,     0,  1, 0],\n",
        "                          [0,     0,  0, 1]])\n",
        "    return intrinsic\n",
        "\n",
        "class RealRenderDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset for rendering\n",
        "    \"\"\"\n",
        "    def __init__(self, args, **kwargs):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            args.data_path: path to data directory\n",
        "            args.img_hw: image size (resize if needed)\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.base_path = args.data_path\n",
        "\n",
        "        print(\"Loading real dataset\", self.base_path)\n",
        "        assert os.path.exists(self.base_path)\n",
        "        #print(\"DEBUG: BEING HERE AFTER ASSERT\")\n",
        "        #self.rgb_paths = sorted(glob.glob(os.path.join(self.base_path, \"*_normalize.jpg\"))) + \\\n",
        "        #    sorted(glob.glob(os.path.join(self.base_path, \"*_normalize.png\")))\n",
        "        self.rgb_paths = sorted(glob.glob(os.path.join(self.base_path, 'rgb','*.jpg'))) + \\\n",
        "            sorted(glob.glob(os.path.join(self.base_path, 'rgb','*.png')))\n",
        "        #print(os.path.join(self.base_path, 'rgb','*.png'))\n",
        "        #print(glob.glob(os.path.join(self.base_path,'rgb', '*.png')))\n",
        "        #print(\"DEBUG: BEING HERE AFTER RGB_PATH\")\n",
        "        self.poses = []\n",
        "        self.intrinsics = []\n",
        "        #print(\"DEBUG: RGB_PATH: \",self.rgb_paths)\n",
        "        for i in range(len(self.rgb_paths)):\n",
        "            #print(\"DEBUG: BEING HERE IN ITERATION\")\n",
        "            intrinsic = parse_intrinsic(args.focal, args.img_hw[0]//2, args.img_hw[1]//2)\n",
        "            cam_pose = trans_t(args.radius)\n",
        "            self.poses.append(cam_pose)\n",
        "            self.intrinsics.append(intrinsic)\n",
        "\n",
        "        self.rgb_paths = np.array(self.rgb_paths)\n",
        "        self.poses = np.stack(self.poses, 0)\n",
        "        self.intrinsics = np.array(self.intrinsics)\n",
        "\n",
        "        self.define_transforms()\n",
        "        self.img_hw = args.img_hw\n",
        "\n",
        "        # default near/far plane depth\n",
        "        self.z_near = args.z_near\n",
        "        self.z_far = args.z_far\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.rgb_paths)\n",
        "\n",
        "    def define_transforms(self):\n",
        "        self.img_transforms = T.Compose(\n",
        "            [T.ToTensor(), T.Normalize((0.0, 0.0, 0.0), (1.0, 1.0, 1.0))]\n",
        "        )\n",
        "        self.mask_transforms = T.Compose(\n",
        "            [T.ToTensor(), T.Normalize((0.0,), (1.0,))]\n",
        "        )\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Read source RGB\n",
        "        src_rgb_path = self.rgb_paths[index]\n",
        "        src_c2w_mat = self.poses[index]\n",
        "        src_intrinsics = self.intrinsics[index]\n",
        "\n",
        "        img = imageio.imread(src_rgb_path)[..., :3]\n",
        "        mask = (img.sum(axis=-1) != 255*3)[..., None].astype(np.uint8) * 255\n",
        "        rgb = self.img_transforms(img)\n",
        "        mask = self.mask_transforms(mask)\n",
        "\n",
        "        h, w = rgb.shape[-2:]\n",
        "        if (h != self.img_hw[0]) or (w != self.img_hw[1]):\n",
        "            scale = self.img_hw[-1] / rgb.shape[-1]\n",
        "            src_intrinsics[:, :2] *= scale\n",
        "\n",
        "            rgb = F.interpolate(rgb, size=self.img_hw, mode=\"area\")\n",
        "            mask = F.interpolate(mask, size=self.img_hw, mode=\"area\")\n",
        "\n",
        "        depth_range = np.array([self.z_near, self.z_far])\n",
        "\n",
        "        return {\n",
        "            \"rgb_path\": src_rgb_path,\n",
        "            \"img_id\": index,\n",
        "            \"img_hw\": self.img_hw,\n",
        "            \"src_rgbs\": rgb[None, ...].permute([0, 2, 3, 1]).float(),\n",
        "            \"src_masks\": mask[None, ...].permute([0, 2, 3, 1]).float(),\n",
        "            \"src_c2w_mats\": torch.FloatTensor(src_c2w_mat)[None, :],\n",
        "            \"src_intrinsics\": torch.FloatTensor(src_intrinsics)[None, :],\n",
        "            \"depth_range\": torch.FloatTensor(depth_range)\n",
        "        }\n",
        "\n",
        "\n",
        "def trans_t(t):\n",
        "    return torch.tensor(\n",
        "        [[-1, 0, 0, 0], [0, 0, -1, t], [0, -1, 0, 0], [0, 0, 0, 1],], dtype=torch.float32,\n",
        "    )\n",
        "\n",
        "def rot_theta(angle):\n",
        "    return torch.tensor(\n",
        "        [\n",
        "            [np.cos(angle), -np.sin(angle), 0, 0],\n",
        "            [np.sin(angle), np.cos(angle), 0, 0],\n",
        "            [0, 0, 1, 0],\n",
        "            [0, 0, 0, 1],\n",
        "        ],\n",
        "        dtype=torch.float32,\n",
        "    )\n",
        "\n",
        "def rot_phi(phi):\n",
        "    return torch.tensor(\n",
        "        [\n",
        "            [1, 0, 0, 0],\n",
        "            [0, np.cos(phi), -np.sin(phi), 0],\n",
        "            [0, np.sin(phi), np.cos(phi), 0],\n",
        "            [0, 0, 0, 1],\n",
        "        ],\n",
        "        dtype=torch.float32,\n",
        "    )\n",
        "\n",
        "def pose_spherical(theta, phi, radius):\n",
        "    \"\"\"\n",
        "    Spherical rendering poses, from NeRF\n",
        "    \"\"\"\n",
        "    c2w = trans_t(radius)\n",
        "    c2w = rot_phi(phi / 180.0 * np.pi) @ c2w\n",
        "    c2w = rot_theta(theta / 180.0 * np.pi) @ c2w\n",
        "\n",
        "    return c2w\n",
        "\n",
        "def gen_video(args):\n",
        "\n",
        "    device = \"cuda\"\n",
        "    print(f\"checkpoints reload from {args.ckptdir}\")\n",
        "    #print(\"DEBUG: Being Here\")\n",
        "    dataset = RealRenderDataset(args)\n",
        "    #print(\"DEBUG: Being Here\")\n",
        "    # Create VisionNeRF model\n",
        "    model = VisionNerfModel(args, False, False)\n",
        "    # create projector\n",
        "    projector = Projector(device=device)\n",
        "    model.switch_to_eval()\n",
        "\n",
        "    if not args.data_index:\n",
        "        args.data_index = [x for x in range(len(dataset))]\n",
        "\n",
        "    for d_idx in args.data_index:\n",
        "        out_folder = os.path.join(args.outdir, args.expname, f'{d_idx:06d}')\n",
        "        print(f'Rendering {dataset[d_idx][\"rgb_path\"][:-15]}')\n",
        "        print(f'videos will be saved to {out_folder}')\n",
        "        os.makedirs(out_folder, exist_ok=True)\n",
        "        # save the args and config files\n",
        "        f = os.path.join(out_folder, 'args.txt')\n",
        "        with open(f, 'w') as file:\n",
        "            for arg in sorted(vars(args)):\n",
        "                attr = getattr(args, arg)\n",
        "                file.write('{} = {}\\n'.format(arg, attr))\n",
        "\n",
        "        if args.config is not None:\n",
        "            f = os.path.join(out_folder, 'config.txt')\n",
        "            if not os.path.isfile(f):\n",
        "                shutil.copy(args.config, f)\n",
        "\n",
        "        sample = dataset[d_idx]\n",
        "        pose_index = 0\n",
        "        data_input = dict(\n",
        "            rgb_path=sample['rgb_path'],\n",
        "            img_id=sample['img_id'],\n",
        "            img_hw=sample['img_hw'],\n",
        "            tgt_intrinsic=sample['src_intrinsics'][0:1],\n",
        "            src_masks=sample['src_masks'][pose_index][None, None, :],\n",
        "            src_rgbs=sample['src_rgbs'][pose_index][None, None, :],\n",
        "            src_c2w_mats=sample['src_c2w_mats'][pose_index][None, None, :],\n",
        "            src_intrinsics=sample['src_intrinsics'][pose_index][None, None, :],\n",
        "            depth_range=sample['depth_range'][None, :]\n",
        "        )\n",
        "\n",
        "        input_im = sample['src_rgbs'][pose_index].cpu().numpy()\n",
        "        filename = os.path.join(out_folder, 'input.png')\n",
        "        imageio.imwrite(filename, (input_im*255.).astype(np.uint8))\n",
        "\n",
        "        radius = (dataset.z_near + dataset.z_far) * 0.5\n",
        "        print(\"> Using default camera radius\", radius)\n",
        "\n",
        "        # Use 360 pose sequence from NeRF\n",
        "        render_poses = torch.stack(\n",
        "            [\n",
        "                pose_spherical(angle, args.elevation, radius)\n",
        "                for angle in np.linspace(-180, 180, args.num_frames)[::-1]\n",
        "            ],\n",
        "            0,\n",
        "        )  # (NV, 4, 4)\n",
        "        # +z is the vertical axis\n",
        "\n",
        "        imgs = []\n",
        "        with torch.no_grad():\n",
        "\n",
        "            for idx, pose in enumerate(tqdm.tqdm(render_poses)):\n",
        "                filename = os.path.join(out_folder, f'{idx:06}.png')\n",
        "                data_input['tgt_c2w_mat'] = pose[None, :]\n",
        "\n",
        "                # load training rays\n",
        "                ray_sampler = RaySamplerSingleImage(data_input, device, render_stride=1)\n",
        "                ray_batch = ray_sampler.get_all()\n",
        "                featmaps = model.encode(ray_batch['src_rgbs'])\n",
        "\n",
        "                ret = render_single_image(ray_sampler=ray_sampler,\n",
        "                                        ray_batch=ray_batch,\n",
        "                                        model=model,\n",
        "                                        projector=projector,\n",
        "                                        chunk_size=args.chunk_size,\n",
        "                                        N_samples=args.N_samples,\n",
        "                                        inv_uniform=args.inv_uniform,\n",
        "                                        N_importance=args.N_importance,\n",
        "                                        det=True,\n",
        "                                        white_bkgd=args.white_bkgd,\n",
        "                                        render_stride=1,\n",
        "                                        featmaps=featmaps)\n",
        "\n",
        "                if ret['outputs_fine']:\n",
        "                    rgb_im = img_HWC2CHW(ret['outputs_fine']['rgb'].detach().cpu())\n",
        "                else:\n",
        "                    rgb_im = img_HWC2CHW(ret['outputs_coarse']['rgb'].detach().cpu())\n",
        "                # clamping RGB images\n",
        "                rgb_im = torch.clamp(rgb_im, 0.0, 1.0).permute([1, 2, 0]).cpu().numpy()\n",
        "                rgb_im = (rgb_im * 255.).astype(np.uint8)\n",
        "                imageio.imwrite(filename, rgb_im)\n",
        "                imgs.append(rgb_im)\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "            imgs = np.stack(imgs, 0)\n",
        "            imageio.mimsave(os.path.join(out_folder, f'output.gif'), imgs, fps=12)\n"
      ],
      "metadata": {
        "id": "6-y-L3Cf7BkL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "args=Parameters_gen_video()"
      ],
      "metadata": {
        "id": "KrOkZGtus2M9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#args=Parameters_eval()\n",
        "args.outdir='/content/gdrive/MyDrive/Colab Notebooks/NeRF/VisionNeRF/out_video/'\n",
        "args.data_path='/content/gdrive/MyDrive/Colab Notebooks/NeRF/Data/srn_cars/cars_test/1a3782ae4bd711b66b418c7d9fedcaa9/'\n",
        "args.ckptdir='/content/gdrive/MyDrive/Colab Notebooks/NeRF/VisionNeRF/Pretrained/'\n",
        "args.ckpt_path='/content/gdrive/MyDrive/Colab Notebooks/NeRF/VisionNeRF/Pretrained/srn_cars500000.pth'\n",
        "args.img_hw=[128,128]\n",
        "args.chunk_size=2048\n",
        "args.mlp_block_num=6\n",
        "args.white_bkgd=True\n",
        "args.im_feat_dim=512\n",
        "args.skip=1\n",
        "args.data_range=[0,3]\n",
        "args.distributed=False\n",
        "args.config=None\n",
        "gen_video(args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mgGnsPJgpiGg",
        "outputId": "4ea42d54-860a-4c55-cae1-50286a8b1271"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "checkpoints reload from /content/gdrive/MyDrive/Colab Notebooks/NeRF/VisionNeRF/Pretrained/\n",
            "Loading real dataset /content/gdrive/MyDrive/Colab Notebooks/NeRF/Data/srn_cars/cars_test/1a3782ae4bd711b66b418c7d9fedcaa9/\n",
            "Reloading from /content/gdrive/MyDrive/Colab Notebooks/NeRF/VisionNeRF/Pretrained/srn_chairs_500000.pth, starting at step=500000\n",
            "Rendering /content/gdrive/MyDrive/Colab Notebooks/NeRF/Data/srn_cars/cars_test/1a3782ae4bd711b66b418c7d9fedcaa9\n",
            "videos will be saved to /content/gdrive/MyDrive/Colab Notebooks/NeRF/VisionNeRF/out_video/000000\n",
            "> Using default camera radius 1.3\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 40/40 [01:32<00:00,  2.30s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rendering /content/gdrive/MyDrive/Colab Notebooks/NeRF/Data/srn_cars/cars_test/1a3782ae4bd711b66b418c7d9fedcaa9\n",
            "videos will be saved to /content/gdrive/MyDrive/Colab Notebooks/NeRF/VisionNeRF/out_video/000001\n",
            "> Using default camera radius 1.3\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 40/40 [01:41<00:00,  2.54s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rendering /content/gdrive/MyDrive/Colab Notebooks/NeRF/Data/srn_cars/cars_test/1a3782ae4bd711b66b418c7d9fedcaa9\n",
            "videos will be saved to /content/gdrive/MyDrive/Colab Notebooks/NeRF/VisionNeRF/out_video/000002\n",
            "> Using default camera radius 1.3\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 40/40 [01:41<00:00,  2.53s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rendering /content/gdrive/MyDrive/Colab Notebooks/NeRF/Data/srn_cars/cars_test/1a3782ae4bd711b66b418c7d9fedcaa9\n",
            "videos will be saved to /content/gdrive/MyDrive/Colab Notebooks/NeRF/VisionNeRF/out_video/000003\n",
            "> Using default camera radius 1.3\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 40/40 [01:30<00:00,  2.26s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rendering /content/gdrive/MyDrive/Colab Notebooks/NeRF/Data/srn_cars/cars_test/1a3782ae4bd711b66b418c7d9fedcaa9\n",
            "videos will be saved to /content/gdrive/MyDrive/Colab Notebooks/NeRF/VisionNeRF/out_video/000004\n",
            "> Using default camera radius 1.3\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 40/40 [01:30<00:00,  2.26s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rendering /content/gdrive/MyDrive/Colab Notebooks/NeRF/Data/srn_cars/cars_test/1a3782ae4bd711b66b418c7d9fedcaa9\n",
            "videos will be saved to /content/gdrive/MyDrive/Colab Notebooks/NeRF/VisionNeRF/out_video/000005\n",
            "> Using default camera radius 1.3\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 40/40 [01:30<00:00,  2.26s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rendering /content/gdrive/MyDrive/Colab Notebooks/NeRF/Data/srn_cars/cars_test/1a3782ae4bd711b66b418c7d9fedcaa9\n",
            "videos will be saved to /content/gdrive/MyDrive/Colab Notebooks/NeRF/VisionNeRF/out_video/000006\n",
            "> Using default camera radius 1.3\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 40/40 [01:30<00:00,  2.26s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rendering /content/gdrive/MyDrive/Colab Notebooks/NeRF/Data/srn_cars/cars_test/1a3782ae4bd711b66b418c7d9fedcaa9\n",
            "videos will be saved to /content/gdrive/MyDrive/Colab Notebooks/NeRF/VisionNeRF/out_video/000007\n",
            "> Using default camera radius 1.3\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 40/40 [01:30<00:00,  2.26s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rendering /content/gdrive/MyDrive/Colab Notebooks/NeRF/Data/srn_cars/cars_test/1a3782ae4bd711b66b418c7d9fedcaa9\n",
            "videos will be saved to /content/gdrive/MyDrive/Colab Notebooks/NeRF/VisionNeRF/out_video/000008\n",
            "> Using default camera radius 1.3\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 40/40 [01:30<00:00,  2.26s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rendering /content/gdrive/MyDrive/Colab Notebooks/NeRF/Data/srn_cars/cars_test/1a3782ae4bd711b66b418c7d9fedcaa9\n",
            "videos will be saved to /content/gdrive/MyDrive/Colab Notebooks/NeRF/VisionNeRF/out_video/000009\n",
            "> Using default camera radius 1.3\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 40/40 [01:30<00:00,  2.26s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rendering /content/gdrive/MyDrive/Colab Notebooks/NeRF/Data/srn_cars/cars_test/1a3782ae4bd711b66b418c7d9fedcaa9\n",
            "videos will be saved to /content/gdrive/MyDrive/Colab Notebooks/NeRF/VisionNeRF/out_video/000010\n",
            "> Using default camera radius 1.3\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 40/40 [01:30<00:00,  2.26s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rendering /content/gdrive/MyDrive/Colab Notebooks/NeRF/Data/srn_cars/cars_test/1a3782ae4bd711b66b418c7d9fedcaa9\n",
            "videos will be saved to /content/gdrive/MyDrive/Colab Notebooks/NeRF/VisionNeRF/out_video/000011\n",
            "> Using default camera radius 1.3\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 40/40 [01:30<00:00,  2.26s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rendering /content/gdrive/MyDrive/Colab Notebooks/NeRF/Data/srn_cars/cars_test/1a3782ae4bd711b66b418c7d9fedcaa9\n",
            "videos will be saved to /content/gdrive/MyDrive/Colab Notebooks/NeRF/VisionNeRF/out_video/000012\n",
            "> Using default camera radius 1.3\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 40/40 [01:30<00:00,  2.26s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rendering /content/gdrive/MyDrive/Colab Notebooks/NeRF/Data/srn_cars/cars_test/1a3782ae4bd711b66b418c7d9fedcaa9\n",
            "videos will be saved to /content/gdrive/MyDrive/Colab Notebooks/NeRF/VisionNeRF/out_video/000013\n",
            "> Using default camera radius 1.3\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 40/40 [01:30<00:00,  2.26s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rendering /content/gdrive/MyDrive/Colab Notebooks/NeRF/Data/srn_cars/cars_test/1a3782ae4bd711b66b418c7d9fedcaa9\n",
            "videos will be saved to /content/gdrive/MyDrive/Colab Notebooks/NeRF/VisionNeRF/out_video/000014\n",
            "> Using default camera radius 1.3\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 40/40 [01:30<00:00,  2.26s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rendering /content/gdrive/MyDrive/Colab Notebooks/NeRF/Data/srn_cars/cars_test/1a3782ae4bd711b66b418c7d9fedcaa9\n",
            "videos will be saved to /content/gdrive/MyDrive/Colab Notebooks/NeRF/VisionNeRF/out_video/000015\n",
            "> Using default camera radius 1.3\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 40/40 [01:30<00:00,  2.26s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rendering /content/gdrive/MyDrive/Colab Notebooks/NeRF/Data/srn_cars/cars_test/1a3782ae4bd711b66b418c7d9fedcaa9\n",
            "videos will be saved to /content/gdrive/MyDrive/Colab Notebooks/NeRF/VisionNeRF/out_video/000016\n",
            "> Using default camera radius 1.3\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 40/40 [01:30<00:00,  2.26s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rendering /content/gdrive/MyDrive/Colab Notebooks/NeRF/Data/srn_cars/cars_test/1a3782ae4bd711b66b418c7d9fedcaa9\n",
            "videos will be saved to /content/gdrive/MyDrive/Colab Notebooks/NeRF/VisionNeRF/out_video/000017\n",
            "> Using default camera radius 1.3\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 40/40 [01:30<00:00,  2.26s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rendering /content/gdrive/MyDrive/Colab Notebooks/NeRF/Data/srn_cars/cars_test/1a3782ae4bd711b66b418c7d9fedcaa9\n",
            "videos will be saved to /content/gdrive/MyDrive/Colab Notebooks/NeRF/VisionNeRF/out_video/000018\n",
            "> Using default camera radius 1.3\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 40/40 [01:30<00:00,  2.26s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rendering /content/gdrive/MyDrive/Colab Notebooks/NeRF/Data/srn_cars/cars_test/1a3782ae4bd711b66b418c7d9fedcaa9\n",
            "videos will be saved to /content/gdrive/MyDrive/Colab Notebooks/NeRF/VisionNeRF/out_video/000019\n",
            "> Using default camera radius 1.3\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 40/40 [01:30<00:00,  2.26s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rendering /content/gdrive/MyDrive/Colab Notebooks/NeRF/Data/srn_cars/cars_test/1a3782ae4bd711b66b418c7d9fedcaa9\n",
            "videos will be saved to /content/gdrive/MyDrive/Colab Notebooks/NeRF/VisionNeRF/out_video/000020\n",
            "> Using default camera radius 1.3\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 40/40 [01:30<00:00,  2.26s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rendering /content/gdrive/MyDrive/Colab Notebooks/NeRF/Data/srn_cars/cars_test/1a3782ae4bd711b66b418c7d9fedcaa9\n",
            "videos will be saved to /content/gdrive/MyDrive/Colab Notebooks/NeRF/VisionNeRF/out_video/000021\n",
            "> Using default camera radius 1.3\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 40/40 [01:30<00:00,  2.26s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rendering /content/gdrive/MyDrive/Colab Notebooks/NeRF/Data/srn_cars/cars_test/1a3782ae4bd711b66b418c7d9fedcaa9\n",
            "videos will be saved to /content/gdrive/MyDrive/Colab Notebooks/NeRF/VisionNeRF/out_video/000022\n",
            "> Using default camera radius 1.3\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 40/40 [01:30<00:00,  2.26s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rendering /content/gdrive/MyDrive/Colab Notebooks/NeRF/Data/srn_cars/cars_test/1a3782ae4bd711b66b418c7d9fedcaa9\n",
            "videos will be saved to /content/gdrive/MyDrive/Colab Notebooks/NeRF/VisionNeRF/out_video/000023\n",
            "> Using default camera radius 1.3\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 40/40 [01:30<00:00,  2.26s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rendering /content/gdrive/MyDrive/Colab Notebooks/NeRF/Data/srn_cars/cars_test/1a3782ae4bd711b66b418c7d9fedcaa9\n",
            "videos will be saved to /content/gdrive/MyDrive/Colab Notebooks/NeRF/VisionNeRF/out_video/000024\n",
            "> Using default camera radius 1.3\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 40/40 [01:30<00:00,  2.26s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rendering /content/gdrive/MyDrive/Colab Notebooks/NeRF/Data/srn_cars/cars_test/1a3782ae4bd711b66b418c7d9fedcaa9\n",
            "videos will be saved to /content/gdrive/MyDrive/Colab Notebooks/NeRF/VisionNeRF/out_video/000025\n",
            "> Using default camera radius 1.3\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 40/40 [01:30<00:00,  2.26s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rendering /content/gdrive/MyDrive/Colab Notebooks/NeRF/Data/srn_cars/cars_test/1a3782ae4bd711b66b418c7d9fedcaa9\n",
            "videos will be saved to /content/gdrive/MyDrive/Colab Notebooks/NeRF/VisionNeRF/out_video/000026\n",
            "> Using default camera radius 1.3\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 40/40 [01:30<00:00,  2.26s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rendering /content/gdrive/MyDrive/Colab Notebooks/NeRF/Data/srn_cars/cars_test/1a3782ae4bd711b66b418c7d9fedcaa9\n",
            "videos will be saved to /content/gdrive/MyDrive/Colab Notebooks/NeRF/VisionNeRF/out_video/000027\n",
            "> Using default camera radius 1.3\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 40/40 [01:30<00:00,  2.26s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rendering /content/gdrive/MyDrive/Colab Notebooks/NeRF/Data/srn_cars/cars_test/1a3782ae4bd711b66b418c7d9fedcaa9\n",
            "videos will be saved to /content/gdrive/MyDrive/Colab Notebooks/NeRF/VisionNeRF/out_video/000028\n",
            "> Using default camera radius 1.3\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 40/40 [01:30<00:00,  2.26s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rendering /content/gdrive/MyDrive/Colab Notebooks/NeRF/Data/srn_cars/cars_test/1a3782ae4bd711b66b418c7d9fedcaa9\n",
            "videos will be saved to /content/gdrive/MyDrive/Colab Notebooks/NeRF/VisionNeRF/out_video/000029\n",
            "> Using default camera radius 1.3\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 40/40 [01:30<00:00,  2.26s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rendering /content/gdrive/MyDrive/Colab Notebooks/NeRF/Data/srn_cars/cars_test/1a3782ae4bd711b66b418c7d9fedcaa9\n",
            "videos will be saved to /content/gdrive/MyDrive/Colab Notebooks/NeRF/VisionNeRF/out_video/000030\n",
            "> Using default camera radius 1.3\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 40/40 [01:30<00:00,  2.26s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rendering /content/gdrive/MyDrive/Colab Notebooks/NeRF/Data/srn_cars/cars_test/1a3782ae4bd711b66b418c7d9fedcaa9\n",
            "videos will be saved to /content/gdrive/MyDrive/Colab Notebooks/NeRF/VisionNeRF/out_video/000031\n",
            "> Using default camera radius 1.3\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 40/40 [01:30<00:00,  2.26s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rendering /content/gdrive/MyDrive/Colab Notebooks/NeRF/Data/srn_cars/cars_test/1a3782ae4bd711b66b418c7d9fedcaa9\n",
            "videos will be saved to /content/gdrive/MyDrive/Colab Notebooks/NeRF/VisionNeRF/out_video/000032\n",
            "> Using default camera radius 1.3\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 40/40 [01:30<00:00,  2.26s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rendering /content/gdrive/MyDrive/Colab Notebooks/NeRF/Data/srn_cars/cars_test/1a3782ae4bd711b66b418c7d9fedcaa9\n",
            "videos will be saved to /content/gdrive/MyDrive/Colab Notebooks/NeRF/VisionNeRF/out_video/000033\n",
            "> Using default camera radius 1.3\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 40/40 [01:30<00:00,  2.26s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rendering /content/gdrive/MyDrive/Colab Notebooks/NeRF/Data/srn_cars/cars_test/1a3782ae4bd711b66b418c7d9fedcaa9\n",
            "videos will be saved to /content/gdrive/MyDrive/Colab Notebooks/NeRF/VisionNeRF/out_video/000034\n",
            "> Using default camera radius 1.3\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 40/40 [01:30<00:00,  2.26s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rendering /content/gdrive/MyDrive/Colab Notebooks/NeRF/Data/srn_cars/cars_test/1a3782ae4bd711b66b418c7d9fedcaa9\n",
            "videos will be saved to /content/gdrive/MyDrive/Colab Notebooks/NeRF/VisionNeRF/out_video/000035\n",
            "> Using default camera radius 1.3\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 40/40 [01:30<00:00,  2.26s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rendering /content/gdrive/MyDrive/Colab Notebooks/NeRF/Data/srn_cars/cars_test/1a3782ae4bd711b66b418c7d9fedcaa9\n",
            "videos will be saved to /content/gdrive/MyDrive/Colab Notebooks/NeRF/VisionNeRF/out_video/000036\n",
            "> Using default camera radius 1.3\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 40/40 [01:30<00:00,  2.26s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rendering /content/gdrive/MyDrive/Colab Notebooks/NeRF/Data/srn_cars/cars_test/1a3782ae4bd711b66b418c7d9fedcaa9\n",
            "videos will be saved to /content/gdrive/MyDrive/Colab Notebooks/NeRF/VisionNeRF/out_video/000037\n",
            "> Using default camera radius 1.3\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 40/40 [01:30<00:00,  2.26s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rendering /content/gdrive/MyDrive/Colab Notebooks/NeRF/Data/srn_cars/cars_test/1a3782ae4bd711b66b418c7d9fedcaa9\n",
            "videos will be saved to /content/gdrive/MyDrive/Colab Notebooks/NeRF/VisionNeRF/out_video/000038\n",
            "> Using default camera radius 1.3\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 40/40 [01:30<00:00,  2.26s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rendering /content/gdrive/MyDrive/Colab Notebooks/NeRF/Data/srn_cars/cars_test/1a3782ae4bd711b66b418c7d9fedcaa9\n",
            "videos will be saved to /content/gdrive/MyDrive/Colab Notebooks/NeRF/VisionNeRF/out_video/000039\n",
            "> Using default camera radius 1.3\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 40/40 [01:30<00:00,  2.26s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rendering /content/gdrive/MyDrive/Colab Notebooks/NeRF/Data/srn_cars/cars_test/1a3782ae4bd711b66b418c7d9fedcaa9\n",
            "videos will be saved to /content/gdrive/MyDrive/Colab Notebooks/NeRF/VisionNeRF/out_video/000040\n",
            "> Using default camera radius 1.3\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 40/40 [01:30<00:00,  2.26s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rendering /content/gdrive/MyDrive/Colab Notebooks/NeRF/Data/srn_cars/cars_test/1a3782ae4bd711b66b418c7d9fedcaa9\n",
            "videos will be saved to /content/gdrive/MyDrive/Colab Notebooks/NeRF/VisionNeRF/out_video/000041\n",
            "> Using default camera radius 1.3\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 40/40 [01:30<00:00,  2.26s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rendering /content/gdrive/MyDrive/Colab Notebooks/NeRF/Data/srn_cars/cars_test/1a3782ae4bd711b66b418c7d9fedcaa9\n",
            "videos will be saved to /content/gdrive/MyDrive/Colab Notebooks/NeRF/VisionNeRF/out_video/000042\n",
            "> Using default camera radius 1.3\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 40/40 [01:30<00:00,  2.26s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rendering /content/gdrive/MyDrive/Colab Notebooks/NeRF/Data/srn_cars/cars_test/1a3782ae4bd711b66b418c7d9fedcaa9\n",
            "videos will be saved to /content/gdrive/MyDrive/Colab Notebooks/NeRF/VisionNeRF/out_video/000043\n",
            "> Using default camera radius 1.3\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 40/40 [01:30<00:00,  2.26s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rendering /content/gdrive/MyDrive/Colab Notebooks/NeRF/Data/srn_cars/cars_test/1a3782ae4bd711b66b418c7d9fedcaa9\n",
            "videos will be saved to /content/gdrive/MyDrive/Colab Notebooks/NeRF/VisionNeRF/out_video/000044\n",
            "> Using default camera radius 1.3\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 40/40 [01:30<00:00,  2.26s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rendering /content/gdrive/MyDrive/Colab Notebooks/NeRF/Data/srn_cars/cars_test/1a3782ae4bd711b66b418c7d9fedcaa9\n",
            "videos will be saved to /content/gdrive/MyDrive/Colab Notebooks/NeRF/VisionNeRF/out_video/000045\n",
            "> Using default camera radius 1.3\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 40/40 [01:30<00:00,  2.26s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rendering /content/gdrive/MyDrive/Colab Notebooks/NeRF/Data/srn_cars/cars_test/1a3782ae4bd711b66b418c7d9fedcaa9\n",
            "videos will be saved to /content/gdrive/MyDrive/Colab Notebooks/NeRF/VisionNeRF/out_video/000046\n",
            "> Using default camera radius 1.3\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 40/40 [01:30<00:00,  2.26s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rendering /content/gdrive/MyDrive/Colab Notebooks/NeRF/Data/srn_cars/cars_test/1a3782ae4bd711b66b418c7d9fedcaa9\n",
            "videos will be saved to /content/gdrive/MyDrive/Colab Notebooks/NeRF/VisionNeRF/out_video/000047\n",
            "> Using default camera radius 1.3\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 40/40 [01:30<00:00,  2.26s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rendering /content/gdrive/MyDrive/Colab Notebooks/NeRF/Data/srn_cars/cars_test/1a3782ae4bd711b66b418c7d9fedcaa9\n",
            "videos will be saved to /content/gdrive/MyDrive/Colab Notebooks/NeRF/VisionNeRF/out_video/000048\n",
            "> Using default camera radius 1.3\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 40/40 [01:30<00:00,  2.26s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rendering /content/gdrive/MyDrive/Colab Notebooks/NeRF/Data/srn_cars/cars_test/1a3782ae4bd711b66b418c7d9fedcaa9\n",
            "videos will be saved to /content/gdrive/MyDrive/Colab Notebooks/NeRF/VisionNeRF/out_video/000049\n",
            "> Using default camera radius 1.3\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 40/40 [01:30<00:00,  2.26s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rendering /content/gdrive/MyDrive/Colab Notebooks/NeRF/Data/srn_cars/cars_test/1a3782ae4bd711b66b418c7d9fedcaa9\n",
            "videos will be saved to /content/gdrive/MyDrive/Colab Notebooks/NeRF/VisionNeRF/out_video/000050\n",
            "> Using default camera radius 1.3\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 40/40 [01:30<00:00,  2.26s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rendering /content/gdrive/MyDrive/Colab Notebooks/NeRF/Data/srn_cars/cars_test/1a3782ae4bd711b66b418c7d9fedcaa9\n",
            "videos will be saved to /content/gdrive/MyDrive/Colab Notebooks/NeRF/VisionNeRF/out_video/000051\n",
            "> Using default camera radius 1.3\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 40/40 [01:30<00:00,  2.26s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rendering /content/gdrive/MyDrive/Colab Notebooks/NeRF/Data/srn_cars/cars_test/1a3782ae4bd711b66b418c7d9fedcaa9\n",
            "videos will be saved to /content/gdrive/MyDrive/Colab Notebooks/NeRF/VisionNeRF/out_video/000052\n",
            "> Using default camera radius 1.3\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 40/40 [01:30<00:00,  2.26s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rendering /content/gdrive/MyDrive/Colab Notebooks/NeRF/Data/srn_cars/cars_test/1a3782ae4bd711b66b418c7d9fedcaa9\n",
            "videos will be saved to /content/gdrive/MyDrive/Colab Notebooks/NeRF/VisionNeRF/out_video/000053\n",
            "> Using default camera radius 1.3\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 40/40 [01:30<00:00,  2.26s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rendering /content/gdrive/MyDrive/Colab Notebooks/NeRF/Data/srn_cars/cars_test/1a3782ae4bd711b66b418c7d9fedcaa9\n",
            "videos will be saved to /content/gdrive/MyDrive/Colab Notebooks/NeRF/VisionNeRF/out_video/000054\n",
            "> Using default camera radius 1.3\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 40/40 [01:30<00:00,  2.26s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rendering /content/gdrive/MyDrive/Colab Notebooks/NeRF/Data/srn_cars/cars_test/1a3782ae4bd711b66b418c7d9fedcaa9\n",
            "videos will be saved to /content/gdrive/MyDrive/Colab Notebooks/NeRF/VisionNeRF/out_video/000055\n",
            "> Using default camera radius 1.3\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 40/40 [01:30<00:00,  2.26s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rendering /content/gdrive/MyDrive/Colab Notebooks/NeRF/Data/srn_cars/cars_test/1a3782ae4bd711b66b418c7d9fedcaa9\n",
            "videos will be saved to /content/gdrive/MyDrive/Colab Notebooks/NeRF/VisionNeRF/out_video/000056\n",
            "> Using default camera radius 1.3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40/40 [01:30<00:00,  2.26s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rendering /content/gdrive/MyDrive/Colab Notebooks/NeRF/Data/srn_cars/cars_test/1a3782ae4bd711b66b418c7d9fedcaa9\n",
            "videos will be saved to /content/gdrive/MyDrive/Colab Notebooks/NeRF/VisionNeRF/out_video/000057\n",
            "> Using default camera radius 1.3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40/40 [01:30<00:00,  2.26s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rendering /content/gdrive/MyDrive/Colab Notebooks/NeRF/Data/srn_cars/cars_test/1a3782ae4bd711b66b418c7d9fedcaa9\n",
            "videos will be saved to /content/gdrive/MyDrive/Colab Notebooks/NeRF/VisionNeRF/out_video/000058\n",
            "> Using default camera radius 1.3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40/40 [01:30<00:00,  2.26s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rendering /content/gdrive/MyDrive/Colab Notebooks/NeRF/Data/srn_cars/cars_test/1a3782ae4bd711b66b418c7d9fedcaa9\n",
            "videos will be saved to /content/gdrive/MyDrive/Colab Notebooks/NeRF/VisionNeRF/out_video/000059\n",
            "> Using default camera radius 1.3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40/40 [01:30<00:00,  2.26s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rendering /content/gdrive/MyDrive/Colab Notebooks/NeRF/Data/srn_cars/cars_test/1a3782ae4bd711b66b418c7d9fedcaa9\n",
            "videos will be saved to /content/gdrive/MyDrive/Colab Notebooks/NeRF/VisionNeRF/out_video/000060\n",
            "> Using default camera radius 1.3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40/40 [01:30<00:00,  2.26s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rendering /content/gdrive/MyDrive/Colab Notebooks/NeRF/Data/srn_cars/cars_test/1a3782ae4bd711b66b418c7d9fedcaa9\n",
            "videos will be saved to /content/gdrive/MyDrive/Colab Notebooks/NeRF/VisionNeRF/out_video/000061\n",
            "> Using default camera radius 1.3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40/40 [01:30<00:00,  2.26s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rendering /content/gdrive/MyDrive/Colab Notebooks/NeRF/Data/srn_cars/cars_test/1a3782ae4bd711b66b418c7d9fedcaa9\n",
            "videos will be saved to /content/gdrive/MyDrive/Colab Notebooks/NeRF/VisionNeRF/out_video/000062\n",
            "> Using default camera radius 1.3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40/40 [01:30<00:00,  2.26s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rendering /content/gdrive/MyDrive/Colab Notebooks/NeRF/Data/srn_cars/cars_test/1a3782ae4bd711b66b418c7d9fedcaa9\n",
            "videos will be saved to /content/gdrive/MyDrive/Colab Notebooks/NeRF/VisionNeRF/out_video/000063\n",
            "> Using default camera radius 1.3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40/40 [01:30<00:00,  2.26s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rendering /content/gdrive/MyDrive/Colab Notebooks/NeRF/Data/srn_cars/cars_test/1a3782ae4bd711b66b418c7d9fedcaa9\n",
            "videos will be saved to /content/gdrive/MyDrive/Colab Notebooks/NeRF/VisionNeRF/out_video/000064\n",
            "> Using default camera radius 1.3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40/40 [01:30<00:00,  2.26s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rendering /content/gdrive/MyDrive/Colab Notebooks/NeRF/Data/srn_cars/cars_test/1a3782ae4bd711b66b418c7d9fedcaa9\n",
            "videos will be saved to /content/gdrive/MyDrive/Colab Notebooks/NeRF/VisionNeRF/out_video/000065\n",
            "> Using default camera radius 1.3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40/40 [01:30<00:00,  2.26s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rendering /content/gdrive/MyDrive/Colab Notebooks/NeRF/Data/srn_cars/cars_test/1a3782ae4bd711b66b418c7d9fedcaa9\n",
            "videos will be saved to /content/gdrive/MyDrive/Colab Notebooks/NeRF/VisionNeRF/out_video/000066\n",
            "> Using default camera radius 1.3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40/40 [01:30<00:00,  2.26s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rendering /content/gdrive/MyDrive/Colab Notebooks/NeRF/Data/srn_cars/cars_test/1a3782ae4bd711b66b418c7d9fedcaa9\n",
            "videos will be saved to /content/gdrive/MyDrive/Colab Notebooks/NeRF/VisionNeRF/out_video/000067\n",
            "> Using default camera radius 1.3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40/40 [01:30<00:00,  2.26s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rendering /content/gdrive/MyDrive/Colab Notebooks/NeRF/Data/srn_cars/cars_test/1a3782ae4bd711b66b418c7d9fedcaa9\n",
            "videos will be saved to /content/gdrive/MyDrive/Colab Notebooks/NeRF/VisionNeRF/out_video/000068\n",
            "> Using default camera radius 1.3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40/40 [01:30<00:00,  2.26s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rendering /content/gdrive/MyDrive/Colab Notebooks/NeRF/Data/srn_cars/cars_test/1a3782ae4bd711b66b418c7d9fedcaa9\n",
            "videos will be saved to /content/gdrive/MyDrive/Colab Notebooks/NeRF/VisionNeRF/out_video/000069\n",
            "> Using default camera radius 1.3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40/40 [01:30<00:00,  2.26s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rendering /content/gdrive/MyDrive/Colab Notebooks/NeRF/Data/srn_cars/cars_test/1a3782ae4bd711b66b418c7d9fedcaa9\n",
            "videos will be saved to /content/gdrive/MyDrive/Colab Notebooks/NeRF/VisionNeRF/out_video/000070\n",
            "> Using default camera radius 1.3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40/40 [01:30<00:00,  2.26s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rendering /content/gdrive/MyDrive/Colab Notebooks/NeRF/Data/srn_cars/cars_test/1a3782ae4bd711b66b418c7d9fedcaa9\n",
            "videos will be saved to /content/gdrive/MyDrive/Colab Notebooks/NeRF/VisionNeRF/out_video/000071\n",
            "> Using default camera radius 1.3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40/40 [01:30<00:00,  2.26s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rendering /content/gdrive/MyDrive/Colab Notebooks/NeRF/Data/srn_cars/cars_test/1a3782ae4bd711b66b418c7d9fedcaa9\n",
            "videos will be saved to /content/gdrive/MyDrive/Colab Notebooks/NeRF/VisionNeRF/out_video/000072\n",
            "> Using default camera radius 1.3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40/40 [01:30<00:00,  2.26s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rendering /content/gdrive/MyDrive/Colab Notebooks/NeRF/Data/srn_cars/cars_test/1a3782ae4bd711b66b418c7d9fedcaa9\n",
            "videos will be saved to /content/gdrive/MyDrive/Colab Notebooks/NeRF/VisionNeRF/out_video/000073\n",
            "> Using default camera radius 1.3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40/40 [01:30<00:00,  2.25s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rendering /content/gdrive/MyDrive/Colab Notebooks/NeRF/Data/srn_cars/cars_test/1a3782ae4bd711b66b418c7d9fedcaa9\n",
            "videos will be saved to /content/gdrive/MyDrive/Colab Notebooks/NeRF/VisionNeRF/out_video/000074\n",
            "> Using default camera radius 1.3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40/40 [01:30<00:00,  2.26s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rendering /content/gdrive/MyDrive/Colab Notebooks/NeRF/Data/srn_cars/cars_test/1a3782ae4bd711b66b418c7d9fedcaa9\n",
            "videos will be saved to /content/gdrive/MyDrive/Colab Notebooks/NeRF/VisionNeRF/out_video/000075\n",
            "> Using default camera radius 1.3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40/40 [01:30<00:00,  2.26s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rendering /content/gdrive/MyDrive/Colab Notebooks/NeRF/Data/srn_cars/cars_test/1a3782ae4bd711b66b418c7d9fedcaa9\n",
            "videos will be saved to /content/gdrive/MyDrive/Colab Notebooks/NeRF/VisionNeRF/out_video/000076\n",
            "> Using default camera radius 1.3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40/40 [01:30<00:00,  2.26s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rendering /content/gdrive/MyDrive/Colab Notebooks/NeRF/Data/srn_cars/cars_test/1a3782ae4bd711b66b418c7d9fedcaa9\n",
            "videos will be saved to /content/gdrive/MyDrive/Colab Notebooks/NeRF/VisionNeRF/out_video/000077\n",
            "> Using default camera radius 1.3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40/40 [01:30<00:00,  2.26s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rendering /content/gdrive/MyDrive/Colab Notebooks/NeRF/Data/srn_cars/cars_test/1a3782ae4bd711b66b418c7d9fedcaa9\n",
            "videos will be saved to /content/gdrive/MyDrive/Colab Notebooks/NeRF/VisionNeRF/out_video/000078\n",
            "> Using default camera radius 1.3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40/40 [01:30<00:00,  2.26s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rendering /content/gdrive/MyDrive/Colab Notebooks/NeRF/Data/srn_cars/cars_test/1a3782ae4bd711b66b418c7d9fedcaa9\n",
            "videos will be saved to /content/gdrive/MyDrive/Colab Notebooks/NeRF/VisionNeRF/out_video/000079\n",
            "> Using default camera radius 1.3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40/40 [01:30<00:00,  2.26s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rendering /content/gdrive/MyDrive/Colab Notebooks/NeRF/Data/srn_cars/cars_test/1a3782ae4bd711b66b418c7d9fedcaa9\n",
            "videos will be saved to /content/gdrive/MyDrive/Colab Notebooks/NeRF/VisionNeRF/out_video/000080\n",
            "> Using default camera radius 1.3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40/40 [01:30<00:00,  2.26s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rendering /content/gdrive/MyDrive/Colab Notebooks/NeRF/Data/srn_cars/cars_test/1a3782ae4bd711b66b418c7d9fedcaa9\n",
            "videos will be saved to /content/gdrive/MyDrive/Colab Notebooks/NeRF/VisionNeRF/out_video/000081\n",
            "> Using default camera radius 1.3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40/40 [01:30<00:00,  2.26s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rendering /content/gdrive/MyDrive/Colab Notebooks/NeRF/Data/srn_cars/cars_test/1a3782ae4bd711b66b418c7d9fedcaa9\n",
            "videos will be saved to /content/gdrive/MyDrive/Colab Notebooks/NeRF/VisionNeRF/out_video/000082\n",
            "> Using default camera radius 1.3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 98%|█████████▊| 39/40 [01:28<00:02,  2.26s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "eval.py"
      ],
      "metadata": {
        "id": "FRzBY6I95-U7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import shutil\n",
        "import tqdm\n",
        "import imageio\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "\n",
        "def parse_intrinsic(path):\n",
        "    with open(path, \"r\") as f:\n",
        "        lines = f.readlines()\n",
        "        focal, cx, cy, _ = map(float, lines[0].split())\n",
        "    intrinsic = np.array([[focal, 0, cx, 0],\n",
        "                          [0, focal, cy, 0],\n",
        "                          [0,     0,  1, 0],\n",
        "                          [0,     0,  0, 1]])\n",
        "    return intrinsic\n",
        "\n",
        "def parse_pose(path):\n",
        "    return np.loadtxt(path, dtype=np.float32).reshape(4, 4)\n",
        "\n",
        "def parse_pose_dvr(path, num_views):\n",
        "    cameras = np.load(path)\n",
        "\n",
        "    intrinsics = []\n",
        "    c2w_mats = []\n",
        "\n",
        "    for i in range(num_views):\n",
        "        # ShapeNet\n",
        "        wmat_inv_key = \"world_mat_inv_\" + str(i)\n",
        "        wmat_key = \"world_mat_\" + str(i)\n",
        "        kmat_key = \"camera_mat_\" + str(i)\n",
        "        if wmat_inv_key in cameras:\n",
        "            c2w_mat = cameras[wmat_inv_key]\n",
        "        else:\n",
        "            w2c_mat = cameras[wmat_key]\n",
        "            if w2c_mat.shape[0] == 3:\n",
        "                w2c_mat = np.vstack((w2c_mat, np.array([0, 0, 0, 1])))\n",
        "            c2w_mat = np.linalg.inv(w2c_mat)\n",
        "\n",
        "        intrinsics.append(cameras[kmat_key])\n",
        "        c2w_mats.append(c2w_mat)\n",
        "\n",
        "    intrinsics = np.stack(intrinsics, 0)\n",
        "    c2w_mats = np.stack(c2w_mats, 0)\n",
        "\n",
        "    return intrinsics, c2w_mats\n",
        "\n",
        "\n",
        "class SRNRenderDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset for rendering\n",
        "    \"\"\"\n",
        "    def __init__(self, args, **kwargs):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            args.data_path: path to data directory\n",
        "            args.img_hw: image size (resize if needed)\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.base_path = args.data_path\n",
        "        self.dataset_name = os.path.basename(args.data_path)\n",
        "\n",
        "        print(\"Loading SRN dataset\", self.base_path, \"name:\", self.dataset_name)\n",
        "        assert os.path.exists(self.base_path)\n",
        "\n",
        "        is_chair = \"chair\" in self.dataset_name\n",
        "\n",
        "        intrinsic_paths = sorted(\n",
        "            glob.glob(os.path.join(self.base_path, \"*\", \"intrinsics.txt\"))\n",
        "        )\n",
        "        print(\"intrinsic\",intrinsic_paths)\n",
        "\n",
        "        self.intrinsics = []\n",
        "        self.poses = []\n",
        "        self.rgb_paths = []\n",
        "        for path in tqdm.tqdm(intrinsic_paths):\n",
        "            dir = os.path.dirname(path)\n",
        "            curr_paths = sorted(glob.glob(os.path.join(dir, \"rgb\", \"*\")))\n",
        "            self.rgb_paths.append(curr_paths)\n",
        "\n",
        "            pose_paths = [f.replace('rgb', 'pose').replace('png', 'txt') for f in curr_paths]\n",
        "            c2w_mats = [parse_pose(x) for x in\n",
        "                    pose_paths]\n",
        "            self.poses.append(c2w_mats)\n",
        "\n",
        "            self.intrinsics.append(parse_intrinsic(path))\n",
        "\n",
        "        self.rgb_paths = np.array(self.rgb_paths)\n",
        "        self.poses = np.stack(self.poses, 0)\n",
        "        self.intrinsics = np.array(self.intrinsics)\n",
        "\n",
        "        assert(len(self.rgb_paths) == len(self.poses))\n",
        "\n",
        "        self.define_transforms()\n",
        "        self.img_hw = args.img_hw\n",
        "\n",
        "        # default near/far plane depth\n",
        "        if is_chair:\n",
        "            self.z_near = 1.25\n",
        "            self.z_far = 2.75\n",
        "        else:\n",
        "            self.z_near = 0.8\n",
        "            self.z_far = 1.8\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.intrinsics)\n",
        "\n",
        "    def define_transforms(self):\n",
        "        self.img_transforms = T.Compose(\n",
        "            [T.ToTensor(), T.Normalize((0.0, 0.0, 0.0), (1.0, 1.0, 1.0))]\n",
        "        )\n",
        "        self.mask_transforms = T.Compose(\n",
        "            [T.ToTensor(), T.Normalize((0.0,), (1.0,))]\n",
        "        )\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Read source RGB\n",
        "        src_rgb_paths = self.rgb_paths[index]\n",
        "        src_c2w_mats = self.poses[index]\n",
        "        src_intrinsics = np.array([self.intrinsics[index]] * len(src_c2w_mats))\n",
        "\n",
        "        src_rgbs = []\n",
        "        src_masks = []\n",
        "        for rgb_path in src_rgb_paths:\n",
        "            img = imageio.imread(rgb_path)[..., :3]\n",
        "            mask = (img.sum(axis=-1) != 255*3)[..., None].astype(np.uint8) * 255\n",
        "            rgb = self.img_transforms(img)\n",
        "            mask = self.mask_transforms(mask)\n",
        "\n",
        "            h, w = rgb.shape[-2:]\n",
        "            if (h != self.img_hw[0]) or (w != self.img_hw[1]):\n",
        "                scale = self.img_hw[-1] / w\n",
        "                src_intrinsics[:, :2] *= scale\n",
        "\n",
        "                rgb = F.interpolate(rgb, size=self.img_hw, mode=\"area\")\n",
        "                mask = F.interpolate(mask, size=self.img_hw, mode=\"area\")\n",
        "\n",
        "            src_rgbs.append(rgb)\n",
        "            src_masks.append(mask)\n",
        "\n",
        "        depth_range = np.array([self.z_near, self.z_far])\n",
        "\n",
        "        return {\n",
        "            \"rgb_path\": rgb_path,\n",
        "            \"img_id\": index,\n",
        "            \"img_hw\": self.img_hw,\n",
        "            \"src_masks\": torch.stack(src_masks).permute([0, 2, 3, 1]).float(),\n",
        "            \"src_rgbs\": torch.stack(src_rgbs).permute([0, 2, 3, 1]).float(),\n",
        "            \"src_c2w_mats\": torch.FloatTensor(src_c2w_mats),\n",
        "            \"src_intrinsics\": torch.FloatTensor(src_intrinsics),\n",
        "            \"depth_range\": torch.FloatTensor(depth_range)\n",
        "        }\n",
        "\n",
        "def trans_t(t):\n",
        "    return torch.tensor(\n",
        "        [[-1, 0, 0, 0], [0, 0, -1, t], [0, -1, 0, 0], [0, 0, 0, 1],], dtype=torch.float32,\n",
        "    )\n",
        "\n",
        "def rot_theta(angle):\n",
        "    return torch.tensor(\n",
        "        [\n",
        "            [np.cos(angle), -np.sin(angle), 0, 0],\n",
        "            [np.sin(angle), np.cos(angle), 0, 0],\n",
        "            [0, 0, 1, 0],\n",
        "            [0, 0, 0, 1],\n",
        "        ],\n",
        "        dtype=torch.float32,\n",
        "    )\n",
        "\n",
        "def rot_phi(phi):\n",
        "    return torch.tensor(\n",
        "        [\n",
        "            [1, 0, 0, 0],\n",
        "            [0, np.cos(phi), -np.sin(phi), 0],\n",
        "            [0, np.sin(phi), np.cos(phi), 0],\n",
        "            [0, 0, 0, 1],\n",
        "        ],\n",
        "        dtype=torch.float32,\n",
        "    )\n",
        "\n",
        "def pose_spherical(theta, phi, radius):\n",
        "    \"\"\"\n",
        "    Spherical rendering poses, from NeRF\n",
        "    \"\"\"\n",
        "    c2w = trans_t(radius)\n",
        "    c2w = rot_phi(phi / 180.0 * np.pi) @ c2w\n",
        "    c2w = rot_theta(theta / 180.0 * np.pi) @ c2w\n",
        "\n",
        "    return c2w\n",
        "\n",
        "def gen_eval(args):\n",
        "\n",
        "    device = \"cuda\"\n",
        "    print(f\"checkpoints reload from {args.ckptdir}\")\n",
        "\n",
        "    dataset = SRNRenderDataset(args)\n",
        "    # Create VisionNeRF model\n",
        "    model = VisionNerfModel(args, False, False)\n",
        "    # create projector\n",
        "    projector = Projector(device=device)\n",
        "    model.switch_to_eval()\n",
        "\n",
        "    if args.use_data_index:\n",
        "        #print(\"use_data_index\")\n",
        "        data_index = args.data_indices\n",
        "    else:\n",
        "        #print(\"not_use_data_index\")\n",
        "        data_index = np.arange(args.data_range[0], args.data_range[1])\n",
        "\n",
        "    for d_idx in data_index:\n",
        "        out_folder = os.path.join(args.outdir, args.expname, f'{d_idx:06d}')\n",
        "        print(f'Rendering {dataset[d_idx][\"rgb_path\"][:-15]}')\n",
        "        print(f'images will be saved to {out_folder}')\n",
        "        os.makedirs(out_folder, exist_ok=True)\n",
        "\n",
        "        obj_name = os.path.basename(dataset[d_idx][\"rgb_path\"][:-15])\n",
        "\n",
        "        # save the args and config files\n",
        "        f = os.path.join(out_folder, 'args.txt')\n",
        "        with open(f, 'w') as file:\n",
        "            for arg in sorted(vars(args)):\n",
        "                attr = getattr(args, arg)\n",
        "                file.write('{} = {}\\n'.format(arg, attr))\n",
        "\n",
        "        #if args.config is not None:\n",
        "        #    f = os.path.join(out_folder, 'config.txt')\n",
        "        #    if not os.path.isfile(f):\n",
        "        #        shutil.copy(args.config, f)\n",
        "\n",
        "        sample = dataset[d_idx]\n",
        "        pose_index = args.pose_index\n",
        "        data_input = dict(\n",
        "            rgb_path=sample['rgb_path'],\n",
        "            img_id=sample['img_id'],\n",
        "            img_hw=sample['img_hw'],\n",
        "            tgt_intrinsic=sample['src_intrinsics'][0:1],\n",
        "            src_masks=sample['src_masks'][pose_index][None, None, :],\n",
        "            src_rgbs=sample['src_rgbs'][pose_index][None, None, :],\n",
        "            src_c2w_mats=sample['src_c2w_mats'][pose_index][None, None, :],\n",
        "            src_intrinsics=sample['src_intrinsics'][pose_index][None, None, :],\n",
        "            depth_range=sample['depth_range'][None, :]\n",
        "        )\n",
        "\n",
        "        input_im = sample['src_rgbs'][pose_index].cpu().numpy() * 255.\n",
        "        input_im = input_im.astype(np.uint8)\n",
        "        filename = os.path.join(out_folder, 'input.png')\n",
        "        imageio.imwrite(filename, input_im)\n",
        "\n",
        "        render_poses = sample['src_c2w_mats']\n",
        "        view_indices = np.arange(0, len(render_poses), args.skip)\n",
        "        render_poses = render_poses[view_indices]\n",
        "\n",
        "        with torch.no_grad():\n",
        "\n",
        "            for idx, pose in tqdm.tqdm(zip(view_indices, render_poses), total=len(view_indices)):\n",
        "                if not args.include_src and idx == args.pose_index:\n",
        "                    continue\n",
        "                filename = os.path.join(out_folder, f'{idx:06}.png')\n",
        "                data_input['tgt_c2w_mat'] = pose[None, :]\n",
        "\n",
        "                # load training rays\n",
        "                ray_sampler = RaySamplerSingleImage(data_input, device, render_stride=1)\n",
        "                ray_batch = ray_sampler.get_all()\n",
        "                featmaps = model.encode(ray_batch['src_rgbs'])\n",
        "\n",
        "                ret = render_single_image(ray_sampler=ray_sampler,\n",
        "                                          ray_batch=ray_batch,\n",
        "                                          model=model,\n",
        "                                          projector=projector,\n",
        "                                          chunk_size=args.chunk_size,\n",
        "                                          N_samples=args.N_samples,\n",
        "                                          inv_uniform=args.inv_uniform,\n",
        "                                          N_importance=args.N_importance,\n",
        "                                          det=True,\n",
        "                                          white_bkgd=args.white_bkgd,\n",
        "                                          render_stride=1,\n",
        "                                          featmaps=featmaps)\n",
        "\n",
        "                rgb_im = img_HWC2CHW(ret['outputs_fine']['rgb'].detach().cpu())\n",
        "                # clamping RGB images\n",
        "                rgb_im = torch.clamp(rgb_im, 0.0, 1.0)\n",
        "                rgb_im = rgb_im.permute([1, 2, 0]).cpu().numpy()\n",
        "\n",
        "                rgb_im = (rgb_im * 255.).astype(np.uint8)\n",
        "                imageio.imwrite(filename, rgb_im)\n",
        "                torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "NNJxAOUK5-pb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "args=Parameters_eval()\n",
        "args.outdir='/content/gdrive/MyDrive/Colab Notebooks/NeRF/VisionNeRF/out_chairs/'\n",
        "args.data_path='/content/gdrive/MyDrive/Colab Notebooks/NeRF/Data/srn_chairs/chairs_test/'\n",
        "args.ckptdir='/content/gdrive/MyDrive/Colab Notebooks/NeRF/VisionNeRF/Pretrained/'\n",
        "args.ckpt_path='/content/gdrive/MyDrive/Colab Notebooks/NeRF/VisionNeRF/Pretrained/srn_chairs500000.pth'\n",
        "args.img_hw=[128,128]\n",
        "args.chunk_size=2048\n",
        "args.mlp_block_num=6\n",
        "args.white_bkgd=True\n",
        "args.im_feat_dim=512\n",
        "args.skip=1\n",
        "args.data_range=[0,10]\n",
        "gen_eval(args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RnRfuDP4ySqG",
        "outputId": "5be45c1a-af7d-4613-8601-97df477ab1c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "checkpoints reload from /content/gdrive/MyDrive/Colab Notebooks/NeRF/VisionNeRF/Pretrained/\n",
            "Loading SRN dataset /content/gdrive/MyDrive/Colab Notebooks/NeRF/Data/srn_chairs/chairs_test/ name: \n",
            "intrinsic ['/content/gdrive/MyDrive/Colab Notebooks/NeRF/Data/srn_chairs/chairs_test/1a8bbf2994788e2743e99e0cae970928/intrinsics.txt', '/content/gdrive/MyDrive/Colab Notebooks/NeRF/Data/srn_chairs/chairs_test/1ab8a3b55c14a7b27eaeab1f0c9120b7/intrinsics.txt', '/content/gdrive/MyDrive/Colab Notebooks/NeRF/Data/srn_chairs/chairs_test/1ac6531a337de85f2f7628d6bf38bcc4/intrinsics.txt', '/content/gdrive/MyDrive/Colab Notebooks/NeRF/Data/srn_chairs/chairs_test/1aeb17f89e1bea954c6deb9ede0648df/intrinsics.txt', '/content/gdrive/MyDrive/Colab Notebooks/NeRF/Data/srn_chairs/chairs_test/1b05971a4373c7d2463600025db2266/intrinsics.txt', '/content/gdrive/MyDrive/Colab Notebooks/NeRF/Data/srn_chairs/chairs_test/1b4071814d1c1ae6e2367b9e27f16a71/intrinsics.txt', '/content/gdrive/MyDrive/Colab Notebooks/NeRF/Data/srn_chairs/chairs_test/1b5e876f3559c231532a8e162f399205/intrinsics.txt', '/content/gdrive/MyDrive/Colab Notebooks/NeRF/Data/srn_chairs/chairs_test/1b67a3a1101a9acb905477d2a8504646/intrinsics.txt', '/content/gdrive/MyDrive/Colab Notebooks/NeRF/Data/srn_chairs/chairs_test/1b81441b7e597235d61420a53a0cb96d/intrinsics.txt', '/content/gdrive/MyDrive/Colab Notebooks/NeRF/Data/srn_chairs/chairs_test/1c9d7e56ae8c90c87ac6ce513ae497d3/intrinsics.txt']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:03<00:00,  3.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reloading from /content/gdrive/MyDrive/Colab Notebooks/NeRF/VisionNeRF/Pretrained/srn_chairs_500000.pth, starting at step=500000\n",
            "Rendering /content/gdrive/MyDrive/Colab Notebooks/NeRF/Data/srn_chairs/chairs_test/1a8bbf2994788e2743e99e0cae970928\n",
            "images will be saved to /content/gdrive/MyDrive/Colab Notebooks/NeRF/VisionNeRF/out_chairs/000000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 251/251 [06:50<00:00,  1.63s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rendering /content/gdrive/MyDrive/Colab Notebooks/NeRF/Data/srn_chairs/chairs_test/1ab8a3b55c14a7b27eaeab1f0c9120b7\n",
            "images will be saved to /content/gdrive/MyDrive/Colab Notebooks/NeRF/VisionNeRF/out_chairs/000001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 251/251 [06:49<00:00,  1.63s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rendering /content/gdrive/MyDrive/Colab Notebooks/NeRF/Data/srn_chairs/chairs_test/1ac6531a337de85f2f7628d6bf38bcc4\n",
            "images will be saved to /content/gdrive/MyDrive/Colab Notebooks/NeRF/VisionNeRF/out_chairs/000002\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 251/251 [06:50<00:00,  1.63s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rendering /content/gdrive/MyDrive/Colab Notebooks/NeRF/Data/srn_chairs/chairs_test/1aeb17f89e1bea954c6deb9ede0648df\n",
            "images will be saved to /content/gdrive/MyDrive/Colab Notebooks/NeRF/VisionNeRF/out_chairs/000003\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 251/251 [06:50<00:00,  1.63s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rendering /content/gdrive/MyDrive/Colab Notebooks/NeRF/Data/srn_chairs/chairs_test/1b05971a4373c7d2463600025db2266\n",
            "images will be saved to /content/gdrive/MyDrive/Colab Notebooks/NeRF/VisionNeRF/out_chairs/000004\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 251/251 [06:50<00:00,  1.63s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rendering /content/gdrive/MyDrive/Colab Notebooks/NeRF/Data/srn_chairs/chairs_test/1b4071814d1c1ae6e2367b9e27f16a71\n",
            "images will be saved to /content/gdrive/MyDrive/Colab Notebooks/NeRF/VisionNeRF/out_chairs/000005\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 251/251 [06:51<00:00,  1.64s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rendering /content/gdrive/MyDrive/Colab Notebooks/NeRF/Data/srn_chairs/chairs_test/1b5e876f3559c231532a8e162f399205\n",
            "images will be saved to /content/gdrive/MyDrive/Colab Notebooks/NeRF/VisionNeRF/out_chairs/000006\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 251/251 [06:50<00:00,  1.64s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rendering /content/gdrive/MyDrive/Colab Notebooks/NeRF/Data/srn_chairs/chairs_test/1b67a3a1101a9acb905477d2a8504646\n",
            "images will be saved to /content/gdrive/MyDrive/Colab Notebooks/NeRF/VisionNeRF/out_chairs/000007\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 251/251 [06:50<00:00,  1.64s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rendering /content/gdrive/MyDrive/Colab Notebooks/NeRF/Data/srn_chairs/chairs_test/1b81441b7e597235d61420a53a0cb96d\n",
            "images will be saved to /content/gdrive/MyDrive/Colab Notebooks/NeRF/VisionNeRF/out_chairs/000008\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 251/251 [06:50<00:00,  1.63s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rendering /content/gdrive/MyDrive/Colab Notebooks/NeRF/Data/srn_chairs/chairs_test/1c9d7e56ae8c90c87ac6ce513ae497d3\n",
            "images will be saved to /content/gdrive/MyDrive/Colab Notebooks/NeRF/VisionNeRF/out_chairs/000009\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 251/251 [06:50<00:00,  1.64s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Si8b1kyr3d8I"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}